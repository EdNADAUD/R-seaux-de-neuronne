{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3 (durée : 3h)\n",
    "# Reconnaissance d’écriture par réseaux de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Analyse des données\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import des librairies utilisées dans le tp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.neural_network import *\n",
    "import time\n",
    "from pandas import plotting\n",
    "import sys \n",
    "import json\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Charger la base de données digits disponible sous sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Déterminer la dimension D des données et le nombre d’exemple par classe. Observer quelques images :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()\n",
    "for i in range(10):\n",
    "    plt.matshow(digits.images[i])\n",
    "#Pour récupérer les données et les labels :\n",
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Séparer une fois pour toutes la base initiale en deux : apprentissage (70%) et test (30%) (model_selection.train_test_split).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nous possédons 1257 images dans le train composé de 64 feature\n",
      "Nous possédons 540 images dans le test composé de 64 feature\n"
     ]
    }
   ],
   "source": [
    "X_train=np.array(X_train)\n",
    "X_test=np.array(X_test)\n",
    "print (\"Nous possédons {} images dans le train composé de {} feature\".format(X_train.shape[0],X_train.shape[1]))\n",
    "print (\"Nous possédons {} images dans le test composé de {} feature\".format(X_test.shape[0],X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Définir le réseau :\n",
    "    Entraîner le réseau (fonction fit). Optimiser la structure du réseau de neurones (nombre de cellules en couche cachée). Étudier l’influence du nombre de neurones cachés sur les taux de reconnaissance en apprentissage et en généralisation (fonction score). Conclure sur l’architecture optimale. Vous pouvez modifier les paramètres en fonction des conclusions tirées au TP2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### clf1 = MLPClassifier(hidden_layer_sizes=C, activation=’tanh’, solver=’sgd’, batch_size=1, alpha=0, learning_rate=’adaptive’, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Le maximun de neuronne dans la couche cachée est de ... pour éviter le sur apprentisage (Nbr de parametre libre <NB_d'exemple_dans_apprentissage*N(nombre de sortie)\n",
    "    Nous avons 2 64 classe et 1797 images dans notre base d'apprentisage soit NB_d'exemple_dans_apprentissage*N(nombre de sortie)=17970\n",
    "    Nbr de parametre libre=C*10+C*64=C*5006 soit C< 17970/74 =24\n",
    "    \n",
    "    \n",
    "    ce nombre etant tres grand et demandant bcp de calcul je me stoperai des que l'accurancy en train et test est supérieur a 95%\n",
    "    \n",
    "    et donc des perfomances en test peu satisfaisante. Nous ferons donc varié i de 1 à ... De plus comme lors du tp précédent j'enregistrerai toutes les données dans des excels pour faciliter les calcul en local plus tard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD et learning rate =0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la reconnaisssance d'image etant aujourd'hui tres performante j'attends donc un taux de reconnaisance haut, pour mOn programme et etant donné que mon ordinateur et lent a faire les calcul j'estimerai que mon model est correct avec 95 pourcent de réussite en moyenne sur chaque classe. Sachant que un neuronne ne serait pas suffisant je commencerai a 12.  je fais la moyenne sur 10 avec un saut de 3 neuronnes dans ma hiden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.88459798\n",
      "Iteration 2, loss = 1.31007215\n",
      "Iteration 3, loss = 1.07845838\n",
      "Iteration 4, loss = 1.02901022\n",
      "Iteration 5, loss = 0.97110549\n",
      "Iteration 6, loss = 0.81523970\n",
      "Iteration 7, loss = 0.68208301\n",
      "Iteration 8, loss = 0.67712412\n",
      "Iteration 9, loss = 0.66031088\n",
      "Iteration 10, loss = 0.56567009\n",
      "Iteration 11, loss = 0.60814138\n",
      "Iteration 12, loss = 0.62768116\n",
      "Iteration 13, loss = 0.59644547\n",
      "Iteration 14, loss = 0.55200551\n",
      "Iteration 15, loss = 0.73476944\n",
      "Iteration 16, loss = 0.53765526\n",
      "Iteration 17, loss = 0.49474585\n",
      "Iteration 18, loss = 0.44529174\n",
      "Iteration 19, loss = 0.52453165\n",
      "Iteration 20, loss = 0.56824336\n",
      "Iteration 21, loss = 0.40967445\n",
      "Iteration 22, loss = 0.42290023\n",
      "Iteration 23, loss = 0.41742243\n",
      "Iteration 24, loss = 0.41357860\n",
      "Iteration 25, loss = 0.38987811\n",
      "Iteration 26, loss = 0.36064060\n",
      "Iteration 27, loss = 0.41959964\n",
      "Iteration 28, loss = 0.39004094\n",
      "Iteration 29, loss = 0.35172782\n",
      "Iteration 30, loss = 0.34936708\n",
      "Iteration 31, loss = 0.46197696\n",
      "Iteration 32, loss = 0.40822308\n",
      "Iteration 33, loss = 0.35115848\n",
      "Iteration 34, loss = 0.42582175\n",
      "Iteration 35, loss = 0.44219649\n",
      "Iteration 36, loss = 0.36955355\n",
      "Iteration 37, loss = 0.43114360\n",
      "Iteration 38, loss = 0.35212518\n",
      "Iteration 39, loss = 0.32516162\n",
      "Iteration 40, loss = 0.33915305\n",
      "Iteration 41, loss = 0.35290428\n",
      "Iteration 42, loss = 0.39580267\n",
      "Iteration 43, loss = 0.57264342\n",
      "Iteration 44, loss = 0.44523610\n",
      "Iteration 45, loss = 0.36601335\n",
      "Iteration 46, loss = 0.35776508\n",
      "Iteration 47, loss = 0.40111527\n",
      "Iteration 48, loss = 0.38968905\n",
      "Iteration 49, loss = 0.38347928\n",
      "Iteration 50, loss = 0.30282899\n",
      "Iteration 51, loss = 0.29619176\n",
      "Iteration 52, loss = 0.35015279\n",
      "Iteration 53, loss = 0.34635430\n",
      "Iteration 54, loss = 0.32467196\n",
      "Iteration 55, loss = 0.33046812\n",
      "Iteration 56, loss = 0.30106213\n",
      "Iteration 57, loss = 0.36820355\n",
      "Iteration 58, loss = 0.30366497\n",
      "Iteration 59, loss = 0.32821037\n",
      "Iteration 60, loss = 0.33882326\n",
      "Iteration 61, loss = 0.45318657\n",
      "Iteration 62, loss = 0.32313850\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 63, loss = 0.26659418\n",
      "Iteration 64, loss = 0.26079576\n",
      "Iteration 65, loss = 0.25568350\n",
      "Iteration 66, loss = 0.23787126\n",
      "Iteration 67, loss = 0.24046318\n",
      "Iteration 68, loss = 0.22726737\n",
      "Iteration 69, loss = 0.22555257\n",
      "Iteration 70, loss = 0.22506341\n",
      "Iteration 71, loss = 0.21972671\n",
      "Iteration 72, loss = 0.22227106\n",
      "Iteration 73, loss = 0.21955043\n",
      "Iteration 74, loss = 0.21104378\n",
      "Iteration 75, loss = 0.21783596\n",
      "Iteration 76, loss = 0.21150301\n",
      "Iteration 77, loss = 0.22029587\n",
      "Iteration 78, loss = 0.20692159\n",
      "Iteration 79, loss = 0.20028202\n",
      "Iteration 80, loss = 0.20006346\n",
      "Iteration 81, loss = 0.20024389\n",
      "Iteration 82, loss = 0.19251836\n",
      "Iteration 83, loss = 0.20812798\n",
      "Iteration 84, loss = 0.19502795\n",
      "Iteration 85, loss = 0.18967768\n",
      "Iteration 86, loss = 0.18936348\n",
      "Iteration 87, loss = 0.18294216\n",
      "Iteration 88, loss = 0.18622138\n",
      "Iteration 89, loss = 0.18597289\n",
      "Iteration 90, loss = 0.19532141\n",
      "Iteration 91, loss = 0.19380809\n",
      "Iteration 92, loss = 0.17910096\n",
      "Iteration 93, loss = 0.17946675\n",
      "Iteration 94, loss = 0.17448823\n",
      "Iteration 95, loss = 0.17383472\n",
      "Iteration 96, loss = 0.17525548\n",
      "Iteration 97, loss = 0.18378235\n",
      "Iteration 98, loss = 0.17396161\n",
      "Iteration 99, loss = 0.17306217\n",
      "Iteration 100, loss = 0.16997372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.79578390\n",
      "Iteration 2, loss = 1.26143385\n",
      "Iteration 3, loss = 1.18511688\n",
      "Iteration 4, loss = 1.20011765\n",
      "Iteration 5, loss = 0.91947312\n",
      "Iteration 6, loss = 0.81884546\n",
      "Iteration 7, loss = 0.81213689\n",
      "Iteration 8, loss = 0.63150869\n",
      "Iteration 9, loss = 0.65300927\n",
      "Iteration 10, loss = 0.81978741\n",
      "Iteration 11, loss = 0.98582884\n",
      "Iteration 12, loss = 0.96535472\n",
      "Iteration 13, loss = 0.91581903\n",
      "Iteration 14, loss = 0.91140641\n",
      "Iteration 15, loss = 0.85406156\n",
      "Iteration 16, loss = 0.59374151\n",
      "Iteration 17, loss = 0.58015218\n",
      "Iteration 18, loss = 0.50999473\n",
      "Iteration 19, loss = 0.60828860\n",
      "Iteration 20, loss = 0.55632925\n",
      "Iteration 21, loss = 0.56555466\n",
      "Iteration 22, loss = 0.65877461\n",
      "Iteration 23, loss = 0.60311164\n",
      "Iteration 24, loss = 0.52473665\n",
      "Iteration 25, loss = 0.47203378\n",
      "Iteration 26, loss = 0.47696623\n",
      "Iteration 27, loss = 0.52455176\n",
      "Iteration 28, loss = 0.47179069\n",
      "Iteration 29, loss = 0.50919773\n",
      "Iteration 30, loss = 0.52341156\n",
      "Iteration 31, loss = 0.46138915\n",
      "Iteration 32, loss = 0.46533693\n",
      "Iteration 33, loss = 0.41448446\n",
      "Iteration 34, loss = 0.43798617\n",
      "Iteration 35, loss = 0.51916891\n",
      "Iteration 36, loss = 0.44794681\n",
      "Iteration 37, loss = 0.41403532\n",
      "Iteration 38, loss = 0.47010633\n",
      "Iteration 39, loss = 0.35965513\n",
      "Iteration 40, loss = 0.37627828\n",
      "Iteration 41, loss = 0.33983512\n",
      "Iteration 42, loss = 0.47765959\n",
      "Iteration 43, loss = 0.32448346\n",
      "Iteration 44, loss = 0.32757625\n",
      "Iteration 45, loss = 0.48920502\n",
      "Iteration 46, loss = 0.35103395\n",
      "Iteration 47, loss = 0.45341011\n",
      "Iteration 48, loss = 0.48106209\n",
      "Iteration 49, loss = 0.40674383\n",
      "Iteration 50, loss = 0.34820321\n",
      "Iteration 51, loss = 0.35806860\n",
      "Iteration 52, loss = 0.37742867\n",
      "Iteration 53, loss = 0.41098787\n",
      "Iteration 54, loss = 0.35713115\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 55, loss = 0.34866528\n",
      "Iteration 56, loss = 0.32446606\n",
      "Iteration 57, loss = 0.28673712\n",
      "Iteration 58, loss = 0.26840184\n",
      "Iteration 59, loss = 0.27889766\n",
      "Iteration 60, loss = 0.24866577\n",
      "Iteration 61, loss = 0.25269455\n",
      "Iteration 62, loss = 0.24565091\n",
      "Iteration 63, loss = 0.24500242\n",
      "Iteration 64, loss = 0.23683061\n",
      "Iteration 65, loss = 0.23632791\n",
      "Iteration 66, loss = 0.24076637\n",
      "Iteration 67, loss = 0.24533607\n",
      "Iteration 68, loss = 0.24972499\n",
      "Iteration 69, loss = 0.23039349\n",
      "Iteration 70, loss = 0.22469110\n",
      "Iteration 71, loss = 0.22764714\n",
      "Iteration 72, loss = 0.21609034\n",
      "Iteration 73, loss = 0.22198065\n",
      "Iteration 74, loss = 0.21827824\n",
      "Iteration 75, loss = 0.21293971\n",
      "Iteration 76, loss = 0.21847815\n",
      "Iteration 77, loss = 0.20739999\n",
      "Iteration 78, loss = 0.21181445\n",
      "Iteration 79, loss = 0.19734742\n",
      "Iteration 80, loss = 0.19585048\n",
      "Iteration 81, loss = 0.19538286\n",
      "Iteration 82, loss = 0.18955743\n",
      "Iteration 83, loss = 0.18799864\n",
      "Iteration 84, loss = 0.18825986\n",
      "Iteration 85, loss = 0.19462000\n",
      "Iteration 86, loss = 0.19912310\n",
      "Iteration 87, loss = 0.19529029\n",
      "Iteration 88, loss = 0.18966986\n",
      "Iteration 89, loss = 0.18638194\n",
      "Iteration 90, loss = 0.19304464\n",
      "Iteration 91, loss = 0.18844672\n",
      "Iteration 92, loss = 0.19291435\n",
      "Iteration 93, loss = 0.18258908\n",
      "Iteration 94, loss = 0.18335262\n",
      "Iteration 95, loss = 0.17780589\n",
      "Iteration 96, loss = 0.17658392\n",
      "Iteration 97, loss = 0.17788295\n",
      "Iteration 98, loss = 0.17774756\n",
      "Iteration 99, loss = 0.17739758\n",
      "Iteration 100, loss = 0.17719742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.81852879\n",
      "Iteration 2, loss = 1.43372408\n",
      "Iteration 3, loss = 1.26058268\n",
      "Iteration 4, loss = 1.03001646\n",
      "Iteration 5, loss = 1.04987502\n",
      "Iteration 6, loss = 0.86091318\n",
      "Iteration 7, loss = 0.85378126\n",
      "Iteration 8, loss = 0.89605021\n",
      "Iteration 9, loss = 0.78982348\n",
      "Iteration 10, loss = 0.68396717\n",
      "Iteration 11, loss = 0.66816497\n",
      "Iteration 12, loss = 0.62967453\n",
      "Iteration 13, loss = 0.61265860\n",
      "Iteration 14, loss = 0.62352868\n",
      "Iteration 15, loss = 0.56071595\n",
      "Iteration 16, loss = 0.54093317\n",
      "Iteration 17, loss = 0.56218825\n",
      "Iteration 18, loss = 0.56498322\n",
      "Iteration 19, loss = 0.59741942\n",
      "Iteration 20, loss = 0.53743538\n",
      "Iteration 21, loss = 0.57074398\n",
      "Iteration 22, loss = 0.48863575\n",
      "Iteration 23, loss = 0.54906506\n",
      "Iteration 24, loss = 0.47238775\n",
      "Iteration 25, loss = 0.54811662\n",
      "Iteration 26, loss = 0.57407585\n",
      "Iteration 27, loss = 0.50718330\n",
      "Iteration 28, loss = 0.59396002\n",
      "Iteration 29, loss = 0.48393239\n",
      "Iteration 30, loss = 0.47293415\n",
      "Iteration 31, loss = 0.41220432\n",
      "Iteration 32, loss = 0.43740232\n",
      "Iteration 33, loss = 0.42630170\n",
      "Iteration 34, loss = 0.54974878\n",
      "Iteration 35, loss = 0.44275317\n",
      "Iteration 36, loss = 0.52661052\n",
      "Iteration 37, loss = 0.40677587\n",
      "Iteration 38, loss = 0.44152476\n",
      "Iteration 39, loss = 0.43304818\n",
      "Iteration 40, loss = 0.41650827\n",
      "Iteration 41, loss = 0.53694554\n",
      "Iteration 42, loss = 0.39868546\n",
      "Iteration 43, loss = 0.37912563\n",
      "Iteration 44, loss = 0.40673139\n",
      "Iteration 45, loss = 0.40924795\n",
      "Iteration 46, loss = 0.46182141\n",
      "Iteration 47, loss = 0.51424716\n",
      "Iteration 48, loss = 0.36986721\n",
      "Iteration 49, loss = 0.36652543\n",
      "Iteration 50, loss = 0.38950183\n",
      "Iteration 51, loss = 0.34829851\n",
      "Iteration 52, loss = 0.32719427\n",
      "Iteration 53, loss = 0.42254813\n",
      "Iteration 54, loss = 0.30330035\n",
      "Iteration 55, loss = 0.32450782\n",
      "Iteration 56, loss = 0.27002925\n",
      "Iteration 57, loss = 0.39169754\n",
      "Iteration 58, loss = 0.34313223\n",
      "Iteration 59, loss = 0.34622250\n",
      "Iteration 60, loss = 0.26859847\n",
      "Iteration 61, loss = 0.28934569\n",
      "Iteration 62, loss = 0.37166113\n",
      "Iteration 63, loss = 0.41036329\n",
      "Iteration 64, loss = 0.33340628\n",
      "Iteration 65, loss = 0.28912840\n",
      "Iteration 66, loss = 0.27910868\n",
      "Iteration 67, loss = 0.24841099\n",
      "Iteration 68, loss = 0.28270085\n",
      "Iteration 69, loss = 0.40591852\n",
      "Iteration 70, loss = 0.38577072\n",
      "Iteration 71, loss = 0.33616820\n",
      "Iteration 72, loss = 0.32641884\n",
      "Iteration 73, loss = 0.28722196\n",
      "Iteration 74, loss = 0.28913457\n",
      "Iteration 75, loss = 0.28064047\n",
      "Iteration 76, loss = 0.32833057\n",
      "Iteration 77, loss = 0.27909662\n",
      "Iteration 78, loss = 0.30689708\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 79, loss = 0.24584857\n",
      "Iteration 80, loss = 0.21055653\n",
      "Iteration 81, loss = 0.19518544\n",
      "Iteration 82, loss = 0.19751133\n",
      "Iteration 83, loss = 0.18861465\n",
      "Iteration 84, loss = 0.18779733\n",
      "Iteration 85, loss = 0.18785480\n",
      "Iteration 86, loss = 0.18064748\n",
      "Iteration 87, loss = 0.17725396\n",
      "Iteration 88, loss = 0.17409803\n",
      "Iteration 89, loss = 0.17179257\n",
      "Iteration 90, loss = 0.17183169\n",
      "Iteration 91, loss = 0.16957727\n",
      "Iteration 92, loss = 0.16846270\n",
      "Iteration 93, loss = 0.16772338\n",
      "Iteration 94, loss = 0.16544435\n",
      "Iteration 95, loss = 0.16411974\n",
      "Iteration 96, loss = 0.16320960\n",
      "Iteration 97, loss = 0.16267919\n",
      "Iteration 98, loss = 0.16290535\n",
      "Iteration 99, loss = 0.16169921\n",
      "Iteration 100, loss = 0.16241053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.72988570\n",
      "Iteration 2, loss = 1.13296545\n",
      "Iteration 3, loss = 0.87190507\n",
      "Iteration 4, loss = 0.72656513\n",
      "Iteration 5, loss = 0.70885811\n",
      "Iteration 6, loss = 0.66153995\n",
      "Iteration 7, loss = 0.59748076\n",
      "Iteration 8, loss = 0.57629623\n",
      "Iteration 9, loss = 0.63971054\n",
      "Iteration 10, loss = 0.56519926\n",
      "Iteration 11, loss = 0.55444790\n",
      "Iteration 12, loss = 0.48345294\n",
      "Iteration 13, loss = 0.50013931\n",
      "Iteration 14, loss = 0.53431480\n",
      "Iteration 15, loss = 0.55590120\n",
      "Iteration 16, loss = 0.44012522\n",
      "Iteration 17, loss = 0.46975922\n",
      "Iteration 18, loss = 0.46542659\n",
      "Iteration 19, loss = 0.45727552\n",
      "Iteration 20, loss = 0.39923730\n",
      "Iteration 21, loss = 0.43852557\n",
      "Iteration 22, loss = 0.32846267\n",
      "Iteration 23, loss = 0.38974019\n",
      "Iteration 24, loss = 0.60226131\n",
      "Iteration 25, loss = 0.49143664\n",
      "Iteration 26, loss = 0.55602256\n",
      "Iteration 27, loss = 0.46907435\n",
      "Iteration 28, loss = 0.43834486\n",
      "Iteration 29, loss = 0.44623984\n",
      "Iteration 30, loss = 0.38539190\n",
      "Iteration 31, loss = 0.34562118\n",
      "Iteration 32, loss = 0.26123397\n",
      "Iteration 33, loss = 0.24420886\n",
      "Iteration 34, loss = 0.42121716\n",
      "Iteration 35, loss = 0.35394209\n",
      "Iteration 36, loss = 0.31393550\n",
      "Iteration 37, loss = 0.41035510\n",
      "Iteration 38, loss = 0.33591977\n",
      "Iteration 39, loss = 0.38432543\n",
      "Iteration 40, loss = 0.37502318\n",
      "Iteration 41, loss = 0.31616974\n",
      "Iteration 42, loss = 0.26610395\n",
      "Iteration 43, loss = 0.27882259\n",
      "Iteration 44, loss = 0.29573078\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 45, loss = 0.32690077\n",
      "Iteration 46, loss = 0.24370068\n",
      "Iteration 47, loss = 0.20171391\n",
      "Iteration 48, loss = 0.19489558\n",
      "Iteration 49, loss = 0.18688972\n",
      "Iteration 50, loss = 0.18103375\n",
      "Iteration 51, loss = 0.16631781\n",
      "Iteration 52, loss = 0.15651065\n",
      "Iteration 53, loss = 0.15998716\n",
      "Iteration 54, loss = 0.15715825\n",
      "Iteration 55, loss = 0.15025154\n",
      "Iteration 56, loss = 0.14686340\n",
      "Iteration 57, loss = 0.14464731\n",
      "Iteration 58, loss = 0.14120247\n",
      "Iteration 59, loss = 0.14116446\n",
      "Iteration 60, loss = 0.13522779\n",
      "Iteration 61, loss = 0.13051423\n",
      "Iteration 62, loss = 0.12436634\n",
      "Iteration 63, loss = 0.12339898\n",
      "Iteration 64, loss = 0.12154573\n",
      "Iteration 65, loss = 0.12149521\n",
      "Iteration 66, loss = 0.12501529\n",
      "Iteration 67, loss = 0.11946955\n",
      "Iteration 68, loss = 0.11384158\n",
      "Iteration 69, loss = 0.11200473\n",
      "Iteration 70, loss = 0.11288919\n",
      "Iteration 71, loss = 0.10824688\n",
      "Iteration 72, loss = 0.10563150\n",
      "Iteration 73, loss = 0.10467032\n",
      "Iteration 74, loss = 0.10515046\n",
      "Iteration 75, loss = 0.10347992\n",
      "Iteration 76, loss = 0.10249449\n",
      "Iteration 77, loss = 0.10082380\n",
      "Iteration 78, loss = 0.10372792\n",
      "Iteration 79, loss = 0.10003099\n",
      "Iteration 80, loss = 0.09936426\n",
      "Iteration 81, loss = 0.09799975\n",
      "Iteration 82, loss = 0.09897566\n",
      "Iteration 83, loss = 0.09671531\n",
      "Iteration 84, loss = 0.09593184\n",
      "Iteration 85, loss = 0.09582480\n",
      "Iteration 86, loss = 0.09490726\n",
      "Iteration 87, loss = 0.09450080\n",
      "Iteration 88, loss = 0.09393971\n",
      "Iteration 89, loss = 0.09358346\n",
      "Iteration 90, loss = 0.09615250\n",
      "Iteration 91, loss = 0.09365201\n",
      "Iteration 92, loss = 0.09459822\n",
      "Iteration 93, loss = 0.09270094\n",
      "Iteration 94, loss = 0.09146043\n",
      "Iteration 95, loss = 0.09129755\n",
      "Iteration 96, loss = 0.09098160\n",
      "Iteration 97, loss = 0.09043010\n",
      "Iteration 98, loss = 0.09028315\n",
      "Iteration 99, loss = 0.09003353\n",
      "Iteration 100, loss = 0.09287055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.77348826\n",
      "Iteration 2, loss = 1.28736781\n",
      "Iteration 3, loss = 1.00859922\n",
      "Iteration 4, loss = 0.90879143\n",
      "Iteration 5, loss = 0.70644660\n",
      "Iteration 6, loss = 0.73278257\n",
      "Iteration 7, loss = 0.66431383\n",
      "Iteration 8, loss = 0.58623251\n",
      "Iteration 9, loss = 0.63149668\n",
      "Iteration 10, loss = 0.59105858\n",
      "Iteration 11, loss = 0.60226163\n",
      "Iteration 12, loss = 0.59222259\n",
      "Iteration 13, loss = 0.64154065\n",
      "Iteration 14, loss = 0.65082615\n",
      "Iteration 15, loss = 0.55564989\n",
      "Iteration 16, loss = 0.52267973\n",
      "Iteration 17, loss = 0.43009777\n",
      "Iteration 18, loss = 0.49087154\n",
      "Iteration 19, loss = 0.40952068\n",
      "Iteration 20, loss = 0.42389940\n",
      "Iteration 21, loss = 0.52190341\n",
      "Iteration 22, loss = 0.50747470\n",
      "Iteration 23, loss = 0.49215652\n",
      "Iteration 24, loss = 0.47624589\n",
      "Iteration 25, loss = 0.47482959\n",
      "Iteration 26, loss = 0.44665542\n",
      "Iteration 27, loss = 0.41661421\n",
      "Iteration 28, loss = 0.32271161\n",
      "Iteration 29, loss = 0.36392920\n",
      "Iteration 30, loss = 0.33208943\n",
      "Iteration 31, loss = 0.35567163\n",
      "Iteration 32, loss = 0.37255005\n",
      "Iteration 33, loss = 0.45161959\n",
      "Iteration 34, loss = 0.41106482\n",
      "Iteration 35, loss = 0.31372683\n",
      "Iteration 36, loss = 0.30775214\n",
      "Iteration 37, loss = 0.34809914\n",
      "Iteration 38, loss = 0.30135142\n",
      "Iteration 39, loss = 0.31040012\n",
      "Iteration 40, loss = 0.37712366\n",
      "Iteration 41, loss = 0.33960568\n",
      "Iteration 42, loss = 0.27880990\n",
      "Iteration 43, loss = 0.29917192\n",
      "Iteration 44, loss = 0.26519106\n",
      "Iteration 45, loss = 0.30294842\n",
      "Iteration 46, loss = 0.27942960\n",
      "Iteration 47, loss = 0.26560537\n",
      "Iteration 48, loss = 0.31883599\n",
      "Iteration 49, loss = 0.35017002\n",
      "Iteration 50, loss = 0.35042263\n",
      "Iteration 51, loss = 0.23187353\n",
      "Iteration 52, loss = 0.25432014\n",
      "Iteration 53, loss = 0.31080214\n",
      "Iteration 54, loss = 0.24464587\n",
      "Iteration 55, loss = 0.30584659\n",
      "Iteration 56, loss = 0.24112579\n",
      "Iteration 57, loss = 0.29969692\n",
      "Iteration 58, loss = 0.27828434\n",
      "Iteration 59, loss = 0.31101023\n",
      "Iteration 60, loss = 0.22982228\n",
      "Iteration 61, loss = 0.30246365\n",
      "Iteration 62, loss = 0.28922943\n",
      "Iteration 63, loss = 0.27266742\n",
      "Iteration 64, loss = 0.23571862\n",
      "Iteration 65, loss = 0.23409156\n",
      "Iteration 66, loss = 0.22703060\n",
      "Iteration 67, loss = 0.24685030\n",
      "Iteration 68, loss = 0.22712918\n",
      "Iteration 69, loss = 0.21537050\n",
      "Iteration 70, loss = 0.28521579\n",
      "Iteration 71, loss = 0.31114522\n",
      "Iteration 72, loss = 0.22005618\n",
      "Iteration 73, loss = 0.28348014\n",
      "Iteration 74, loss = 0.32249043\n",
      "Iteration 75, loss = 0.27396026\n",
      "Iteration 76, loss = 0.29376903\n",
      "Iteration 77, loss = 0.28875123\n",
      "Iteration 78, loss = 0.28419104\n",
      "Iteration 79, loss = 0.20877655\n",
      "Iteration 80, loss = 0.23776572\n",
      "Iteration 81, loss = 0.24125113\n",
      "Iteration 82, loss = 0.19696711\n",
      "Iteration 83, loss = 0.21136814\n",
      "Iteration 84, loss = 0.18168187\n",
      "Iteration 85, loss = 0.22453726\n",
      "Iteration 86, loss = 0.21517030\n",
      "Iteration 87, loss = 0.21726136\n",
      "Iteration 88, loss = 0.23062494\n",
      "Iteration 89, loss = 0.24421191\n",
      "Iteration 90, loss = 0.24097845\n",
      "Iteration 91, loss = 0.31353494\n",
      "Iteration 92, loss = 0.24574085\n",
      "Iteration 93, loss = 0.32900668\n",
      "Iteration 94, loss = 0.30358790\n",
      "Iteration 95, loss = 0.25702872\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 96, loss = 0.20715671\n",
      "Iteration 97, loss = 0.17101355\n",
      "Iteration 98, loss = 0.16458504\n",
      "Iteration 99, loss = 0.15595657\n",
      "Iteration 100, loss = 0.15030552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.75595490\n",
      "Iteration 2, loss = 1.11836576\n",
      "Iteration 3, loss = 0.92573439\n",
      "Iteration 4, loss = 0.78450185\n",
      "Iteration 5, loss = 0.69194167\n",
      "Iteration 6, loss = 0.62869687\n",
      "Iteration 7, loss = 0.71961220\n",
      "Iteration 8, loss = 0.80073932\n",
      "Iteration 9, loss = 0.63435229\n",
      "Iteration 10, loss = 0.56267331\n",
      "Iteration 11, loss = 0.50183096\n",
      "Iteration 12, loss = 0.49335660\n",
      "Iteration 13, loss = 0.61788295\n",
      "Iteration 14, loss = 0.55543434\n",
      "Iteration 15, loss = 0.49708310\n",
      "Iteration 16, loss = 0.46122680\n",
      "Iteration 17, loss = 0.49994591\n",
      "Iteration 18, loss = 0.46830302\n",
      "Iteration 19, loss = 0.47739778\n",
      "Iteration 20, loss = 0.41973750\n",
      "Iteration 21, loss = 0.39205645\n",
      "Iteration 22, loss = 0.38497325\n",
      "Iteration 23, loss = 0.53969347\n",
      "Iteration 24, loss = 0.39755757\n",
      "Iteration 25, loss = 0.33116621\n",
      "Iteration 26, loss = 0.37869257\n",
      "Iteration 27, loss = 0.45373956\n",
      "Iteration 28, loss = 0.54216323\n",
      "Iteration 29, loss = 0.51362080\n",
      "Iteration 30, loss = 0.55794718\n",
      "Iteration 31, loss = 0.32501888\n",
      "Iteration 32, loss = 0.30517086\n",
      "Iteration 33, loss = 0.34236685\n",
      "Iteration 34, loss = 0.33674480\n",
      "Iteration 35, loss = 0.39873581\n",
      "Iteration 36, loss = 0.28445185\n",
      "Iteration 37, loss = 0.43653618\n",
      "Iteration 38, loss = 0.28857549\n",
      "Iteration 39, loss = 0.30037517\n",
      "Iteration 40, loss = 0.29926284\n",
      "Iteration 41, loss = 0.29755286\n",
      "Iteration 42, loss = 0.29205287\n",
      "Iteration 43, loss = 0.29929193\n",
      "Iteration 44, loss = 0.28592266\n",
      "Iteration 45, loss = 0.26084347\n",
      "Iteration 46, loss = 0.34038704\n",
      "Iteration 47, loss = 0.25978325\n",
      "Iteration 48, loss = 0.29956856\n",
      "Iteration 49, loss = 0.39577797\n",
      "Iteration 50, loss = 0.25678309\n",
      "Iteration 51, loss = 0.31007781\n",
      "Iteration 52, loss = 0.26579744\n",
      "Iteration 53, loss = 0.30772531\n",
      "Iteration 54, loss = 0.24562552\n",
      "Iteration 55, loss = 0.24029201\n",
      "Iteration 56, loss = 0.34102213\n",
      "Iteration 57, loss = 0.32852471\n",
      "Iteration 58, loss = 0.32822284\n",
      "Iteration 59, loss = 0.23553684\n",
      "Iteration 60, loss = 0.48384692\n",
      "Iteration 61, loss = 0.34429008\n",
      "Iteration 62, loss = 0.41402225\n",
      "Iteration 63, loss = 0.28448371\n",
      "Iteration 64, loss = 0.27676650\n",
      "Iteration 65, loss = 0.26126855\n",
      "Iteration 66, loss = 0.28567555\n",
      "Iteration 67, loss = 0.28763549\n",
      "Iteration 68, loss = 0.30023736\n",
      "Iteration 69, loss = 0.38671222\n",
      "Iteration 70, loss = 0.24142879\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 71, loss = 0.20020179\n",
      "Iteration 72, loss = 0.18837089\n",
      "Iteration 73, loss = 0.18095146\n",
      "Iteration 74, loss = 0.17846410\n",
      "Iteration 75, loss = 0.16661858\n",
      "Iteration 76, loss = 0.17189566\n",
      "Iteration 77, loss = 0.17050902\n",
      "Iteration 78, loss = 0.16350058\n",
      "Iteration 79, loss = 0.15891934\n",
      "Iteration 80, loss = 0.16411421\n",
      "Iteration 81, loss = 0.15987636\n",
      "Iteration 82, loss = 0.15334619\n",
      "Iteration 83, loss = 0.15305610\n",
      "Iteration 84, loss = 0.15265789\n",
      "Iteration 85, loss = 0.15055268\n",
      "Iteration 86, loss = 0.15372853\n",
      "Iteration 87, loss = 0.15080000\n",
      "Iteration 88, loss = 0.14901234\n",
      "Iteration 89, loss = 0.14846679\n",
      "Iteration 90, loss = 0.14772862\n",
      "Iteration 91, loss = 0.14720476\n",
      "Iteration 92, loss = 0.14563604\n",
      "Iteration 93, loss = 0.14540527\n",
      "Iteration 94, loss = 0.14517610\n",
      "Iteration 95, loss = 0.14474242\n",
      "Iteration 96, loss = 0.14475383\n",
      "Iteration 97, loss = 0.14440767\n",
      "Iteration 98, loss = 0.14402619\n",
      "Iteration 99, loss = 0.14366350\n",
      "Iteration 100, loss = 0.14418781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.63753551\n",
      "Iteration 2, loss = 0.89336749\n",
      "Iteration 3, loss = 0.70085567\n",
      "Iteration 4, loss = 0.75875675\n",
      "Iteration 5, loss = 0.79770263\n",
      "Iteration 6, loss = 0.67409758\n",
      "Iteration 7, loss = 0.64687616\n",
      "Iteration 8, loss = 0.62724008\n",
      "Iteration 9, loss = 0.51444583\n",
      "Iteration 10, loss = 0.52610214\n",
      "Iteration 11, loss = 0.56837536\n",
      "Iteration 12, loss = 0.43875413\n",
      "Iteration 13, loss = 0.50659511\n",
      "Iteration 14, loss = 0.46085412\n",
      "Iteration 15, loss = 0.62609187\n",
      "Iteration 16, loss = 0.37377712\n",
      "Iteration 17, loss = 0.38223015\n",
      "Iteration 18, loss = 0.46794301\n",
      "Iteration 19, loss = 0.45119466\n",
      "Iteration 20, loss = 0.39459673\n",
      "Iteration 21, loss = 0.34599882\n",
      "Iteration 22, loss = 0.34994592\n",
      "Iteration 23, loss = 0.31671661\n",
      "Iteration 24, loss = 0.29700068\n",
      "Iteration 25, loss = 0.38563211\n",
      "Iteration 26, loss = 0.41960811\n",
      "Iteration 27, loss = 0.35612671\n",
      "Iteration 28, loss = 0.29499359\n",
      "Iteration 29, loss = 0.40925585\n",
      "Iteration 30, loss = 0.29401613\n",
      "Iteration 31, loss = 0.36999938\n",
      "Iteration 32, loss = 0.30331348\n",
      "Iteration 33, loss = 0.33040695\n",
      "Iteration 34, loss = 0.40316835\n",
      "Iteration 35, loss = 0.30156009\n",
      "Iteration 36, loss = 0.45427126\n",
      "Iteration 37, loss = 0.34747456\n",
      "Iteration 38, loss = 0.23532980\n",
      "Iteration 39, loss = 0.30651376\n",
      "Iteration 40, loss = 0.33671932\n",
      "Iteration 41, loss = 0.28457996\n",
      "Iteration 42, loss = 0.24999328\n",
      "Iteration 43, loss = 0.33023527\n",
      "Iteration 44, loss = 0.19034960\n",
      "Iteration 45, loss = 0.29789491\n",
      "Iteration 46, loss = 0.42391898\n",
      "Iteration 47, loss = 0.36294539\n",
      "Iteration 48, loss = 0.33778131\n",
      "Iteration 49, loss = 0.31197490\n",
      "Iteration 50, loss = 0.26117878\n",
      "Iteration 51, loss = 0.25852630\n",
      "Iteration 52, loss = 0.26833930\n",
      "Iteration 53, loss = 0.25810941\n",
      "Iteration 54, loss = 0.27538931\n",
      "Iteration 55, loss = 0.23088394\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 56, loss = 0.18328520\n",
      "Iteration 57, loss = 0.15324917\n",
      "Iteration 58, loss = 0.14739605\n",
      "Iteration 59, loss = 0.14421990\n",
      "Iteration 60, loss = 0.14595026\n",
      "Iteration 61, loss = 0.14334795\n",
      "Iteration 62, loss = 0.14165815\n",
      "Iteration 63, loss = 0.13856612\n",
      "Iteration 64, loss = 0.13847853\n",
      "Iteration 65, loss = 0.13764921\n",
      "Iteration 66, loss = 0.13701662\n",
      "Iteration 67, loss = 0.13609328\n",
      "Iteration 68, loss = 0.13545648\n",
      "Iteration 69, loss = 0.13447792\n",
      "Iteration 70, loss = 0.13404640\n",
      "Iteration 71, loss = 0.13382821\n",
      "Iteration 72, loss = 0.13338492\n",
      "Iteration 73, loss = 0.13288947\n",
      "Iteration 74, loss = 0.13316085\n",
      "Iteration 75, loss = 0.13291137\n",
      "Iteration 76, loss = 0.13179466\n",
      "Iteration 77, loss = 0.13107199\n",
      "Iteration 78, loss = 0.12920560\n",
      "Iteration 79, loss = 0.12888142\n",
      "Iteration 80, loss = 0.12873722\n",
      "Iteration 81, loss = 0.12818572\n",
      "Iteration 82, loss = 0.12802885\n",
      "Iteration 83, loss = 0.12761343\n",
      "Iteration 84, loss = 0.12711355\n",
      "Iteration 85, loss = 0.12858689\n",
      "Iteration 86, loss = 0.12622988\n",
      "Iteration 87, loss = 0.12588272\n",
      "Iteration 88, loss = 0.12534294\n",
      "Iteration 89, loss = 0.12584257\n",
      "Iteration 90, loss = 0.12524297\n",
      "Iteration 91, loss = 0.12495876\n",
      "Iteration 92, loss = 0.12547341\n",
      "Iteration 93, loss = 0.12293021\n",
      "Iteration 94, loss = 0.12251190\n",
      "Iteration 95, loss = 0.12163440\n",
      "Iteration 96, loss = 0.12127662\n",
      "Iteration 97, loss = 0.12189984\n",
      "Iteration 98, loss = 0.12022181\n",
      "Iteration 99, loss = 0.11999574\n",
      "Iteration 100, loss = 0.12208175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.96424922\n",
      "Iteration 2, loss = 1.23561002\n",
      "Iteration 3, loss = 0.86680775\n",
      "Iteration 4, loss = 0.68745633\n",
      "Iteration 5, loss = 0.60580368\n",
      "Iteration 6, loss = 0.52087745\n",
      "Iteration 7, loss = 0.54489517\n",
      "Iteration 8, loss = 0.42112133\n",
      "Iteration 9, loss = 0.59605062\n",
      "Iteration 10, loss = 0.41065623\n",
      "Iteration 11, loss = 0.34661999\n",
      "Iteration 12, loss = 0.33898009\n",
      "Iteration 13, loss = 0.46399563\n",
      "Iteration 14, loss = 0.45988590\n",
      "Iteration 15, loss = 0.30774222\n",
      "Iteration 16, loss = 0.33744997\n",
      "Iteration 17, loss = 0.24745237\n",
      "Iteration 18, loss = 0.21969009\n",
      "Iteration 19, loss = 0.30972229\n",
      "Iteration 20, loss = 0.24322021\n",
      "Iteration 21, loss = 0.31478691\n",
      "Iteration 22, loss = 0.28820032\n",
      "Iteration 23, loss = 0.30079709\n",
      "Iteration 24, loss = 0.29574571\n",
      "Iteration 25, loss = 0.33665191\n",
      "Iteration 26, loss = 0.28585789\n",
      "Iteration 27, loss = 0.32203519\n",
      "Iteration 28, loss = 0.30878570\n",
      "Iteration 29, loss = 0.36711372\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 30, loss = 0.22108375\n",
      "Iteration 31, loss = 0.19521111\n",
      "Iteration 32, loss = 0.17063018\n",
      "Iteration 33, loss = 0.15800582\n",
      "Iteration 34, loss = 0.15480580\n",
      "Iteration 35, loss = 0.14648427\n",
      "Iteration 36, loss = 0.14363722\n",
      "Iteration 37, loss = 0.13581197\n",
      "Iteration 38, loss = 0.13522725\n",
      "Iteration 39, loss = 0.13202746\n",
      "Iteration 40, loss = 0.12695448\n",
      "Iteration 41, loss = 0.13246377\n",
      "Iteration 42, loss = 0.12693319\n",
      "Iteration 43, loss = 0.12168750\n",
      "Iteration 44, loss = 0.11179912\n",
      "Iteration 45, loss = 0.11315508\n",
      "Iteration 46, loss = 0.11272853\n",
      "Iteration 47, loss = 0.10816895\n",
      "Iteration 48, loss = 0.10645702\n",
      "Iteration 49, loss = 0.10345272\n",
      "Iteration 50, loss = 0.10853257\n",
      "Iteration 51, loss = 0.10385041\n",
      "Iteration 52, loss = 0.10203571\n",
      "Iteration 53, loss = 0.10542505\n",
      "Iteration 54, loss = 0.10488250\n",
      "Iteration 55, loss = 0.09850802\n",
      "Iteration 56, loss = 0.09768859\n",
      "Iteration 57, loss = 0.09719867\n",
      "Iteration 58, loss = 0.09712597\n",
      "Iteration 59, loss = 0.09590863\n",
      "Iteration 60, loss = 0.09533170\n",
      "Iteration 61, loss = 0.09596941\n",
      "Iteration 62, loss = 0.09473637\n",
      "Iteration 63, loss = 0.09292381\n",
      "Iteration 64, loss = 0.09247752\n",
      "Iteration 65, loss = 0.09184846\n",
      "Iteration 66, loss = 0.09138586\n",
      "Iteration 67, loss = 0.09140733\n",
      "Iteration 68, loss = 0.09049808\n",
      "Iteration 69, loss = 0.09050408\n",
      "Iteration 70, loss = 0.08989046\n",
      "Iteration 71, loss = 0.08917274\n",
      "Iteration 72, loss = 0.08983453\n",
      "Iteration 73, loss = 0.08901612\n",
      "Iteration 74, loss = 0.08818964\n",
      "Iteration 75, loss = 0.08750324\n",
      "Iteration 76, loss = 0.08700350\n",
      "Iteration 77, loss = 0.08664655\n",
      "Iteration 78, loss = 0.08619446\n",
      "Iteration 79, loss = 0.08639568\n",
      "Iteration 80, loss = 0.08598497\n",
      "Iteration 81, loss = 0.08588693\n",
      "Iteration 82, loss = 0.08531168\n",
      "Iteration 83, loss = 0.08508133\n",
      "Iteration 84, loss = 0.08446701\n",
      "Iteration 85, loss = 0.08391781\n",
      "Iteration 86, loss = 0.08345099\n",
      "Iteration 87, loss = 0.08308233\n",
      "Iteration 88, loss = 0.08280757\n",
      "Iteration 89, loss = 0.08257392\n",
      "Iteration 90, loss = 0.08236388\n",
      "Iteration 91, loss = 0.08190689\n",
      "Iteration 92, loss = 0.08173948\n",
      "Iteration 93, loss = 0.08156881\n",
      "Iteration 94, loss = 0.08135512\n",
      "Iteration 95, loss = 0.08107414\n",
      "Iteration 96, loss = 0.08090551\n",
      "Iteration 97, loss = 0.08087488\n",
      "Iteration 98, loss = 0.08052121\n",
      "Iteration 99, loss = 0.08026510\n",
      "Iteration 100, loss = 0.08066108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.76144106\n",
      "Iteration 2, loss = 1.29930564\n",
      "Iteration 3, loss = 0.95406608\n",
      "Iteration 4, loss = 0.91057413\n",
      "Iteration 5, loss = 0.81212052\n",
      "Iteration 6, loss = 0.76279999\n",
      "Iteration 7, loss = 0.77320922\n",
      "Iteration 8, loss = 0.67759734\n",
      "Iteration 9, loss = 0.68956684\n",
      "Iteration 10, loss = 0.61352112\n",
      "Iteration 11, loss = 0.65199557\n",
      "Iteration 12, loss = 0.64548429\n",
      "Iteration 13, loss = 0.65451943\n",
      "Iteration 14, loss = 0.56569184\n",
      "Iteration 15, loss = 0.53128797\n",
      "Iteration 16, loss = 0.46175859\n",
      "Iteration 17, loss = 0.55005339\n",
      "Iteration 18, loss = 0.58794739\n",
      "Iteration 19, loss = 0.53008929\n",
      "Iteration 20, loss = 0.42345792\n",
      "Iteration 21, loss = 0.47036204\n",
      "Iteration 22, loss = 0.48225629\n",
      "Iteration 23, loss = 0.41238664\n",
      "Iteration 24, loss = 0.44619409\n",
      "Iteration 25, loss = 0.47503051\n",
      "Iteration 26, loss = 0.45685461\n",
      "Iteration 27, loss = 0.39415706\n",
      "Iteration 28, loss = 0.37365707\n",
      "Iteration 29, loss = 0.34688284\n",
      "Iteration 30, loss = 0.34962312\n",
      "Iteration 31, loss = 0.34898115\n",
      "Iteration 32, loss = 0.34926333\n",
      "Iteration 33, loss = 0.31463772\n",
      "Iteration 34, loss = 0.31371247\n",
      "Iteration 35, loss = 0.33623324\n",
      "Iteration 36, loss = 0.45033454\n",
      "Iteration 37, loss = 0.31301286\n",
      "Iteration 38, loss = 0.30123690\n",
      "Iteration 39, loss = 0.38594498\n",
      "Iteration 40, loss = 0.25091096\n",
      "Iteration 41, loss = 0.35106511\n",
      "Iteration 42, loss = 0.45659430\n",
      "Iteration 43, loss = 0.39984675\n",
      "Iteration 44, loss = 0.29271417\n",
      "Iteration 45, loss = 0.26119670\n",
      "Iteration 46, loss = 0.22860705\n",
      "Iteration 47, loss = 0.29485083\n",
      "Iteration 48, loss = 0.28219356\n",
      "Iteration 49, loss = 0.21775755\n",
      "Iteration 50, loss = 0.22999147\n",
      "Iteration 51, loss = 0.25749241\n",
      "Iteration 52, loss = 0.30106487\n",
      "Iteration 53, loss = 0.29145743\n",
      "Iteration 54, loss = 0.31197459\n",
      "Iteration 55, loss = 0.37949418\n",
      "Iteration 56, loss = 0.27890227\n",
      "Iteration 57, loss = 0.27624440\n",
      "Iteration 58, loss = 0.28921477\n",
      "Iteration 59, loss = 0.27898886\n",
      "Iteration 60, loss = 0.28779421\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 61, loss = 0.27557419\n",
      "Iteration 62, loss = 0.24565449\n",
      "Iteration 63, loss = 0.25481677\n",
      "Iteration 64, loss = 0.23562879\n",
      "Iteration 65, loss = 0.22193219\n",
      "Iteration 66, loss = 0.21929657\n",
      "Iteration 67, loss = 0.19893288\n",
      "Iteration 68, loss = 0.20033407\n",
      "Iteration 69, loss = 0.19351597\n",
      "Iteration 70, loss = 0.18796428\n",
      "Iteration 71, loss = 0.18139125\n",
      "Iteration 72, loss = 0.18134405\n",
      "Iteration 73, loss = 0.17453734\n",
      "Iteration 74, loss = 0.19137460\n",
      "Iteration 75, loss = 0.16409634\n",
      "Iteration 76, loss = 0.16265383\n",
      "Iteration 77, loss = 0.15607932\n",
      "Iteration 78, loss = 0.15652883\n",
      "Iteration 79, loss = 0.16776297\n",
      "Iteration 80, loss = 0.15966776\n",
      "Iteration 81, loss = 0.15828495\n",
      "Iteration 82, loss = 0.15222872\n",
      "Iteration 83, loss = 0.15080110\n",
      "Iteration 84, loss = 0.15011780\n",
      "Iteration 85, loss = 0.14977999\n",
      "Iteration 86, loss = 0.15018508\n",
      "Iteration 87, loss = 0.14892045\n",
      "Iteration 88, loss = 0.14756309\n",
      "Iteration 89, loss = 0.14771201\n",
      "Iteration 90, loss = 0.14615128\n",
      "Iteration 91, loss = 0.14650276\n",
      "Iteration 92, loss = 0.14606177\n",
      "Iteration 93, loss = 0.14597606\n",
      "Iteration 94, loss = 0.14616946\n",
      "Iteration 95, loss = 0.14325149\n",
      "Iteration 96, loss = 0.14089186\n",
      "Iteration 97, loss = 0.13876259\n",
      "Iteration 98, loss = 0.13681319\n",
      "Iteration 99, loss = 0.13586372\n",
      "Iteration 100, loss = 0.13621303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.73944522\n",
      "Iteration 2, loss = 1.09570720\n",
      "Iteration 3, loss = 0.83777991\n",
      "Iteration 4, loss = 0.68100213\n",
      "Iteration 5, loss = 0.67589685\n",
      "Iteration 6, loss = 0.54060237\n",
      "Iteration 7, loss = 0.50151357\n",
      "Iteration 8, loss = 0.48003081\n",
      "Iteration 9, loss = 0.53833512\n",
      "Iteration 10, loss = 0.39976414\n",
      "Iteration 11, loss = 0.39596878\n",
      "Iteration 12, loss = 0.31294595\n",
      "Iteration 13, loss = 0.30399887\n",
      "Iteration 14, loss = 0.39858946\n",
      "Iteration 15, loss = 0.32893132\n",
      "Iteration 16, loss = 0.39884392\n",
      "Iteration 17, loss = 0.31218161\n",
      "Iteration 18, loss = 0.26771198\n",
      "Iteration 19, loss = 0.34797611\n",
      "Iteration 20, loss = 0.40919748\n",
      "Iteration 21, loss = 0.29691991\n",
      "Iteration 22, loss = 0.38708135\n",
      "Iteration 23, loss = 0.30680089\n",
      "Iteration 24, loss = 0.24814513\n",
      "Iteration 25, loss = 0.26395040\n",
      "Iteration 26, loss = 0.29759243\n",
      "Iteration 27, loss = 0.25921946\n",
      "Iteration 28, loss = 0.31156340\n",
      "Iteration 29, loss = 0.21043741\n",
      "Iteration 30, loss = 0.17728046\n",
      "Iteration 31, loss = 0.30257535\n",
      "Iteration 32, loss = 0.19011832\n",
      "Iteration 33, loss = 0.17864690\n",
      "Iteration 34, loss = 0.19741402\n",
      "Iteration 35, loss = 0.18057764\n",
      "Iteration 36, loss = 0.21640305\n",
      "Iteration 37, loss = 0.18955034\n",
      "Iteration 38, loss = 0.19093664\n",
      "Iteration 39, loss = 0.21870924\n",
      "Iteration 40, loss = 0.28572513\n",
      "Iteration 41, loss = 0.18108870\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 42, loss = 0.14651096\n",
      "Iteration 43, loss = 0.13113559\n",
      "Iteration 44, loss = 0.11876322\n",
      "Iteration 45, loss = 0.13485643\n",
      "Iteration 46, loss = 0.11432360\n",
      "Iteration 47, loss = 0.10266247\n",
      "Iteration 48, loss = 0.09691367\n",
      "Iteration 49, loss = 0.10851726\n",
      "Iteration 50, loss = 0.09341312\n",
      "Iteration 51, loss = 0.09732693\n",
      "Iteration 52, loss = 0.10303429\n",
      "Iteration 53, loss = 0.08932869\n",
      "Iteration 54, loss = 0.08476684\n",
      "Iteration 55, loss = 0.08394365\n",
      "Iteration 56, loss = 0.08362942\n",
      "Iteration 57, loss = 0.08268578\n",
      "Iteration 58, loss = 0.08360869\n",
      "Iteration 59, loss = 0.08156180\n",
      "Iteration 60, loss = 0.08083520\n",
      "Iteration 61, loss = 0.08024111\n",
      "Iteration 62, loss = 0.07972437\n",
      "Iteration 63, loss = 0.07965904\n",
      "Iteration 64, loss = 0.07902602\n",
      "Iteration 65, loss = 0.07843495\n",
      "Iteration 66, loss = 0.07813116\n",
      "Iteration 67, loss = 0.07775686\n",
      "Iteration 68, loss = 0.07741670\n",
      "Iteration 69, loss = 0.07810581\n",
      "Iteration 70, loss = 0.07643156\n",
      "Iteration 71, loss = 0.07572213\n",
      "Iteration 72, loss = 0.07587885\n",
      "Iteration 73, loss = 0.07495994\n",
      "Iteration 74, loss = 0.07396506\n",
      "Iteration 75, loss = 0.07414118\n",
      "Iteration 76, loss = 0.07377574\n",
      "Iteration 77, loss = 0.07385853\n",
      "Iteration 78, loss = 0.07322394\n",
      "Iteration 79, loss = 0.07282901\n",
      "Iteration 80, loss = 0.07270164\n",
      "Iteration 81, loss = 0.07201524\n",
      "Iteration 82, loss = 0.07203918\n",
      "Iteration 83, loss = 0.07139195\n",
      "Iteration 84, loss = 0.07106688\n",
      "Iteration 85, loss = 0.07087082\n",
      "Iteration 86, loss = 0.07072763\n",
      "Iteration 87, loss = 0.07060610\n",
      "Iteration 88, loss = 0.07028064\n",
      "Iteration 89, loss = 0.07020924\n",
      "Iteration 90, loss = 0.06977146\n",
      "Iteration 91, loss = 0.06966693\n",
      "Iteration 92, loss = 0.06961925\n",
      "Iteration 93, loss = 0.06952621\n",
      "Iteration 94, loss = 0.06910601\n",
      "Iteration 95, loss = 0.06915322\n",
      "Iteration 96, loss = 0.06879577\n",
      "Iteration 97, loss = 0.06867869\n",
      "Iteration 98, loss = 0.06844695\n",
      "Iteration 99, loss = 0.06806311\n",
      "Iteration 100, loss = 0.06791466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.68276675\n",
      "Iteration 2, loss = 1.11015449\n",
      "Iteration 3, loss = 0.91945909\n",
      "Iteration 4, loss = 0.83625621\n",
      "Iteration 5, loss = 0.63306105\n",
      "Iteration 6, loss = 0.61875720\n",
      "Iteration 7, loss = 0.74949554\n",
      "Iteration 8, loss = 0.62830473\n",
      "Iteration 9, loss = 0.43968917\n",
      "Iteration 10, loss = 0.50369658\n",
      "Iteration 11, loss = 0.44792308\n",
      "Iteration 12, loss = 0.36405250\n",
      "Iteration 13, loss = 0.47759197\n",
      "Iteration 14, loss = 0.41138795\n",
      "Iteration 15, loss = 0.45353475\n",
      "Iteration 16, loss = 0.47601271\n",
      "Iteration 17, loss = 0.39014792\n",
      "Iteration 18, loss = 0.40365099\n",
      "Iteration 19, loss = 0.33743302\n",
      "Iteration 20, loss = 0.34043206\n",
      "Iteration 21, loss = 0.39599236\n",
      "Iteration 22, loss = 0.31037030\n",
      "Iteration 23, loss = 0.30017664\n",
      "Iteration 24, loss = 0.33205036\n",
      "Iteration 25, loss = 0.37282323\n",
      "Iteration 26, loss = 0.28055561\n",
      "Iteration 27, loss = 0.26508778\n",
      "Iteration 28, loss = 0.31976400\n",
      "Iteration 29, loss = 0.29732786\n",
      "Iteration 30, loss = 0.38547363\n",
      "Iteration 31, loss = 0.37946544\n",
      "Iteration 32, loss = 0.30369685\n",
      "Iteration 33, loss = 0.29940975\n",
      "Iteration 34, loss = 0.32087964\n",
      "Iteration 35, loss = 0.27771440\n",
      "Iteration 36, loss = 0.31254358\n",
      "Iteration 37, loss = 0.25391157\n",
      "Iteration 38, loss = 0.31554684\n",
      "Iteration 39, loss = 0.30025754\n",
      "Iteration 40, loss = 0.40595888\n",
      "Iteration 41, loss = 0.23956316\n",
      "Iteration 42, loss = 0.23739211\n",
      "Iteration 43, loss = 0.17185949\n",
      "Iteration 44, loss = 0.21257805\n",
      "Iteration 45, loss = 0.17850189\n",
      "Iteration 46, loss = 0.25834869\n",
      "Iteration 47, loss = 0.28816103\n",
      "Iteration 48, loss = 0.24290046\n",
      "Iteration 49, loss = 0.23444004\n",
      "Iteration 50, loss = 0.18598938\n",
      "Iteration 51, loss = 0.26127700\n",
      "Iteration 52, loss = 0.24137370\n",
      "Iteration 53, loss = 0.19057855\n",
      "Iteration 54, loss = 0.21639029\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 55, loss = 0.14871263\n",
      "Iteration 56, loss = 0.12287413\n",
      "Iteration 57, loss = 0.12023137\n",
      "Iteration 58, loss = 0.11148085\n",
      "Iteration 59, loss = 0.10805600\n",
      "Iteration 60, loss = 0.10579337\n",
      "Iteration 61, loss = 0.10745503\n",
      "Iteration 62, loss = 0.10550726\n",
      "Iteration 63, loss = 0.10031279\n",
      "Iteration 64, loss = 0.09307284\n",
      "Iteration 65, loss = 0.09058631\n",
      "Iteration 66, loss = 0.08960718\n",
      "Iteration 67, loss = 0.08799669\n",
      "Iteration 68, loss = 0.08717626\n",
      "Iteration 69, loss = 0.08644518\n",
      "Iteration 70, loss = 0.08619470\n",
      "Iteration 71, loss = 0.08587343\n",
      "Iteration 72, loss = 0.08372487\n",
      "Iteration 73, loss = 0.08353524\n",
      "Iteration 74, loss = 0.08279053\n",
      "Iteration 75, loss = 0.08230856\n",
      "Iteration 76, loss = 0.08182133\n",
      "Iteration 77, loss = 0.08143788\n",
      "Iteration 78, loss = 0.08114006\n",
      "Iteration 79, loss = 0.08093627\n",
      "Iteration 80, loss = 0.08031958\n",
      "Iteration 81, loss = 0.08007871\n",
      "Iteration 82, loss = 0.07972991\n",
      "Iteration 83, loss = 0.07941196\n",
      "Iteration 84, loss = 0.07910555\n",
      "Iteration 85, loss = 0.07878286\n",
      "Iteration 86, loss = 0.07839426\n",
      "Iteration 87, loss = 0.07827936\n",
      "Iteration 88, loss = 0.07785804\n",
      "Iteration 89, loss = 0.07764220\n",
      "Iteration 90, loss = 0.07740553\n",
      "Iteration 91, loss = 0.07718019\n",
      "Iteration 92, loss = 0.07693789\n",
      "Iteration 93, loss = 0.07667836\n",
      "Iteration 94, loss = 0.07633863\n",
      "Iteration 95, loss = 0.07683414\n",
      "Iteration 96, loss = 0.07654333\n",
      "Iteration 97, loss = 0.07628917\n",
      "Iteration 98, loss = 0.07599037\n",
      "Iteration 99, loss = 0.07574888\n",
      "Iteration 100, loss = 0.07524252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.97773079\n",
      "Iteration 2, loss = 1.41361646\n",
      "Iteration 3, loss = 0.92382358\n",
      "Iteration 4, loss = 0.83079444\n",
      "Iteration 5, loss = 0.64743503\n",
      "Iteration 6, loss = 0.63923388\n",
      "Iteration 7, loss = 0.52930507\n",
      "Iteration 8, loss = 0.58200849\n",
      "Iteration 9, loss = 0.53122784\n",
      "Iteration 10, loss = 0.48219296\n",
      "Iteration 11, loss = 0.47979788\n",
      "Iteration 12, loss = 0.63802705\n",
      "Iteration 13, loss = 0.49311859\n",
      "Iteration 14, loss = 0.43322837\n",
      "Iteration 15, loss = 0.48654910\n",
      "Iteration 16, loss = 0.39092913\n",
      "Iteration 17, loss = 0.45007185\n",
      "Iteration 18, loss = 0.36610177\n",
      "Iteration 19, loss = 0.38719980\n",
      "Iteration 20, loss = 0.31620058\n",
      "Iteration 21, loss = 0.35468674\n",
      "Iteration 22, loss = 0.36570901\n",
      "Iteration 23, loss = 0.41150676\n",
      "Iteration 24, loss = 0.34875796\n",
      "Iteration 25, loss = 0.34877921\n",
      "Iteration 26, loss = 0.35682093\n",
      "Iteration 27, loss = 0.45489268\n",
      "Iteration 28, loss = 0.30441149\n",
      "Iteration 29, loss = 0.22558932\n",
      "Iteration 30, loss = 0.27520935\n",
      "Iteration 31, loss = 0.25300186\n",
      "Iteration 32, loss = 0.29757484\n",
      "Iteration 33, loss = 0.24925881\n",
      "Iteration 34, loss = 0.22835213\n",
      "Iteration 35, loss = 0.27935012\n",
      "Iteration 36, loss = 0.27620929\n",
      "Iteration 37, loss = 0.33223894\n",
      "Iteration 38, loss = 0.24800149\n",
      "Iteration 39, loss = 0.22595692\n",
      "Iteration 40, loss = 0.21685018\n",
      "Iteration 41, loss = 0.22504780\n",
      "Iteration 42, loss = 0.24765522\n",
      "Iteration 43, loss = 0.23008196\n",
      "Iteration 44, loss = 0.21812215\n",
      "Iteration 45, loss = 0.26186345\n",
      "Iteration 46, loss = 0.30787665\n",
      "Iteration 47, loss = 0.26217228\n",
      "Iteration 48, loss = 0.27222475\n",
      "Iteration 49, loss = 0.32299145\n",
      "Iteration 50, loss = 0.31986602\n",
      "Iteration 51, loss = 0.25200773\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 52, loss = 0.22297375\n",
      "Iteration 53, loss = 0.18052499\n",
      "Iteration 54, loss = 0.15590722\n",
      "Iteration 55, loss = 0.14385977\n",
      "Iteration 56, loss = 0.15846911\n",
      "Iteration 57, loss = 0.14118000\n",
      "Iteration 58, loss = 0.12869695\n",
      "Iteration 59, loss = 0.12473847\n",
      "Iteration 60, loss = 0.12705613\n",
      "Iteration 61, loss = 0.12233221\n",
      "Iteration 62, loss = 0.12054391\n",
      "Iteration 63, loss = 0.11909304\n",
      "Iteration 64, loss = 0.11540217\n",
      "Iteration 65, loss = 0.11549788\n",
      "Iteration 66, loss = 0.11097134\n",
      "Iteration 67, loss = 0.11094937\n",
      "Iteration 68, loss = 0.11095466\n",
      "Iteration 69, loss = 0.10843471\n",
      "Iteration 70, loss = 0.10801684\n",
      "Iteration 71, loss = 0.10922393\n",
      "Iteration 72, loss = 0.10748089\n",
      "Iteration 73, loss = 0.10861364\n",
      "Iteration 74, loss = 0.11014038\n",
      "Iteration 75, loss = 0.10514531\n",
      "Iteration 76, loss = 0.10959213\n",
      "Iteration 77, loss = 0.10935770\n",
      "Iteration 78, loss = 0.10745513\n",
      "Iteration 79, loss = 0.10356730\n",
      "Iteration 80, loss = 0.10353924\n",
      "Iteration 81, loss = 0.10271315\n",
      "Iteration 82, loss = 0.10182096\n",
      "Iteration 83, loss = 0.10226713\n",
      "Iteration 84, loss = 0.10237816\n",
      "Iteration 85, loss = 0.10338376\n",
      "Iteration 86, loss = 0.10186245\n",
      "Iteration 87, loss = 0.10365138\n",
      "Iteration 88, loss = 0.10123943\n",
      "Iteration 89, loss = 0.09958811\n",
      "Iteration 90, loss = 0.09938776\n",
      "Iteration 91, loss = 0.09901875\n",
      "Iteration 92, loss = 0.09848108\n",
      "Iteration 93, loss = 0.09865954\n",
      "Iteration 94, loss = 0.09791602\n",
      "Iteration 95, loss = 0.09829339\n",
      "Iteration 96, loss = 0.09801261\n",
      "Iteration 97, loss = 0.09765687\n",
      "Iteration 98, loss = 0.09754059\n",
      "Iteration 99, loss = 0.09716176\n",
      "Iteration 100, loss = 0.09712120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.62301897\n",
      "Iteration 2, loss = 0.96218780\n",
      "Iteration 3, loss = 0.82659582\n",
      "Iteration 4, loss = 0.70490474\n",
      "Iteration 5, loss = 0.75345564\n",
      "Iteration 6, loss = 0.52926728\n",
      "Iteration 7, loss = 0.50595480\n",
      "Iteration 8, loss = 0.54668946\n",
      "Iteration 9, loss = 0.45933375\n",
      "Iteration 10, loss = 0.43443196\n",
      "Iteration 11, loss = 0.38767139\n",
      "Iteration 12, loss = 0.41818797\n",
      "Iteration 13, loss = 0.40940260\n",
      "Iteration 14, loss = 0.42296369\n",
      "Iteration 15, loss = 0.40993385\n",
      "Iteration 16, loss = 0.39528548\n",
      "Iteration 17, loss = 0.32424692\n",
      "Iteration 18, loss = 0.27695371\n",
      "Iteration 19, loss = 0.39757689\n",
      "Iteration 20, loss = 0.26303478\n",
      "Iteration 21, loss = 0.33712212\n",
      "Iteration 22, loss = 0.26611202\n",
      "Iteration 23, loss = 0.29854938\n",
      "Iteration 24, loss = 0.36780121\n",
      "Iteration 25, loss = 0.35478214\n",
      "Iteration 26, loss = 0.32085361\n",
      "Iteration 27, loss = 0.35248290\n",
      "Iteration 28, loss = 0.33809169\n",
      "Iteration 29, loss = 0.40101431\n",
      "Iteration 30, loss = 0.24425125\n",
      "Iteration 31, loss = 0.22274013\n",
      "Iteration 32, loss = 0.21997971\n",
      "Iteration 33, loss = 0.19772970\n",
      "Iteration 34, loss = 0.17626981\n",
      "Iteration 35, loss = 0.24053902\n",
      "Iteration 36, loss = 0.25597152\n",
      "Iteration 37, loss = 0.23475260\n",
      "Iteration 38, loss = 0.20835150\n",
      "Iteration 39, loss = 0.19656076\n",
      "Iteration 40, loss = 0.29337697\n",
      "Iteration 41, loss = 0.23390518\n",
      "Iteration 42, loss = 0.17462138\n",
      "Iteration 43, loss = 0.19554761\n",
      "Iteration 44, loss = 0.24533193\n",
      "Iteration 45, loss = 0.27052467\n",
      "Iteration 46, loss = 0.15741909\n",
      "Iteration 47, loss = 0.21704820\n",
      "Iteration 48, loss = 0.39564337\n",
      "Iteration 49, loss = 0.19562907\n",
      "Iteration 50, loss = 0.16026286\n",
      "Iteration 51, loss = 0.18142057\n",
      "Iteration 52, loss = 0.19217275\n",
      "Iteration 53, loss = 0.27560567\n",
      "Iteration 54, loss = 0.25930997\n",
      "Iteration 55, loss = 0.25808054\n",
      "Iteration 56, loss = 0.23550528\n",
      "Iteration 57, loss = 0.24613853\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 58, loss = 0.20618173\n",
      "Iteration 59, loss = 0.16761046\n",
      "Iteration 60, loss = 0.15005382\n",
      "Iteration 61, loss = 0.13370695\n",
      "Iteration 62, loss = 0.12557061\n",
      "Iteration 63, loss = 0.11724627\n",
      "Iteration 64, loss = 0.10596538\n",
      "Iteration 65, loss = 0.09975802\n",
      "Iteration 66, loss = 0.09614242\n",
      "Iteration 67, loss = 0.09530082\n",
      "Iteration 68, loss = 0.09475708\n",
      "Iteration 69, loss = 0.09269536\n",
      "Iteration 70, loss = 0.09128254\n",
      "Iteration 71, loss = 0.08544827\n",
      "Iteration 72, loss = 0.08791034\n",
      "Iteration 73, loss = 0.08009395\n",
      "Iteration 74, loss = 0.07830068\n",
      "Iteration 75, loss = 0.07669124\n",
      "Iteration 76, loss = 0.07602946\n",
      "Iteration 77, loss = 0.07512204\n",
      "Iteration 78, loss = 0.07458463\n",
      "Iteration 79, loss = 0.07390134\n",
      "Iteration 80, loss = 0.07341972\n",
      "Iteration 81, loss = 0.07266689\n",
      "Iteration 82, loss = 0.07144588\n",
      "Iteration 83, loss = 0.07029419\n",
      "Iteration 84, loss = 0.06988630\n",
      "Iteration 85, loss = 0.07145583\n",
      "Iteration 86, loss = 0.07049767\n",
      "Iteration 87, loss = 0.07006598\n",
      "Iteration 88, loss = 0.06857762\n",
      "Iteration 89, loss = 0.06763889\n",
      "Iteration 90, loss = 0.06719323\n",
      "Iteration 91, loss = 0.06677749\n",
      "Iteration 92, loss = 0.06643943\n",
      "Iteration 93, loss = 0.06628421\n",
      "Iteration 94, loss = 0.06600570\n",
      "Iteration 95, loss = 0.06555955\n",
      "Iteration 96, loss = 0.06561823\n",
      "Iteration 97, loss = 0.06465377\n",
      "Iteration 98, loss = 0.06435610\n",
      "Iteration 99, loss = 0.06406914\n",
      "Iteration 100, loss = 0.06377188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.73868182\n",
      "Iteration 2, loss = 1.07995342\n",
      "Iteration 3, loss = 0.71265242\n",
      "Iteration 4, loss = 0.57935787\n",
      "Iteration 5, loss = 0.50372271\n",
      "Iteration 6, loss = 0.61976335\n",
      "Iteration 7, loss = 0.55200746\n",
      "Iteration 8, loss = 0.56100193\n",
      "Iteration 9, loss = 0.47931132\n",
      "Iteration 10, loss = 0.42013716\n",
      "Iteration 11, loss = 0.45843319\n",
      "Iteration 12, loss = 0.46722077\n",
      "Iteration 13, loss = 0.45738403\n",
      "Iteration 14, loss = 0.46817780\n",
      "Iteration 15, loss = 0.49190101\n",
      "Iteration 16, loss = 0.45863447\n",
      "Iteration 17, loss = 0.39862742\n",
      "Iteration 18, loss = 0.40518991\n",
      "Iteration 19, loss = 0.35076205\n",
      "Iteration 20, loss = 0.33453078\n",
      "Iteration 21, loss = 0.47251208\n",
      "Iteration 22, loss = 0.49659570\n",
      "Iteration 23, loss = 0.43988271\n",
      "Iteration 24, loss = 0.41535916\n",
      "Iteration 25, loss = 0.36935314\n",
      "Iteration 26, loss = 0.34333537\n",
      "Iteration 27, loss = 0.41987795\n",
      "Iteration 28, loss = 0.39540203\n",
      "Iteration 29, loss = 0.36030454\n",
      "Iteration 30, loss = 0.42252297\n",
      "Iteration 31, loss = 0.32653831\n",
      "Iteration 32, loss = 0.33880163\n",
      "Iteration 33, loss = 0.36375600\n",
      "Iteration 34, loss = 0.29224429\n",
      "Iteration 35, loss = 0.32046608\n",
      "Iteration 36, loss = 0.26925818\n",
      "Iteration 37, loss = 0.30384082\n",
      "Iteration 38, loss = 0.31859499\n",
      "Iteration 39, loss = 0.31079849\n",
      "Iteration 40, loss = 0.44599956\n",
      "Iteration 41, loss = 0.30803558\n",
      "Iteration 42, loss = 0.29774913\n",
      "Iteration 43, loss = 0.41081871\n",
      "Iteration 44, loss = 0.42672460\n",
      "Iteration 45, loss = 0.29819345\n",
      "Iteration 46, loss = 0.28178500\n",
      "Iteration 47, loss = 0.25368220\n",
      "Iteration 48, loss = 0.29138261\n",
      "Iteration 49, loss = 0.32253953\n",
      "Iteration 50, loss = 0.23757816\n",
      "Iteration 51, loss = 0.29747681\n",
      "Iteration 52, loss = 0.22791032\n",
      "Iteration 53, loss = 0.22001137\n",
      "Iteration 54, loss = 0.29478355\n",
      "Iteration 55, loss = 0.29927233\n",
      "Iteration 56, loss = 0.20106986\n",
      "Iteration 57, loss = 0.20201273\n",
      "Iteration 58, loss = 0.35674357\n",
      "Iteration 59, loss = 0.51045387\n",
      "Iteration 60, loss = 0.45480164\n",
      "Iteration 61, loss = 0.33045717\n",
      "Iteration 62, loss = 0.32818103\n",
      "Iteration 63, loss = 0.27796050\n",
      "Iteration 64, loss = 0.32995726\n",
      "Iteration 65, loss = 0.27620605\n",
      "Iteration 66, loss = 0.25055634\n",
      "Iteration 67, loss = 0.26010664\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 68, loss = 0.20740442\n",
      "Iteration 69, loss = 0.17923477\n",
      "Iteration 70, loss = 0.18460630\n",
      "Iteration 71, loss = 0.17402844\n",
      "Iteration 72, loss = 0.15239656\n",
      "Iteration 73, loss = 0.14729025\n",
      "Iteration 74, loss = 0.14745862\n",
      "Iteration 75, loss = 0.14078341\n",
      "Iteration 76, loss = 0.13969077\n",
      "Iteration 77, loss = 0.14604180\n",
      "Iteration 78, loss = 0.14161085\n",
      "Iteration 79, loss = 0.14037606\n",
      "Iteration 80, loss = 0.13429023\n",
      "Iteration 81, loss = 0.13234379\n",
      "Iteration 82, loss = 0.13180910\n",
      "Iteration 83, loss = 0.13133732\n",
      "Iteration 84, loss = 0.13066504\n",
      "Iteration 85, loss = 0.13058019\n",
      "Iteration 86, loss = 0.13043883\n",
      "Iteration 87, loss = 0.12917264\n",
      "Iteration 88, loss = 0.12809496\n",
      "Iteration 89, loss = 0.12804056\n",
      "Iteration 90, loss = 0.12781769\n",
      "Iteration 91, loss = 0.12709295\n",
      "Iteration 92, loss = 0.12667204\n",
      "Iteration 93, loss = 0.12654062\n",
      "Iteration 94, loss = 0.12620572\n",
      "Iteration 95, loss = 0.12588251\n",
      "Iteration 96, loss = 0.12565419\n",
      "Iteration 97, loss = 0.12544926\n",
      "Iteration 98, loss = 0.12515727\n",
      "Iteration 99, loss = 0.12504238\n",
      "Iteration 100, loss = 0.12474864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.68644457\n",
      "Iteration 2, loss = 1.17968049\n",
      "Iteration 3, loss = 0.99774579\n",
      "Iteration 4, loss = 0.85971781\n",
      "Iteration 5, loss = 0.80358561\n",
      "Iteration 6, loss = 0.80198002\n",
      "Iteration 7, loss = 0.61078952\n",
      "Iteration 8, loss = 0.57323708\n",
      "Iteration 9, loss = 0.52926045\n",
      "Iteration 10, loss = 0.45281481\n",
      "Iteration 11, loss = 0.43365970\n",
      "Iteration 12, loss = 0.52765010\n",
      "Iteration 13, loss = 0.48400228\n",
      "Iteration 14, loss = 0.64928626\n",
      "Iteration 15, loss = 0.59002085\n",
      "Iteration 16, loss = 0.46322385\n",
      "Iteration 17, loss = 0.36803186\n",
      "Iteration 18, loss = 0.37700702\n",
      "Iteration 19, loss = 0.37619222\n",
      "Iteration 20, loss = 0.34616904\n",
      "Iteration 21, loss = 0.35771351\n",
      "Iteration 22, loss = 0.36700575\n",
      "Iteration 23, loss = 0.33431227\n",
      "Iteration 24, loss = 0.36656727\n",
      "Iteration 25, loss = 0.34058877\n",
      "Iteration 26, loss = 0.34233115\n",
      "Iteration 27, loss = 0.29190810\n",
      "Iteration 28, loss = 0.31759922\n",
      "Iteration 29, loss = 0.30168487\n",
      "Iteration 30, loss = 0.38196256\n",
      "Iteration 31, loss = 0.33226226\n",
      "Iteration 32, loss = 0.29578071\n",
      "Iteration 33, loss = 0.29840359\n",
      "Iteration 34, loss = 0.35680380\n",
      "Iteration 35, loss = 0.23935737\n",
      "Iteration 36, loss = 0.30472492\n",
      "Iteration 37, loss = 0.36074275\n",
      "Iteration 38, loss = 0.35711159\n",
      "Iteration 39, loss = 0.30184786\n",
      "Iteration 40, loss = 0.29710115\n",
      "Iteration 41, loss = 0.26245532\n",
      "Iteration 42, loss = 0.31818604\n",
      "Iteration 43, loss = 0.25529291\n",
      "Iteration 44, loss = 0.23983033\n",
      "Iteration 45, loss = 0.31551447\n",
      "Iteration 46, loss = 0.28219654\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 47, loss = 0.21953752\n",
      "Iteration 48, loss = 0.19029117\n",
      "Iteration 49, loss = 0.18545942\n",
      "Iteration 50, loss = 0.18159373\n",
      "Iteration 51, loss = 0.17201015\n",
      "Iteration 52, loss = 0.16816426\n",
      "Iteration 53, loss = 0.16644938\n",
      "Iteration 54, loss = 0.16642965\n",
      "Iteration 55, loss = 0.16092250\n",
      "Iteration 56, loss = 0.16364049\n",
      "Iteration 57, loss = 0.15482860\n",
      "Iteration 58, loss = 0.15517356\n",
      "Iteration 59, loss = 0.15435207\n",
      "Iteration 60, loss = 0.15390784\n",
      "Iteration 61, loss = 0.15332592\n",
      "Iteration 62, loss = 0.15287119\n",
      "Iteration 63, loss = 0.14941544\n",
      "Iteration 64, loss = 0.15624450\n",
      "Iteration 65, loss = 0.14784287\n",
      "Iteration 66, loss = 0.14549017\n",
      "Iteration 67, loss = 0.14639515\n",
      "Iteration 68, loss = 0.14166498\n",
      "Iteration 69, loss = 0.14260261\n",
      "Iteration 70, loss = 0.14104706\n",
      "Iteration 71, loss = 0.14091171\n",
      "Iteration 72, loss = 0.14358526\n",
      "Iteration 73, loss = 0.14231440\n",
      "Iteration 74, loss = 0.13756837\n",
      "Iteration 75, loss = 0.13720113\n",
      "Iteration 76, loss = 0.13646101\n",
      "Iteration 77, loss = 0.13641917\n",
      "Iteration 78, loss = 0.13589785\n",
      "Iteration 79, loss = 0.13537792\n",
      "Iteration 80, loss = 0.13623947\n",
      "Iteration 81, loss = 0.13505620\n",
      "Iteration 82, loss = 0.13454056\n",
      "Iteration 83, loss = 0.13442359\n",
      "Iteration 84, loss = 0.13360320\n",
      "Iteration 85, loss = 0.13417557\n",
      "Iteration 86, loss = 0.13750452\n",
      "Iteration 87, loss = 0.13401141\n",
      "Iteration 88, loss = 0.13325712\n",
      "Iteration 89, loss = 0.13223620\n",
      "Iteration 90, loss = 0.12966280\n",
      "Iteration 91, loss = 0.13147694\n",
      "Iteration 92, loss = 0.12777053\n",
      "Iteration 93, loss = 0.12797142\n",
      "Iteration 94, loss = 0.12774162\n",
      "Iteration 95, loss = 0.12759604\n",
      "Iteration 96, loss = 0.12741801\n",
      "Iteration 97, loss = 0.12604453\n",
      "Iteration 98, loss = 0.12590354\n",
      "Iteration 99, loss = 0.12671758\n",
      "Iteration 100, loss = 0.12792592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.62257475\n",
      "Iteration 2, loss = 0.96596189\n",
      "Iteration 3, loss = 0.77537791\n",
      "Iteration 4, loss = 0.75963107\n",
      "Iteration 5, loss = 0.59183510\n",
      "Iteration 6, loss = 0.48162191\n",
      "Iteration 7, loss = 0.50705883\n",
      "Iteration 8, loss = 0.52519199\n",
      "Iteration 9, loss = 0.34592534\n",
      "Iteration 10, loss = 0.34499692\n",
      "Iteration 11, loss = 0.34898012\n",
      "Iteration 12, loss = 0.38766475\n",
      "Iteration 13, loss = 0.28650483\n",
      "Iteration 14, loss = 0.26373872\n",
      "Iteration 15, loss = 0.36427102\n",
      "Iteration 16, loss = 0.27834923\n",
      "Iteration 17, loss = 0.30925333\n",
      "Iteration 18, loss = 0.24516850\n",
      "Iteration 19, loss = 0.27283426\n",
      "Iteration 20, loss = 0.37390665\n",
      "Iteration 21, loss = 0.25228476\n",
      "Iteration 22, loss = 0.19871185\n",
      "Iteration 23, loss = 0.27235299\n",
      "Iteration 24, loss = 0.27871311\n",
      "Iteration 25, loss = 0.16890070\n",
      "Iteration 26, loss = 0.17145291\n",
      "Iteration 27, loss = 0.23046341\n",
      "Iteration 28, loss = 0.32195230\n",
      "Iteration 29, loss = 0.19730783\n",
      "Iteration 30, loss = 0.16696614\n",
      "Iteration 31, loss = 0.19714404\n",
      "Iteration 32, loss = 0.28003011\n",
      "Iteration 33, loss = 0.29132633\n",
      "Iteration 34, loss = 0.21108420\n",
      "Iteration 35, loss = 0.37296708\n",
      "Iteration 36, loss = 0.34251403\n",
      "Iteration 37, loss = 0.26140813\n",
      "Iteration 38, loss = 0.22595015\n",
      "Iteration 39, loss = 0.27757606\n",
      "Iteration 40, loss = 0.21498471\n",
      "Iteration 41, loss = 0.15715464\n",
      "Iteration 42, loss = 0.14046876\n",
      "Iteration 43, loss = 0.15649736\n",
      "Iteration 44, loss = 0.17970044\n",
      "Iteration 45, loss = 0.20462681\n",
      "Iteration 46, loss = 0.23947077\n",
      "Iteration 47, loss = 0.24400861\n",
      "Iteration 48, loss = 0.16494451\n",
      "Iteration 49, loss = 0.17965519\n",
      "Iteration 50, loss = 0.17165241\n",
      "Iteration 51, loss = 0.14066648\n",
      "Iteration 52, loss = 0.14411617\n",
      "Iteration 53, loss = 0.12944494\n",
      "Iteration 54, loss = 0.15516894\n",
      "Iteration 55, loss = 0.16563756\n",
      "Iteration 56, loss = 0.15017786\n",
      "Iteration 57, loss = 0.12316705\n",
      "Iteration 58, loss = 0.13814757\n",
      "Iteration 59, loss = 0.14968559\n",
      "Iteration 60, loss = 0.09248510\n",
      "Iteration 61, loss = 0.14250631\n",
      "Iteration 62, loss = 0.15223297\n",
      "Iteration 63, loss = 0.13272076\n",
      "Iteration 64, loss = 0.13033010\n",
      "Iteration 65, loss = 0.13166449\n",
      "Iteration 66, loss = 0.19685244\n",
      "Iteration 67, loss = 0.18297499\n",
      "Iteration 68, loss = 0.25293409\n",
      "Iteration 69, loss = 0.19001387\n",
      "Iteration 70, loss = 0.19950822\n",
      "Iteration 71, loss = 0.23970938\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 72, loss = 0.14613738\n",
      "Iteration 73, loss = 0.12211608\n",
      "Iteration 74, loss = 0.12023603\n",
      "Iteration 75, loss = 0.11226138\n",
      "Iteration 76, loss = 0.10180090\n",
      "Iteration 77, loss = 0.08977844\n",
      "Iteration 78, loss = 0.08342814\n",
      "Iteration 79, loss = 0.07550473\n",
      "Iteration 80, loss = 0.07223955\n",
      "Iteration 81, loss = 0.06975891\n",
      "Iteration 82, loss = 0.06827794\n",
      "Iteration 83, loss = 0.06722377\n",
      "Iteration 84, loss = 0.06644568\n",
      "Iteration 85, loss = 0.06587449\n",
      "Iteration 86, loss = 0.06514478\n",
      "Iteration 87, loss = 0.06453105\n",
      "Iteration 88, loss = 0.06410710\n",
      "Iteration 89, loss = 0.06363253\n",
      "Iteration 90, loss = 0.06290856\n",
      "Iteration 91, loss = 0.06247255\n",
      "Iteration 92, loss = 0.06179604\n",
      "Iteration 93, loss = 0.06187065\n",
      "Iteration 94, loss = 0.06125355\n",
      "Iteration 95, loss = 0.06079051\n",
      "Iteration 96, loss = 0.06011342\n",
      "Iteration 97, loss = 0.05931500\n",
      "Iteration 98, loss = 0.05894095\n",
      "Iteration 99, loss = 0.05849315\n",
      "Iteration 100, loss = 0.05744826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.59996134\n",
      "Iteration 2, loss = 0.95500950\n",
      "Iteration 3, loss = 0.81998763\n",
      "Iteration 4, loss = 0.67418440\n",
      "Iteration 5, loss = 0.73784638\n",
      "Iteration 6, loss = 0.77672681\n",
      "Iteration 7, loss = 0.56580756\n",
      "Iteration 8, loss = 0.67678041\n",
      "Iteration 9, loss = 0.57379284\n",
      "Iteration 10, loss = 0.59652449\n",
      "Iteration 11, loss = 0.43494052\n",
      "Iteration 12, loss = 0.49138516\n",
      "Iteration 13, loss = 0.50515902\n",
      "Iteration 14, loss = 0.50373623\n",
      "Iteration 15, loss = 0.44467062\n",
      "Iteration 16, loss = 0.44577150\n",
      "Iteration 17, loss = 0.43774766\n",
      "Iteration 18, loss = 0.34763384\n",
      "Iteration 19, loss = 0.42794374\n",
      "Iteration 20, loss = 0.43758273\n",
      "Iteration 21, loss = 0.41644353\n",
      "Iteration 22, loss = 0.34828768\n",
      "Iteration 23, loss = 0.36369770\n",
      "Iteration 24, loss = 0.35512570\n",
      "Iteration 25, loss = 0.42508178\n",
      "Iteration 26, loss = 0.43966970\n",
      "Iteration 27, loss = 0.30973792\n",
      "Iteration 28, loss = 0.35152753\n",
      "Iteration 29, loss = 0.33334158\n",
      "Iteration 30, loss = 0.30508478\n",
      "Iteration 31, loss = 0.36273061\n",
      "Iteration 32, loss = 0.43495203\n",
      "Iteration 33, loss = 0.30843944\n",
      "Iteration 34, loss = 0.49734383\n",
      "Iteration 35, loss = 0.41443416\n",
      "Iteration 36, loss = 0.46972024\n",
      "Iteration 37, loss = 0.27807871\n",
      "Iteration 38, loss = 0.36951556\n",
      "Iteration 39, loss = 0.30199922\n",
      "Iteration 40, loss = 0.26516506\n",
      "Iteration 41, loss = 0.27990318\n",
      "Iteration 42, loss = 0.29083152\n",
      "Iteration 43, loss = 0.30381366\n",
      "Iteration 44, loss = 0.34285986\n",
      "Iteration 45, loss = 0.37988701\n",
      "Iteration 46, loss = 0.29402493\n",
      "Iteration 47, loss = 0.33064630\n",
      "Iteration 48, loss = 0.37201184\n",
      "Iteration 49, loss = 0.29187736\n",
      "Iteration 50, loss = 0.30643744\n",
      "Iteration 51, loss = 0.28072153\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 52, loss = 0.24679463\n",
      "Iteration 53, loss = 0.19114312\n",
      "Iteration 54, loss = 0.19198598\n",
      "Iteration 55, loss = 0.18274463\n",
      "Iteration 56, loss = 0.18891430\n",
      "Iteration 57, loss = 0.18621255\n",
      "Iteration 58, loss = 0.17164173\n",
      "Iteration 59, loss = 0.17246690\n",
      "Iteration 60, loss = 0.16323683\n",
      "Iteration 61, loss = 0.15877272\n",
      "Iteration 62, loss = 0.15702499\n",
      "Iteration 63, loss = 0.15897235\n",
      "Iteration 64, loss = 0.15887661\n",
      "Iteration 65, loss = 0.15313850\n",
      "Iteration 66, loss = 0.15216890\n",
      "Iteration 67, loss = 0.15027509\n",
      "Iteration 68, loss = 0.14938347\n",
      "Iteration 69, loss = 0.15342036\n",
      "Iteration 70, loss = 0.15066782\n",
      "Iteration 71, loss = 0.14594805\n",
      "Iteration 72, loss = 0.14871656\n",
      "Iteration 73, loss = 0.15184044\n",
      "Iteration 74, loss = 0.14617216\n",
      "Iteration 75, loss = 0.14276141\n",
      "Iteration 76, loss = 0.14075436\n",
      "Iteration 77, loss = 0.14113794\n",
      "Iteration 78, loss = 0.13884735\n",
      "Iteration 79, loss = 0.13847096\n",
      "Iteration 80, loss = 0.13767591\n",
      "Iteration 81, loss = 0.13768669\n",
      "Iteration 82, loss = 0.13700489\n",
      "Iteration 83, loss = 0.14037566\n",
      "Iteration 84, loss = 0.14154483\n",
      "Iteration 85, loss = 0.13779133\n",
      "Iteration 86, loss = 0.13711512\n",
      "Iteration 87, loss = 0.13353140\n",
      "Iteration 88, loss = 0.13519897\n",
      "Iteration 89, loss = 0.13515392\n",
      "Iteration 90, loss = 0.13464733\n",
      "Iteration 91, loss = 0.13406059\n",
      "Iteration 92, loss = 0.13460823\n",
      "Iteration 93, loss = 0.13337604\n",
      "Iteration 94, loss = 0.13222274\n",
      "Iteration 95, loss = 0.13188497\n",
      "Iteration 96, loss = 0.13168689\n",
      "Iteration 97, loss = 0.13125872\n",
      "Iteration 98, loss = 0.13098231\n",
      "Iteration 99, loss = 0.13065703\n",
      "Iteration 100, loss = 0.13056664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.53966381\n",
      "Iteration 2, loss = 0.97436055\n",
      "Iteration 3, loss = 0.80620159\n",
      "Iteration 4, loss = 0.68270731\n",
      "Iteration 5, loss = 0.62931404\n",
      "Iteration 6, loss = 0.68121904\n",
      "Iteration 7, loss = 0.56335007\n",
      "Iteration 8, loss = 0.51542182\n",
      "Iteration 9, loss = 0.60409335\n",
      "Iteration 10, loss = 0.56720065\n",
      "Iteration 11, loss = 0.61945010\n",
      "Iteration 12, loss = 0.43942945\n",
      "Iteration 13, loss = 0.40678274\n",
      "Iteration 14, loss = 0.46114112\n",
      "Iteration 15, loss = 0.44633272\n",
      "Iteration 16, loss = 0.50363713\n",
      "Iteration 17, loss = 0.42122629\n",
      "Iteration 18, loss = 0.41976258\n",
      "Iteration 19, loss = 0.46360859\n",
      "Iteration 20, loss = 0.50701687\n",
      "Iteration 21, loss = 0.41653786\n",
      "Iteration 22, loss = 0.34329599\n",
      "Iteration 23, loss = 0.30277453\n",
      "Iteration 24, loss = 0.32853658\n",
      "Iteration 25, loss = 0.35416840\n",
      "Iteration 26, loss = 0.32550904\n",
      "Iteration 27, loss = 0.25791296\n",
      "Iteration 28, loss = 0.23894797\n",
      "Iteration 29, loss = 0.29437468\n",
      "Iteration 30, loss = 0.27558904\n",
      "Iteration 31, loss = 0.40772046\n",
      "Iteration 32, loss = 0.43826397\n",
      "Iteration 33, loss = 0.29371258\n",
      "Iteration 34, loss = 0.30015692\n",
      "Iteration 35, loss = 0.30991741\n",
      "Iteration 36, loss = 0.25742033\n",
      "Iteration 37, loss = 0.28896119\n",
      "Iteration 38, loss = 0.31471006\n",
      "Iteration 39, loss = 0.28432804\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 40, loss = 0.23420379\n",
      "Iteration 41, loss = 0.19292047\n",
      "Iteration 42, loss = 0.19070454\n",
      "Iteration 43, loss = 0.18468994\n",
      "Iteration 44, loss = 0.18179593\n",
      "Iteration 45, loss = 0.17311083\n",
      "Iteration 46, loss = 0.18005264\n",
      "Iteration 47, loss = 0.16759690\n",
      "Iteration 48, loss = 0.17329291\n",
      "Iteration 49, loss = 0.17209427\n",
      "Iteration 50, loss = 0.16125216\n",
      "Iteration 51, loss = 0.15754443\n",
      "Iteration 52, loss = 0.15379195\n",
      "Iteration 53, loss = 0.14437832\n",
      "Iteration 54, loss = 0.14257028\n",
      "Iteration 55, loss = 0.14169035\n",
      "Iteration 56, loss = 0.14083669\n",
      "Iteration 57, loss = 0.14018594\n",
      "Iteration 58, loss = 0.13934273\n",
      "Iteration 59, loss = 0.14007965\n",
      "Iteration 60, loss = 0.13827030\n",
      "Iteration 61, loss = 0.13761592\n",
      "Iteration 62, loss = 0.13723748\n",
      "Iteration 63, loss = 0.13870115\n",
      "Iteration 64, loss = 0.13614628\n",
      "Iteration 65, loss = 0.13612687\n",
      "Iteration 66, loss = 0.13327396\n",
      "Iteration 67, loss = 0.13511765\n",
      "Iteration 68, loss = 0.13358963\n",
      "Iteration 69, loss = 0.13428429\n",
      "Iteration 70, loss = 0.13124962\n",
      "Iteration 71, loss = 0.13410815\n",
      "Iteration 72, loss = 0.13284961\n",
      "Iteration 73, loss = 0.13328916\n",
      "Iteration 74, loss = 0.12747407\n",
      "Iteration 75, loss = 0.12788963\n",
      "Iteration 76, loss = 0.12852091\n",
      "Iteration 77, loss = 0.12582005\n",
      "Iteration 78, loss = 0.12574071\n",
      "Iteration 79, loss = 0.12464508\n",
      "Iteration 80, loss = 0.12882616\n",
      "Iteration 81, loss = 0.12762766\n",
      "Iteration 82, loss = 0.13513577\n",
      "Iteration 83, loss = 0.12424786\n",
      "Iteration 84, loss = 0.12283577\n",
      "Iteration 85, loss = 0.12326813\n",
      "Iteration 86, loss = 0.12163616\n",
      "Iteration 87, loss = 0.12095415\n",
      "Iteration 88, loss = 0.12038909\n",
      "Iteration 89, loss = 0.12020772\n",
      "Iteration 90, loss = 0.12230745\n",
      "Iteration 91, loss = 0.11993310\n",
      "Iteration 92, loss = 0.11965745\n",
      "Iteration 93, loss = 0.11935659\n",
      "Iteration 94, loss = 0.11874215\n",
      "Iteration 95, loss = 0.11850612\n",
      "Iteration 96, loss = 0.11799088\n",
      "Iteration 97, loss = 0.12245224\n",
      "Iteration 98, loss = 0.11743436\n",
      "Iteration 99, loss = 0.11791415\n",
      "Iteration 100, loss = 0.11778398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.59823740\n",
      "Iteration 2, loss = 0.98848865\n",
      "Iteration 3, loss = 0.92793112\n",
      "Iteration 4, loss = 0.70538981\n",
      "Iteration 5, loss = 0.64913415\n",
      "Iteration 6, loss = 0.65177253\n",
      "Iteration 7, loss = 0.61833157\n",
      "Iteration 8, loss = 0.59068545\n",
      "Iteration 9, loss = 0.54229071\n",
      "Iteration 10, loss = 0.56022878\n",
      "Iteration 11, loss = 0.51236818\n",
      "Iteration 12, loss = 0.51860669\n",
      "Iteration 13, loss = 0.64094569\n",
      "Iteration 14, loss = 0.49672293\n",
      "Iteration 15, loss = 0.60562166\n",
      "Iteration 16, loss = 0.40270769\n",
      "Iteration 17, loss = 0.43109738\n",
      "Iteration 18, loss = 0.34808068\n",
      "Iteration 19, loss = 0.44918184\n",
      "Iteration 20, loss = 0.43186595\n",
      "Iteration 21, loss = 0.49598619\n",
      "Iteration 22, loss = 0.37558011\n",
      "Iteration 23, loss = 0.38797929\n",
      "Iteration 24, loss = 0.48920956\n",
      "Iteration 25, loss = 0.36569952\n",
      "Iteration 26, loss = 0.35859698\n",
      "Iteration 27, loss = 0.30896405\n",
      "Iteration 28, loss = 0.35609147\n",
      "Iteration 29, loss = 0.37311106\n",
      "Iteration 30, loss = 0.30561778\n",
      "Iteration 31, loss = 0.26070503\n",
      "Iteration 32, loss = 0.42283385\n",
      "Iteration 33, loss = 0.39221654\n",
      "Iteration 34, loss = 0.52522715\n",
      "Iteration 35, loss = 0.41542521\n",
      "Iteration 36, loss = 0.31443827\n",
      "Iteration 37, loss = 0.33072142\n",
      "Iteration 38, loss = 0.36762245\n",
      "Iteration 39, loss = 0.40841995\n",
      "Iteration 40, loss = 0.36053711\n",
      "Iteration 41, loss = 0.29415632\n",
      "Iteration 42, loss = 0.42052476\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 43, loss = 0.36417303\n",
      "Iteration 44, loss = 0.26855372\n",
      "Iteration 45, loss = 0.23283120\n",
      "Iteration 46, loss = 0.22498049\n",
      "Iteration 47, loss = 0.20637680\n",
      "Iteration 48, loss = 0.20288634\n",
      "Iteration 49, loss = 0.19813849\n",
      "Iteration 50, loss = 0.19019414\n",
      "Iteration 51, loss = 0.18643031\n",
      "Iteration 52, loss = 0.17962659\n",
      "Iteration 53, loss = 0.17729202\n",
      "Iteration 54, loss = 0.17431476\n",
      "Iteration 55, loss = 0.17497605\n",
      "Iteration 56, loss = 0.17410790\n",
      "Iteration 57, loss = 0.17126536\n",
      "Iteration 58, loss = 0.17156205\n",
      "Iteration 59, loss = 0.18415482\n",
      "Iteration 60, loss = 0.16185681\n",
      "Iteration 61, loss = 0.15893799\n",
      "Iteration 62, loss = 0.17041301\n",
      "Iteration 63, loss = 0.16171446\n",
      "Iteration 64, loss = 0.15547556\n",
      "Iteration 65, loss = 0.15703576\n",
      "Iteration 66, loss = 0.15560317\n",
      "Iteration 67, loss = 0.15581467\n",
      "Iteration 68, loss = 0.14872548\n",
      "Iteration 69, loss = 0.15031200\n",
      "Iteration 70, loss = 0.15083410\n",
      "Iteration 71, loss = 0.14836232\n",
      "Iteration 72, loss = 0.15015763\n",
      "Iteration 73, loss = 0.14843400\n",
      "Iteration 74, loss = 0.14682819\n",
      "Iteration 75, loss = 0.14735613\n",
      "Iteration 76, loss = 0.14584865\n",
      "Iteration 77, loss = 0.14541908\n",
      "Iteration 78, loss = 0.14553693\n",
      "Iteration 79, loss = 0.14470322\n",
      "Iteration 80, loss = 0.14248249\n",
      "Iteration 81, loss = 0.14221589\n",
      "Iteration 82, loss = 0.14185066\n",
      "Iteration 83, loss = 0.14175477\n",
      "Iteration 84, loss = 0.14131820\n",
      "Iteration 85, loss = 0.14107280\n",
      "Iteration 86, loss = 0.14067704\n",
      "Iteration 87, loss = 0.14039013\n",
      "Iteration 88, loss = 0.14013976\n",
      "Iteration 89, loss = 0.13989145\n",
      "Iteration 90, loss = 0.13957951\n",
      "Iteration 91, loss = 0.13906083\n",
      "Iteration 92, loss = 0.13854181\n",
      "Iteration 93, loss = 0.13806544\n",
      "Iteration 94, loss = 0.13695678\n",
      "Iteration 95, loss = 0.14309587\n",
      "Iteration 96, loss = 0.13677244\n",
      "Iteration 97, loss = 0.13638224\n",
      "Iteration 98, loss = 0.13559159\n",
      "Iteration 99, loss = 0.14244587\n",
      "Iteration 100, loss = 0.13359596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.93908944\n",
      "Iteration 2, loss = 1.61547393\n",
      "Iteration 3, loss = 1.37167262\n",
      "Iteration 4, loss = 1.11291855\n",
      "Iteration 5, loss = 0.87463377\n",
      "Iteration 6, loss = 0.90712807\n",
      "Iteration 7, loss = 0.72776304\n",
      "Iteration 8, loss = 0.72867740\n",
      "Iteration 9, loss = 0.76992332\n",
      "Iteration 10, loss = 0.57726902\n",
      "Iteration 11, loss = 0.61448752\n",
      "Iteration 12, loss = 0.58423370\n",
      "Iteration 13, loss = 0.62445917\n",
      "Iteration 14, loss = 0.64957709\n",
      "Iteration 15, loss = 0.76273539\n",
      "Iteration 16, loss = 0.64040891\n",
      "Iteration 17, loss = 0.48538147\n",
      "Iteration 18, loss = 0.58617121\n",
      "Iteration 19, loss = 0.72343416\n",
      "Iteration 20, loss = 0.63314376\n",
      "Iteration 21, loss = 0.47856813\n",
      "Iteration 22, loss = 0.52926932\n",
      "Iteration 23, loss = 0.48337155\n",
      "Iteration 24, loss = 0.53029108\n",
      "Iteration 25, loss = 0.51057152\n",
      "Iteration 26, loss = 0.61638198\n",
      "Iteration 27, loss = 0.50666646\n",
      "Iteration 28, loss = 0.52109853\n",
      "Iteration 29, loss = 0.53360949\n",
      "Iteration 30, loss = 0.50100662\n",
      "Iteration 31, loss = 0.43838767\n",
      "Iteration 32, loss = 0.44272800\n",
      "Iteration 33, loss = 0.40031060\n",
      "Iteration 34, loss = 0.41790434\n",
      "Iteration 35, loss = 0.41056391\n",
      "Iteration 36, loss = 0.52112182\n",
      "Iteration 37, loss = 0.44256440\n",
      "Iteration 38, loss = 0.38717794\n",
      "Iteration 39, loss = 0.36886940\n",
      "Iteration 40, loss = 0.35540924\n",
      "Iteration 41, loss = 0.35203215\n",
      "Iteration 42, loss = 0.36200930\n",
      "Iteration 43, loss = 0.45688679\n",
      "Iteration 44, loss = 0.46834265\n",
      "Iteration 45, loss = 0.36475180\n",
      "Iteration 46, loss = 0.45109909\n",
      "Iteration 47, loss = 0.39802521\n",
      "Iteration 48, loss = 0.33669757\n",
      "Iteration 49, loss = 0.45633806\n",
      "Iteration 50, loss = 0.42824241\n",
      "Iteration 51, loss = 0.40860816\n",
      "Iteration 52, loss = 0.41545887\n",
      "Iteration 53, loss = 0.33465622\n",
      "Iteration 54, loss = 0.34446377\n",
      "Iteration 55, loss = 0.36017520\n",
      "Iteration 56, loss = 0.33316583\n",
      "Iteration 57, loss = 0.43593657\n",
      "Iteration 58, loss = 0.44140147\n",
      "Iteration 59, loss = 0.49457572\n",
      "Iteration 60, loss = 0.37653589\n",
      "Iteration 61, loss = 0.34435818\n",
      "Iteration 62, loss = 0.27927060\n",
      "Iteration 63, loss = 0.29800020\n",
      "Iteration 64, loss = 0.32206019\n",
      "Iteration 65, loss = 0.30598187\n",
      "Iteration 66, loss = 0.36510237\n",
      "Iteration 67, loss = 0.29020521\n",
      "Iteration 68, loss = 0.35826331\n",
      "Iteration 69, loss = 0.29737207\n",
      "Iteration 70, loss = 0.29110404\n",
      "Iteration 71, loss = 0.35666652\n",
      "Iteration 72, loss = 0.35387447\n",
      "Iteration 73, loss = 0.35783776\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 74, loss = 0.29416898\n",
      "Iteration 75, loss = 0.27122274\n",
      "Iteration 76, loss = 0.27004458\n",
      "Iteration 77, loss = 0.25614110\n",
      "Iteration 78, loss = 0.25565839\n",
      "Iteration 79, loss = 0.26382582\n",
      "Iteration 80, loss = 0.23733989\n",
      "Iteration 81, loss = 0.24109500\n",
      "Iteration 82, loss = 0.23278095\n",
      "Iteration 83, loss = 0.25148300\n",
      "Iteration 84, loss = 0.23831661\n",
      "Iteration 85, loss = 0.23153071\n",
      "Iteration 86, loss = 0.23603599\n",
      "Iteration 87, loss = 0.23629760\n",
      "Iteration 88, loss = 0.23568868\n",
      "Iteration 89, loss = 0.23522565\n",
      "Iteration 90, loss = 0.23390935\n",
      "Iteration 91, loss = 0.21922422\n",
      "Iteration 92, loss = 0.22786141\n",
      "Iteration 93, loss = 0.22442184\n",
      "Iteration 94, loss = 0.21506841\n",
      "Iteration 95, loss = 0.23216854\n",
      "Iteration 96, loss = 0.23540726\n",
      "Iteration 97, loss = 0.22855249\n",
      "Iteration 98, loss = 0.22559194\n",
      "Iteration 99, loss = 0.21065697\n",
      "Iteration 100, loss = 0.20474140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.50413188\n",
      "Iteration 2, loss = 0.86079199\n",
      "Iteration 3, loss = 0.65965269\n",
      "Iteration 4, loss = 0.57016606\n",
      "Iteration 5, loss = 0.46779672\n",
      "Iteration 6, loss = 0.50381372\n",
      "Iteration 7, loss = 0.42723062\n",
      "Iteration 8, loss = 0.37582088\n",
      "Iteration 9, loss = 0.37471794\n",
      "Iteration 10, loss = 0.44188324\n",
      "Iteration 11, loss = 0.54571918\n",
      "Iteration 12, loss = 0.35505893\n",
      "Iteration 13, loss = 0.31269978\n",
      "Iteration 14, loss = 0.34887554\n",
      "Iteration 15, loss = 0.46034665\n",
      "Iteration 16, loss = 0.39602117\n",
      "Iteration 17, loss = 0.33622181\n",
      "Iteration 18, loss = 0.31082301\n",
      "Iteration 19, loss = 0.26414337\n",
      "Iteration 20, loss = 0.38344673\n",
      "Iteration 21, loss = 0.30697255\n",
      "Iteration 22, loss = 0.27602069\n",
      "Iteration 23, loss = 0.22689654\n",
      "Iteration 24, loss = 0.27154997\n",
      "Iteration 25, loss = 0.24770208\n",
      "Iteration 26, loss = 0.30442681\n",
      "Iteration 27, loss = 0.25807911\n",
      "Iteration 28, loss = 0.25397152\n",
      "Iteration 29, loss = 0.26741386\n",
      "Iteration 30, loss = 0.33744724\n",
      "Iteration 31, loss = 0.26325958\n",
      "Iteration 32, loss = 0.25680491\n",
      "Iteration 33, loss = 0.25846018\n",
      "Iteration 34, loss = 0.23746913\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 35, loss = 0.17861117\n",
      "Iteration 36, loss = 0.16644871\n",
      "Iteration 37, loss = 0.15348349\n",
      "Iteration 38, loss = 0.14296845\n",
      "Iteration 39, loss = 0.14245728\n",
      "Iteration 40, loss = 0.13459437\n",
      "Iteration 41, loss = 0.13006219\n",
      "Iteration 42, loss = 0.12694264\n",
      "Iteration 43, loss = 0.12019716\n",
      "Iteration 44, loss = 0.11920867\n",
      "Iteration 45, loss = 0.11848432\n",
      "Iteration 46, loss = 0.11287401\n",
      "Iteration 47, loss = 0.11143567\n",
      "Iteration 48, loss = 0.11172973\n",
      "Iteration 49, loss = 0.10947718\n",
      "Iteration 50, loss = 0.10673718\n",
      "Iteration 51, loss = 0.10536815\n",
      "Iteration 52, loss = 0.10506135\n",
      "Iteration 53, loss = 0.10429591\n",
      "Iteration 54, loss = 0.10583646\n",
      "Iteration 55, loss = 0.10548540\n",
      "Iteration 56, loss = 0.10170812\n",
      "Iteration 57, loss = 0.10456268\n",
      "Iteration 58, loss = 0.09962359\n",
      "Iteration 59, loss = 0.09859045\n",
      "Iteration 60, loss = 0.09811777\n",
      "Iteration 61, loss = 0.09863496\n",
      "Iteration 62, loss = 0.09615270\n",
      "Iteration 63, loss = 0.09327141\n",
      "Iteration 64, loss = 0.09296349\n",
      "Iteration 65, loss = 0.09181246\n",
      "Iteration 66, loss = 0.09092407\n",
      "Iteration 67, loss = 0.09264324\n",
      "Iteration 68, loss = 0.08950185\n",
      "Iteration 69, loss = 0.09116836\n",
      "Iteration 70, loss = 0.08914556\n",
      "Iteration 71, loss = 0.08834708\n",
      "Iteration 72, loss = 0.08684636\n",
      "Iteration 73, loss = 0.08578029\n",
      "Iteration 74, loss = 0.08583396\n",
      "Iteration 75, loss = 0.08558451\n",
      "Iteration 76, loss = 0.08520812\n",
      "Iteration 77, loss = 0.08475797\n",
      "Iteration 78, loss = 0.08414137\n",
      "Iteration 79, loss = 0.08373229\n",
      "Iteration 80, loss = 0.08340513\n",
      "Iteration 81, loss = 0.08342835\n",
      "Iteration 82, loss = 0.08344310\n",
      "Iteration 83, loss = 0.08303481\n",
      "Iteration 84, loss = 0.08266362\n",
      "Iteration 85, loss = 0.08245063\n",
      "Iteration 86, loss = 0.08121626\n",
      "Iteration 87, loss = 0.07820034\n",
      "Iteration 88, loss = 0.07633332\n",
      "Iteration 89, loss = 0.07520090\n",
      "Iteration 90, loss = 0.07521930\n",
      "Iteration 91, loss = 0.07434858\n",
      "Iteration 92, loss = 0.07384402\n",
      "Iteration 93, loss = 0.07357625\n",
      "Iteration 94, loss = 0.07332703\n",
      "Iteration 95, loss = 0.07292201\n",
      "Iteration 96, loss = 0.07280724\n",
      "Iteration 97, loss = 0.07253470\n",
      "Iteration 98, loss = 0.07233449\n",
      "Iteration 99, loss = 0.07203419\n",
      "Iteration 100, loss = 0.07174657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.40804277\n",
      "Iteration 2, loss = 0.79305119\n",
      "Iteration 3, loss = 0.52134445\n",
      "Iteration 4, loss = 0.43975195\n",
      "Iteration 5, loss = 0.47642265\n",
      "Iteration 6, loss = 0.41275326\n",
      "Iteration 7, loss = 0.42033187\n",
      "Iteration 8, loss = 0.32517005\n",
      "Iteration 9, loss = 0.38793054\n",
      "Iteration 10, loss = 0.48761192\n",
      "Iteration 11, loss = 0.45736060\n",
      "Iteration 12, loss = 0.38394643\n",
      "Iteration 13, loss = 0.35598658\n",
      "Iteration 14, loss = 0.32859627\n",
      "Iteration 15, loss = 0.31074052\n",
      "Iteration 16, loss = 0.34459730\n",
      "Iteration 17, loss = 0.33378611\n",
      "Iteration 18, loss = 0.29710471\n",
      "Iteration 19, loss = 0.40377659\n",
      "Iteration 20, loss = 0.40563302\n",
      "Iteration 21, loss = 0.34638130\n",
      "Iteration 22, loss = 0.37546590\n",
      "Iteration 23, loss = 0.28159517\n",
      "Iteration 24, loss = 0.30464856\n",
      "Iteration 25, loss = 0.22949300\n",
      "Iteration 26, loss = 0.27712548\n",
      "Iteration 27, loss = 0.27789449\n",
      "Iteration 28, loss = 0.19912046\n",
      "Iteration 29, loss = 0.21407289\n",
      "Iteration 30, loss = 0.23960021\n",
      "Iteration 31, loss = 0.25373417\n",
      "Iteration 32, loss = 0.34725144\n",
      "Iteration 33, loss = 0.19917181\n",
      "Iteration 34, loss = 0.20673815\n",
      "Iteration 35, loss = 0.24509389\n",
      "Iteration 36, loss = 0.21971646\n",
      "Iteration 37, loss = 0.14884266\n",
      "Iteration 38, loss = 0.18721281\n",
      "Iteration 39, loss = 0.18167306\n",
      "Iteration 40, loss = 0.16186127\n",
      "Iteration 41, loss = 0.22425423\n",
      "Iteration 42, loss = 0.16718020\n",
      "Iteration 43, loss = 0.22234541\n",
      "Iteration 44, loss = 0.17726206\n",
      "Iteration 45, loss = 0.15706959\n",
      "Iteration 46, loss = 0.13335574\n",
      "Iteration 47, loss = 0.18602374\n",
      "Iteration 48, loss = 0.12626260\n",
      "Iteration 49, loss = 0.13965120\n",
      "Iteration 50, loss = 0.16534213\n",
      "Iteration 51, loss = 0.17383355\n",
      "Iteration 52, loss = 0.13016621\n",
      "Iteration 53, loss = 0.17312659\n",
      "Iteration 54, loss = 0.21304293\n",
      "Iteration 55, loss = 0.21785543\n",
      "Iteration 56, loss = 0.16814230\n",
      "Iteration 57, loss = 0.11451687\n",
      "Iteration 58, loss = 0.11473129\n",
      "Iteration 59, loss = 0.12729004\n",
      "Iteration 60, loss = 0.19923685\n",
      "Iteration 61, loss = 0.15586610\n",
      "Iteration 62, loss = 0.13544289\n",
      "Iteration 63, loss = 0.14984022\n",
      "Iteration 64, loss = 0.20790286\n",
      "Iteration 65, loss = 0.14438431\n",
      "Iteration 66, loss = 0.16037596\n",
      "Iteration 67, loss = 0.15296701\n",
      "Iteration 68, loss = 0.17860615\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 69, loss = 0.12598202\n",
      "Iteration 70, loss = 0.10553412\n",
      "Iteration 71, loss = 0.08956835\n",
      "Iteration 72, loss = 0.07457059\n",
      "Iteration 73, loss = 0.07839465\n",
      "Iteration 74, loss = 0.07409012\n",
      "Iteration 75, loss = 0.07495461\n",
      "Iteration 76, loss = 0.07438615\n",
      "Iteration 77, loss = 0.06455657\n",
      "Iteration 78, loss = 0.06602548\n",
      "Iteration 79, loss = 0.06563929\n",
      "Iteration 80, loss = 0.06404139\n",
      "Iteration 81, loss = 0.06179169\n",
      "Iteration 82, loss = 0.06087578\n",
      "Iteration 83, loss = 0.06038668\n",
      "Iteration 84, loss = 0.05900566\n",
      "Iteration 85, loss = 0.05823741\n",
      "Iteration 86, loss = 0.05802757\n",
      "Iteration 87, loss = 0.05662161\n",
      "Iteration 88, loss = 0.05602996\n",
      "Iteration 89, loss = 0.05566677\n",
      "Iteration 90, loss = 0.05532720\n",
      "Iteration 91, loss = 0.05494111\n",
      "Iteration 92, loss = 0.05474714\n",
      "Iteration 93, loss = 0.05443739\n",
      "Iteration 94, loss = 0.05410375\n",
      "Iteration 95, loss = 0.05384660\n",
      "Iteration 96, loss = 0.05367825\n",
      "Iteration 97, loss = 0.05326058\n",
      "Iteration 98, loss = 0.05302594\n",
      "Iteration 99, loss = 0.05281893\n",
      "Iteration 100, loss = 0.05246520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.67667552\n",
      "Iteration 2, loss = 0.97492547\n",
      "Iteration 3, loss = 0.83735280\n",
      "Iteration 4, loss = 0.65659093\n",
      "Iteration 5, loss = 0.55401183\n",
      "Iteration 6, loss = 0.51407672\n",
      "Iteration 7, loss = 0.45092169\n",
      "Iteration 8, loss = 0.35812524\n",
      "Iteration 9, loss = 0.42242906\n",
      "Iteration 10, loss = 0.40245721\n",
      "Iteration 11, loss = 0.40097921\n",
      "Iteration 12, loss = 0.37281478\n",
      "Iteration 13, loss = 0.31830955\n",
      "Iteration 14, loss = 0.29244279\n",
      "Iteration 15, loss = 0.33248845\n",
      "Iteration 16, loss = 0.28777467\n",
      "Iteration 17, loss = 0.34405005\n",
      "Iteration 18, loss = 0.43215224\n",
      "Iteration 19, loss = 0.46888956\n",
      "Iteration 20, loss = 0.38634604\n",
      "Iteration 21, loss = 0.32333898\n",
      "Iteration 22, loss = 0.25545957\n",
      "Iteration 23, loss = 0.22259230\n",
      "Iteration 24, loss = 0.25236755\n",
      "Iteration 25, loss = 0.20305311\n",
      "Iteration 26, loss = 0.25420428\n",
      "Iteration 27, loss = 0.22562098\n",
      "Iteration 28, loss = 0.43862304\n",
      "Iteration 29, loss = 0.41607808\n",
      "Iteration 30, loss = 0.35998923\n",
      "Iteration 31, loss = 0.29929988\n",
      "Iteration 32, loss = 0.22575349\n",
      "Iteration 33, loss = 0.33337833\n",
      "Iteration 34, loss = 0.26300917\n",
      "Iteration 35, loss = 0.25034127\n",
      "Iteration 36, loss = 0.25371707\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 37, loss = 0.18255718\n",
      "Iteration 38, loss = 0.15400064\n",
      "Iteration 39, loss = 0.14753158\n",
      "Iteration 40, loss = 0.14380746\n",
      "Iteration 41, loss = 0.13951144\n",
      "Iteration 42, loss = 0.12916559\n",
      "Iteration 43, loss = 0.13436864\n",
      "Iteration 44, loss = 0.13653666\n",
      "Iteration 45, loss = 0.12162416\n",
      "Iteration 46, loss = 0.11967139\n",
      "Iteration 47, loss = 0.11749017\n",
      "Iteration 48, loss = 0.11757837\n",
      "Iteration 49, loss = 0.11082425\n",
      "Iteration 50, loss = 0.10031256\n",
      "Iteration 51, loss = 0.09750670\n",
      "Iteration 52, loss = 0.09652137\n",
      "Iteration 53, loss = 0.09774495\n",
      "Iteration 54, loss = 0.09560791\n",
      "Iteration 55, loss = 0.09521582\n",
      "Iteration 56, loss = 0.09138381\n",
      "Iteration 57, loss = 0.09094673\n",
      "Iteration 58, loss = 0.09055942\n",
      "Iteration 59, loss = 0.09043663\n",
      "Iteration 60, loss = 0.08970441\n",
      "Iteration 61, loss = 0.08914695\n",
      "Iteration 62, loss = 0.08810293\n",
      "Iteration 63, loss = 0.08827184\n",
      "Iteration 64, loss = 0.08719250\n",
      "Iteration 65, loss = 0.08794180\n",
      "Iteration 66, loss = 0.08689310\n",
      "Iteration 67, loss = 0.08971933\n",
      "Iteration 68, loss = 0.08554460\n",
      "Iteration 69, loss = 0.08207681\n",
      "Iteration 70, loss = 0.08129780\n",
      "Iteration 71, loss = 0.08006810\n",
      "Iteration 72, loss = 0.07994971\n",
      "Iteration 73, loss = 0.08025648\n",
      "Iteration 74, loss = 0.07873521\n",
      "Iteration 75, loss = 0.07799607\n",
      "Iteration 76, loss = 0.07758757\n",
      "Iteration 77, loss = 0.07723466\n",
      "Iteration 78, loss = 0.07691176\n",
      "Iteration 79, loss = 0.07656006\n",
      "Iteration 80, loss = 0.07635254\n",
      "Iteration 81, loss = 0.07607731\n",
      "Iteration 82, loss = 0.07580724\n",
      "Iteration 83, loss = 0.07545024\n",
      "Iteration 84, loss = 0.07515671\n",
      "Iteration 85, loss = 0.07475949\n",
      "Iteration 86, loss = 0.07503146\n",
      "Iteration 87, loss = 0.07466944\n",
      "Iteration 88, loss = 0.07425504\n",
      "Iteration 89, loss = 0.07255214\n",
      "Iteration 90, loss = 0.07316115\n",
      "Iteration 91, loss = 0.07264724\n",
      "Iteration 92, loss = 0.07174473\n",
      "Iteration 93, loss = 0.07201796\n",
      "Iteration 94, loss = 0.07158710\n",
      "Iteration 95, loss = 0.07129482\n",
      "Iteration 96, loss = 0.06966532\n",
      "Iteration 97, loss = 0.07023315\n",
      "Iteration 98, loss = 0.06881921\n",
      "Iteration 99, loss = 0.06884149\n",
      "Iteration 100, loss = 0.06766945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.68477080\n",
      "Iteration 2, loss = 0.99006104\n",
      "Iteration 3, loss = 0.74996448\n",
      "Iteration 4, loss = 0.68063870\n",
      "Iteration 5, loss = 0.65765223\n",
      "Iteration 6, loss = 0.52246987\n",
      "Iteration 7, loss = 0.60023087\n",
      "Iteration 8, loss = 0.54837607\n",
      "Iteration 9, loss = 0.45138169\n",
      "Iteration 10, loss = 0.55800327\n",
      "Iteration 11, loss = 0.51516254\n",
      "Iteration 12, loss = 0.52411894\n",
      "Iteration 13, loss = 0.44618596\n",
      "Iteration 14, loss = 0.50993520\n",
      "Iteration 15, loss = 0.40730973\n",
      "Iteration 16, loss = 0.40986773\n",
      "Iteration 17, loss = 0.35674700\n",
      "Iteration 18, loss = 0.37628308\n",
      "Iteration 19, loss = 0.27051620\n",
      "Iteration 20, loss = 0.27841942\n",
      "Iteration 21, loss = 0.35060097\n",
      "Iteration 22, loss = 0.36732118\n",
      "Iteration 23, loss = 0.32466746\n",
      "Iteration 24, loss = 0.32321081\n",
      "Iteration 25, loss = 0.25821343\n",
      "Iteration 26, loss = 0.25491029\n",
      "Iteration 27, loss = 0.27201468\n",
      "Iteration 28, loss = 0.21723860\n",
      "Iteration 29, loss = 0.21096002\n",
      "Iteration 30, loss = 0.22087670\n",
      "Iteration 31, loss = 0.19055501\n",
      "Iteration 32, loss = 0.20463144\n",
      "Iteration 33, loss = 0.33772473\n",
      "Iteration 34, loss = 0.25116297\n",
      "Iteration 35, loss = 0.28603463\n",
      "Iteration 36, loss = 0.27743613\n",
      "Iteration 37, loss = 0.20032235\n",
      "Iteration 38, loss = 0.12670125\n",
      "Iteration 39, loss = 0.17216324\n",
      "Iteration 40, loss = 0.16317037\n",
      "Iteration 41, loss = 0.14624027\n",
      "Iteration 42, loss = 0.20240100\n",
      "Iteration 43, loss = 0.30328347\n",
      "Iteration 44, loss = 0.13465885\n",
      "Iteration 45, loss = 0.15543861\n",
      "Iteration 46, loss = 0.19012481\n",
      "Iteration 47, loss = 0.22136033\n",
      "Iteration 48, loss = 0.18181211\n",
      "Iteration 49, loss = 0.18653166\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 50, loss = 0.25564188\n",
      "Iteration 51, loss = 0.18576650\n",
      "Iteration 52, loss = 0.13245208\n",
      "Iteration 53, loss = 0.12497264\n",
      "Iteration 54, loss = 0.11613348\n",
      "Iteration 55, loss = 0.11967843\n",
      "Iteration 56, loss = 0.11195189\n",
      "Iteration 57, loss = 0.10780144\n",
      "Iteration 58, loss = 0.09985751\n",
      "Iteration 59, loss = 0.09351695\n",
      "Iteration 60, loss = 0.08931277\n",
      "Iteration 61, loss = 0.08516773\n",
      "Iteration 62, loss = 0.08256817\n",
      "Iteration 63, loss = 0.08075553\n",
      "Iteration 64, loss = 0.07858748\n",
      "Iteration 65, loss = 0.07762101\n",
      "Iteration 66, loss = 0.07689085\n",
      "Iteration 67, loss = 0.07627049\n",
      "Iteration 68, loss = 0.07568896\n",
      "Iteration 69, loss = 0.07643344\n",
      "Iteration 70, loss = 0.07520479\n",
      "Iteration 71, loss = 0.07459564\n",
      "Iteration 72, loss = 0.07414010\n",
      "Iteration 73, loss = 0.07396934\n",
      "Iteration 74, loss = 0.07278200\n",
      "Iteration 75, loss = 0.07204032\n",
      "Iteration 76, loss = 0.07168221\n",
      "Iteration 77, loss = 0.07125236\n",
      "Iteration 78, loss = 0.07091175\n",
      "Iteration 79, loss = 0.07068716\n",
      "Iteration 80, loss = 0.07030919\n",
      "Iteration 81, loss = 0.07005666\n",
      "Iteration 82, loss = 0.06977562\n",
      "Iteration 83, loss = 0.06954342\n",
      "Iteration 84, loss = 0.06928890\n",
      "Iteration 85, loss = 0.06906348\n",
      "Iteration 86, loss = 0.06879409\n",
      "Iteration 87, loss = 0.06859200\n",
      "Iteration 88, loss = 0.06836347\n",
      "Iteration 89, loss = 0.06815572\n",
      "Iteration 90, loss = 0.06791397\n",
      "Iteration 91, loss = 0.06776188\n",
      "Iteration 92, loss = 0.06751780\n",
      "Iteration 93, loss = 0.06733270\n",
      "Iteration 94, loss = 0.06710368\n",
      "Iteration 95, loss = 0.06689880\n",
      "Iteration 96, loss = 0.06675308\n",
      "Iteration 97, loss = 0.06575732\n",
      "Iteration 98, loss = 0.06550772\n",
      "Iteration 99, loss = 0.06536589\n",
      "Iteration 100, loss = 0.06511003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.68523782\n",
      "Iteration 2, loss = 1.24399174\n",
      "Iteration 3, loss = 0.89600047\n",
      "Iteration 4, loss = 0.65840100\n",
      "Iteration 5, loss = 0.60611751\n",
      "Iteration 6, loss = 0.42211430\n",
      "Iteration 7, loss = 0.33376516\n",
      "Iteration 8, loss = 0.42986302\n",
      "Iteration 9, loss = 0.43962624\n",
      "Iteration 10, loss = 0.41675696\n",
      "Iteration 11, loss = 0.45620161\n",
      "Iteration 12, loss = 0.34224701\n",
      "Iteration 13, loss = 0.37539417\n",
      "Iteration 14, loss = 0.39740127\n",
      "Iteration 15, loss = 0.39156556\n",
      "Iteration 16, loss = 0.41640145\n",
      "Iteration 17, loss = 0.26600365\n",
      "Iteration 18, loss = 0.26613362\n",
      "Iteration 19, loss = 0.30673777\n",
      "Iteration 20, loss = 0.34968390\n",
      "Iteration 21, loss = 0.37770795\n",
      "Iteration 22, loss = 0.32156862\n",
      "Iteration 23, loss = 0.31023055\n",
      "Iteration 24, loss = 0.34619061\n",
      "Iteration 25, loss = 0.24489334\n",
      "Iteration 26, loss = 0.25392964\n",
      "Iteration 27, loss = 0.20954641\n",
      "Iteration 28, loss = 0.21019963\n",
      "Iteration 29, loss = 0.28799445\n",
      "Iteration 30, loss = 0.25556257\n",
      "Iteration 31, loss = 0.26417821\n",
      "Iteration 32, loss = 0.18553188\n",
      "Iteration 33, loss = 0.15441484\n",
      "Iteration 34, loss = 0.16468658\n",
      "Iteration 35, loss = 0.18836861\n",
      "Iteration 36, loss = 0.15084438\n",
      "Iteration 37, loss = 0.17441846\n",
      "Iteration 38, loss = 0.24591593\n",
      "Iteration 39, loss = 0.39400267\n",
      "Iteration 40, loss = 0.24031431\n",
      "Iteration 41, loss = 0.19000643\n",
      "Iteration 42, loss = 0.17848062\n",
      "Iteration 43, loss = 0.15036773\n",
      "Iteration 44, loss = 0.16496775\n",
      "Iteration 45, loss = 0.34683338\n",
      "Iteration 46, loss = 0.28248795\n",
      "Iteration 47, loss = 0.23263266\n",
      "Iteration 48, loss = 0.12935016\n",
      "Iteration 49, loss = 0.18026874\n",
      "Iteration 50, loss = 0.27381220\n",
      "Iteration 51, loss = 0.17241804\n",
      "Iteration 52, loss = 0.12288690\n",
      "Iteration 53, loss = 0.10704549\n",
      "Iteration 54, loss = 0.10860676\n",
      "Iteration 55, loss = 0.10073993\n",
      "Iteration 56, loss = 0.10137295\n",
      "Iteration 57, loss = 0.12171847\n",
      "Iteration 58, loss = 0.10802363\n",
      "Iteration 59, loss = 0.09476463\n",
      "Iteration 60, loss = 0.09739385\n",
      "Iteration 61, loss = 0.15239021\n",
      "Iteration 62, loss = 0.11337502\n",
      "Iteration 63, loss = 0.10322082\n",
      "Iteration 64, loss = 0.18991092\n",
      "Iteration 65, loss = 0.19152899\n",
      "Iteration 66, loss = 0.14483819\n",
      "Iteration 67, loss = 0.11607502\n",
      "Iteration 68, loss = 0.11805419\n",
      "Iteration 69, loss = 0.21771955\n",
      "Iteration 70, loss = 0.17012874\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 71, loss = 0.15775527\n",
      "Iteration 72, loss = 0.14081043\n",
      "Iteration 73, loss = 0.13033700\n",
      "Iteration 74, loss = 0.12563759\n",
      "Iteration 75, loss = 0.09886340\n",
      "Iteration 76, loss = 0.09742658\n",
      "Iteration 77, loss = 0.09217203\n",
      "Iteration 78, loss = 0.08980335\n",
      "Iteration 79, loss = 0.08619279\n",
      "Iteration 80, loss = 0.07851596\n",
      "Iteration 81, loss = 0.07813573\n",
      "Iteration 82, loss = 0.07725541\n",
      "Iteration 83, loss = 0.07266221\n",
      "Iteration 84, loss = 0.06994600\n",
      "Iteration 85, loss = 0.06790661\n",
      "Iteration 86, loss = 0.06548478\n",
      "Iteration 87, loss = 0.06455128\n",
      "Iteration 88, loss = 0.06378081\n",
      "Iteration 89, loss = 0.06316364\n",
      "Iteration 90, loss = 0.06214060\n",
      "Iteration 91, loss = 0.06143813\n",
      "Iteration 92, loss = 0.06104445\n",
      "Iteration 93, loss = 0.06046079\n",
      "Iteration 94, loss = 0.06004881\n",
      "Iteration 95, loss = 0.05938441\n",
      "Iteration 96, loss = 0.05884264\n",
      "Iteration 97, loss = 0.05823645\n",
      "Iteration 98, loss = 0.05793016\n",
      "Iteration 99, loss = 0.05729451\n",
      "Iteration 100, loss = 0.05689323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.48706775\n",
      "Iteration 2, loss = 0.81414827\n",
      "Iteration 3, loss = 0.78492835\n",
      "Iteration 4, loss = 0.58551022\n",
      "Iteration 5, loss = 0.54521908\n",
      "Iteration 6, loss = 0.52873040\n",
      "Iteration 7, loss = 0.51658853\n",
      "Iteration 8, loss = 0.53941594\n",
      "Iteration 9, loss = 0.46384613\n",
      "Iteration 10, loss = 0.40897386\n",
      "Iteration 11, loss = 0.38856177\n",
      "Iteration 12, loss = 0.31897256\n",
      "Iteration 13, loss = 0.36699552\n",
      "Iteration 14, loss = 0.43040435\n",
      "Iteration 15, loss = 0.39789227\n",
      "Iteration 16, loss = 0.45820244\n",
      "Iteration 17, loss = 0.40895127\n",
      "Iteration 18, loss = 0.38983467\n",
      "Iteration 19, loss = 0.43813205\n",
      "Iteration 20, loss = 0.37416070\n",
      "Iteration 21, loss = 0.31573588\n",
      "Iteration 22, loss = 0.27038947\n",
      "Iteration 23, loss = 0.33334954\n",
      "Iteration 24, loss = 0.40219542\n",
      "Iteration 25, loss = 0.33659099\n",
      "Iteration 26, loss = 0.29633903\n",
      "Iteration 27, loss = 0.36603101\n",
      "Iteration 28, loss = 0.50106892\n",
      "Iteration 29, loss = 0.35578972\n",
      "Iteration 30, loss = 0.34559446\n",
      "Iteration 31, loss = 0.22095085\n",
      "Iteration 32, loss = 0.30636114\n",
      "Iteration 33, loss = 0.28738793\n",
      "Iteration 34, loss = 0.23001820\n",
      "Iteration 35, loss = 0.25397195\n",
      "Iteration 36, loss = 0.25428156\n",
      "Iteration 37, loss = 0.25531242\n",
      "Iteration 38, loss = 0.19363088\n",
      "Iteration 39, loss = 0.29837502\n",
      "Iteration 40, loss = 0.24618210\n",
      "Iteration 41, loss = 0.21413608\n",
      "Iteration 42, loss = 0.25534752\n",
      "Iteration 43, loss = 0.27242215\n",
      "Iteration 44, loss = 0.22157319\n",
      "Iteration 45, loss = 0.20220914\n",
      "Iteration 46, loss = 0.18174578\n",
      "Iteration 47, loss = 0.12411820\n",
      "Iteration 48, loss = 0.15384645\n",
      "Iteration 49, loss = 0.22277250\n",
      "Iteration 50, loss = 0.34719977\n",
      "Iteration 51, loss = 0.35593100\n",
      "Iteration 52, loss = 0.31791791\n",
      "Iteration 53, loss = 0.31442894\n",
      "Iteration 54, loss = 0.24810708\n",
      "Iteration 55, loss = 0.24140793\n",
      "Iteration 56, loss = 0.19635374\n",
      "Iteration 57, loss = 0.27842754\n",
      "Iteration 58, loss = 0.21065033\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 59, loss = 0.25257146\n",
      "Iteration 60, loss = 0.17918682\n",
      "Iteration 61, loss = 0.16474471\n",
      "Iteration 62, loss = 0.14709415\n",
      "Iteration 63, loss = 0.14437416\n",
      "Iteration 64, loss = 0.13223267\n",
      "Iteration 65, loss = 0.13280358\n",
      "Iteration 66, loss = 0.12963067\n",
      "Iteration 67, loss = 0.13052504\n",
      "Iteration 68, loss = 0.12101787\n",
      "Iteration 69, loss = 0.12054636\n",
      "Iteration 70, loss = 0.11096381\n",
      "Iteration 71, loss = 0.10934725\n",
      "Iteration 72, loss = 0.10781750\n",
      "Iteration 73, loss = 0.10644475\n",
      "Iteration 74, loss = 0.10616831\n",
      "Iteration 75, loss = 0.10918145\n",
      "Iteration 76, loss = 0.10432789\n",
      "Iteration 77, loss = 0.10429390\n",
      "Iteration 78, loss = 0.10125851\n",
      "Iteration 79, loss = 0.10111642\n",
      "Iteration 80, loss = 0.09881123\n",
      "Iteration 81, loss = 0.09769057\n",
      "Iteration 82, loss = 0.09596012\n",
      "Iteration 83, loss = 0.09577735\n",
      "Iteration 84, loss = 0.09498300\n",
      "Iteration 85, loss = 0.09337479\n",
      "Iteration 86, loss = 0.09756656\n",
      "Iteration 87, loss = 0.09642702\n",
      "Iteration 88, loss = 0.09298063\n",
      "Iteration 89, loss = 0.09242463\n",
      "Iteration 90, loss = 0.09362789\n",
      "Iteration 91, loss = 0.09395694\n",
      "Iteration 92, loss = 0.09036410\n",
      "Iteration 93, loss = 0.09108945\n",
      "Iteration 94, loss = 0.09010357\n",
      "Iteration 95, loss = 0.08829745\n",
      "Iteration 96, loss = 0.08709322\n",
      "Iteration 97, loss = 0.08858240\n",
      "Iteration 98, loss = 0.08568033\n",
      "Iteration 99, loss = 0.08512340\n",
      "Iteration 100, loss = 0.08448925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.56115121\n",
      "Iteration 2, loss = 0.91314127\n",
      "Iteration 3, loss = 0.71655902\n",
      "Iteration 4, loss = 0.70216802\n",
      "Iteration 5, loss = 0.64794662\n",
      "Iteration 6, loss = 0.61791985\n",
      "Iteration 7, loss = 0.56237612\n",
      "Iteration 8, loss = 0.43652215\n",
      "Iteration 9, loss = 0.60232119\n",
      "Iteration 10, loss = 0.46842726\n",
      "Iteration 11, loss = 0.38277730\n",
      "Iteration 12, loss = 0.38810751\n",
      "Iteration 13, loss = 0.39564676\n",
      "Iteration 14, loss = 0.40743229\n",
      "Iteration 15, loss = 0.50641753\n",
      "Iteration 16, loss = 0.41054260\n",
      "Iteration 17, loss = 0.27459368\n",
      "Iteration 18, loss = 0.36685731\n",
      "Iteration 19, loss = 0.44712414\n",
      "Iteration 20, loss = 0.38616771\n",
      "Iteration 21, loss = 0.29432929\n",
      "Iteration 22, loss = 0.31734395\n",
      "Iteration 23, loss = 0.22573785\n",
      "Iteration 24, loss = 0.35418208\n",
      "Iteration 25, loss = 0.27938144\n",
      "Iteration 26, loss = 0.21587293\n",
      "Iteration 27, loss = 0.20343855\n",
      "Iteration 28, loss = 0.28601328\n",
      "Iteration 29, loss = 0.21476973\n",
      "Iteration 30, loss = 0.19396949\n",
      "Iteration 31, loss = 0.30061693\n",
      "Iteration 32, loss = 0.38304095\n",
      "Iteration 33, loss = 0.26205643\n",
      "Iteration 34, loss = 0.50843841\n",
      "Iteration 35, loss = 0.33270170\n",
      "Iteration 36, loss = 0.37162569\n",
      "Iteration 37, loss = 0.35029693\n",
      "Iteration 38, loss = 0.44282792\n",
      "Iteration 39, loss = 0.38443074\n",
      "Iteration 40, loss = 0.35952839\n",
      "Iteration 41, loss = 0.37274426\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 42, loss = 0.37917879\n",
      "Iteration 43, loss = 0.24652114\n",
      "Iteration 44, loss = 0.21077837\n",
      "Iteration 45, loss = 0.18875632\n",
      "Iteration 46, loss = 0.18644074\n",
      "Iteration 47, loss = 0.16928061\n",
      "Iteration 48, loss = 0.15455279\n",
      "Iteration 49, loss = 0.14648817\n",
      "Iteration 50, loss = 0.13789583\n",
      "Iteration 51, loss = 0.12763511\n",
      "Iteration 52, loss = 0.12917703\n",
      "Iteration 53, loss = 0.12679486\n",
      "Iteration 54, loss = 0.13131784\n",
      "Iteration 55, loss = 0.12576842\n",
      "Iteration 56, loss = 0.12343316\n",
      "Iteration 57, loss = 0.11918640\n",
      "Iteration 58, loss = 0.11640396\n",
      "Iteration 59, loss = 0.11334845\n",
      "Iteration 60, loss = 0.11436773\n",
      "Iteration 61, loss = 0.11352275\n",
      "Iteration 62, loss = 0.11241897\n",
      "Iteration 63, loss = 0.10855411\n",
      "Iteration 64, loss = 0.10872352\n",
      "Iteration 65, loss = 0.10659827\n",
      "Iteration 66, loss = 0.10522467\n",
      "Iteration 67, loss = 0.10427841\n",
      "Iteration 68, loss = 0.10593800\n",
      "Iteration 69, loss = 0.10282044\n",
      "Iteration 70, loss = 0.10174485\n",
      "Iteration 71, loss = 0.10123326\n",
      "Iteration 72, loss = 0.10060293\n",
      "Iteration 73, loss = 0.09974532\n",
      "Iteration 74, loss = 0.10018747\n",
      "Iteration 75, loss = 0.09859353\n",
      "Iteration 76, loss = 0.09823305\n",
      "Iteration 77, loss = 0.09784150\n",
      "Iteration 78, loss = 0.09738854\n",
      "Iteration 79, loss = 0.09698208\n",
      "Iteration 80, loss = 0.09621869\n",
      "Iteration 81, loss = 0.09665585\n",
      "Iteration 82, loss = 0.09599895\n",
      "Iteration 83, loss = 0.09541891\n",
      "Iteration 84, loss = 0.09510961\n",
      "Iteration 85, loss = 0.09467436\n",
      "Iteration 86, loss = 0.09439743\n",
      "Iteration 87, loss = 0.09385116\n",
      "Iteration 88, loss = 0.09350209\n",
      "Iteration 89, loss = 0.09310616\n",
      "Iteration 90, loss = 0.09270642\n",
      "Iteration 91, loss = 0.09220169\n",
      "Iteration 92, loss = 0.09193852\n",
      "Iteration 93, loss = 0.09140575\n",
      "Iteration 94, loss = 0.09137153\n",
      "Iteration 95, loss = 0.09110915\n",
      "Iteration 96, loss = 0.09078465\n",
      "Iteration 97, loss = 0.09054318\n",
      "Iteration 98, loss = 0.09021356\n",
      "Iteration 99, loss = 0.08961127\n",
      "Iteration 100, loss = 0.08980216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.57094357\n",
      "Iteration 2, loss = 0.95881769\n",
      "Iteration 3, loss = 0.83578645\n",
      "Iteration 4, loss = 0.69749950\n",
      "Iteration 5, loss = 0.70992069\n",
      "Iteration 6, loss = 0.61785309\n",
      "Iteration 7, loss = 0.58976568\n",
      "Iteration 8, loss = 0.46553437\n",
      "Iteration 9, loss = 0.49885060\n",
      "Iteration 10, loss = 0.52639931\n",
      "Iteration 11, loss = 0.50393200\n",
      "Iteration 12, loss = 0.43369391\n",
      "Iteration 13, loss = 0.45601949\n",
      "Iteration 14, loss = 0.34757584\n",
      "Iteration 15, loss = 0.45829769\n",
      "Iteration 16, loss = 0.38685564\n",
      "Iteration 17, loss = 0.38614587\n",
      "Iteration 18, loss = 0.37707989\n",
      "Iteration 19, loss = 0.38080760\n",
      "Iteration 20, loss = 0.35760029\n",
      "Iteration 21, loss = 0.32026806\n",
      "Iteration 22, loss = 0.40921927\n",
      "Iteration 23, loss = 0.43654860\n",
      "Iteration 24, loss = 0.37156910\n",
      "Iteration 25, loss = 0.42989976\n",
      "Iteration 26, loss = 0.39536865\n",
      "Iteration 27, loss = 0.49875587\n",
      "Iteration 28, loss = 0.47227968\n",
      "Iteration 29, loss = 0.32758033\n",
      "Iteration 30, loss = 0.31906614\n",
      "Iteration 31, loss = 0.29437140\n",
      "Iteration 32, loss = 0.26560950\n",
      "Iteration 33, loss = 0.30618641\n",
      "Iteration 34, loss = 0.30160470\n",
      "Iteration 35, loss = 0.38191908\n",
      "Iteration 36, loss = 0.31683010\n",
      "Iteration 37, loss = 0.27165780\n",
      "Iteration 38, loss = 0.24166132\n",
      "Iteration 39, loss = 0.22170707\n",
      "Iteration 40, loss = 0.29012739\n",
      "Iteration 41, loss = 0.25989351\n",
      "Iteration 42, loss = 0.27257864\n",
      "Iteration 43, loss = 0.30685453\n",
      "Iteration 44, loss = 0.28220020\n",
      "Iteration 45, loss = 0.22831476\n",
      "Iteration 46, loss = 0.26732221\n",
      "Iteration 47, loss = 0.25997154\n",
      "Iteration 48, loss = 0.31036558\n",
      "Iteration 49, loss = 0.23677077\n",
      "Iteration 50, loss = 0.27796339\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 51, loss = 0.29174559\n",
      "Iteration 52, loss = 0.23227034\n",
      "Iteration 53, loss = 0.19692082\n",
      "Iteration 54, loss = 0.16818252\n",
      "Iteration 55, loss = 0.16061786\n",
      "Iteration 56, loss = 0.14775660\n",
      "Iteration 57, loss = 0.14619622\n",
      "Iteration 58, loss = 0.14237076\n",
      "Iteration 59, loss = 0.12958669\n",
      "Iteration 60, loss = 0.14479759\n",
      "Iteration 61, loss = 0.13524324\n",
      "Iteration 62, loss = 0.11992763\n",
      "Iteration 63, loss = 0.12059904\n",
      "Iteration 64, loss = 0.11314200\n",
      "Iteration 65, loss = 0.11366768\n",
      "Iteration 66, loss = 0.11606518\n",
      "Iteration 67, loss = 0.11178187\n",
      "Iteration 68, loss = 0.11066382\n",
      "Iteration 69, loss = 0.10974452\n",
      "Iteration 70, loss = 0.10771925\n",
      "Iteration 71, loss = 0.10755905\n",
      "Iteration 72, loss = 0.10661499\n",
      "Iteration 73, loss = 0.10575671\n",
      "Iteration 74, loss = 0.10535955\n",
      "Iteration 75, loss = 0.10498674\n",
      "Iteration 76, loss = 0.10375777\n",
      "Iteration 77, loss = 0.10379950\n",
      "Iteration 78, loss = 0.10268328\n",
      "Iteration 79, loss = 0.10228959\n",
      "Iteration 80, loss = 0.10195043\n",
      "Iteration 81, loss = 0.10172873\n",
      "Iteration 82, loss = 0.10104127\n",
      "Iteration 83, loss = 0.10066925\n",
      "Iteration 84, loss = 0.10031394\n",
      "Iteration 85, loss = 0.10017874\n",
      "Iteration 86, loss = 0.09951144\n",
      "Iteration 87, loss = 0.09922673\n",
      "Iteration 88, loss = 0.09844978\n",
      "Iteration 89, loss = 0.09814391\n",
      "Iteration 90, loss = 0.09820541\n",
      "Iteration 91, loss = 0.09865703\n",
      "Iteration 92, loss = 0.09695720\n",
      "Iteration 93, loss = 0.09612701\n",
      "Iteration 94, loss = 0.09591774\n",
      "Iteration 95, loss = 0.09541450\n",
      "Iteration 96, loss = 0.09520977\n",
      "Iteration 97, loss = 0.09514432\n",
      "Iteration 98, loss = 0.09484896\n",
      "Iteration 99, loss = 0.09477286\n",
      "Iteration 100, loss = 0.09396677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.63089645\n",
      "Iteration 2, loss = 1.06270918\n",
      "Iteration 3, loss = 0.91026089\n",
      "Iteration 4, loss = 0.72104913\n",
      "Iteration 5, loss = 0.62919617\n",
      "Iteration 6, loss = 0.54833328\n",
      "Iteration 7, loss = 0.55491071\n",
      "Iteration 8, loss = 0.48532728\n",
      "Iteration 9, loss = 0.65426977\n",
      "Iteration 10, loss = 0.43994526\n",
      "Iteration 11, loss = 0.40651815\n",
      "Iteration 12, loss = 0.44782122\n",
      "Iteration 13, loss = 0.52390417\n",
      "Iteration 14, loss = 0.55341994\n",
      "Iteration 15, loss = 0.49527984\n",
      "Iteration 16, loss = 0.56043062\n",
      "Iteration 17, loss = 0.63365811\n",
      "Iteration 18, loss = 0.51211958\n",
      "Iteration 19, loss = 0.61374354\n",
      "Iteration 20, loss = 0.61577006\n",
      "Iteration 21, loss = 0.50149036\n",
      "Iteration 22, loss = 0.61024360\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 23, loss = 0.56447622\n",
      "Iteration 24, loss = 0.43766512\n",
      "Iteration 25, loss = 0.39025197\n",
      "Iteration 26, loss = 0.37597768\n",
      "Iteration 27, loss = 0.35144886\n",
      "Iteration 28, loss = 0.34509908\n",
      "Iteration 29, loss = 0.33042158\n",
      "Iteration 30, loss = 0.32562262\n",
      "Iteration 31, loss = 0.31988249\n",
      "Iteration 32, loss = 0.29894481\n",
      "Iteration 33, loss = 0.29625497\n",
      "Iteration 34, loss = 0.29752672\n",
      "Iteration 35, loss = 0.31182760\n",
      "Iteration 36, loss = 0.28470429\n",
      "Iteration 37, loss = 0.30921798\n",
      "Iteration 38, loss = 0.28871408\n",
      "Iteration 39, loss = 0.28737521\n",
      "Iteration 40, loss = 0.27943221\n",
      "Iteration 41, loss = 0.26769434\n",
      "Iteration 42, loss = 0.28640655\n",
      "Iteration 43, loss = 0.25702929\n",
      "Iteration 44, loss = 0.25203368\n",
      "Iteration 45, loss = 0.24355066\n",
      "Iteration 46, loss = 0.24762817\n",
      "Iteration 47, loss = 0.23620205\n",
      "Iteration 48, loss = 0.24043316\n",
      "Iteration 49, loss = 0.23366536\n",
      "Iteration 50, loss = 0.23256285\n",
      "Iteration 51, loss = 0.23356177\n",
      "Iteration 52, loss = 0.23157151\n",
      "Iteration 53, loss = 0.22124409\n",
      "Iteration 54, loss = 0.22394680\n",
      "Iteration 55, loss = 0.21926054\n",
      "Iteration 56, loss = 0.22250470\n",
      "Iteration 57, loss = 0.21496051\n",
      "Iteration 58, loss = 0.21298497\n",
      "Iteration 59, loss = 0.21417563\n",
      "Iteration 60, loss = 0.21167066\n",
      "Iteration 61, loss = 0.20910863\n",
      "Iteration 62, loss = 0.20659976\n",
      "Iteration 63, loss = 0.20459630\n",
      "Iteration 64, loss = 0.20497181\n",
      "Iteration 65, loss = 0.20155282\n",
      "Iteration 66, loss = 0.20050708\n",
      "Iteration 67, loss = 0.19831636\n",
      "Iteration 68, loss = 0.20433625\n",
      "Iteration 69, loss = 0.19856981\n",
      "Iteration 70, loss = 0.19413175\n",
      "Iteration 71, loss = 0.20134596\n",
      "Iteration 72, loss = 0.19478452\n",
      "Iteration 73, loss = 0.19699267\n",
      "Iteration 74, loss = 0.19400704\n",
      "Iteration 75, loss = 0.19073201\n",
      "Iteration 76, loss = 0.18971618\n",
      "Iteration 77, loss = 0.19715176\n",
      "Iteration 78, loss = 0.19905337\n",
      "Iteration 79, loss = 0.19295198\n",
      "Iteration 80, loss = 0.18940033\n",
      "Iteration 81, loss = 0.18900662\n",
      "Iteration 82, loss = 0.19467243\n",
      "Iteration 83, loss = 0.18850362\n",
      "Iteration 84, loss = 0.19109765\n",
      "Iteration 85, loss = 0.20490668\n",
      "Iteration 86, loss = 0.18984657\n",
      "Iteration 87, loss = 0.18442793\n",
      "Iteration 88, loss = 0.18621097\n",
      "Iteration 89, loss = 0.19389668\n",
      "Iteration 90, loss = 0.18840111\n",
      "Iteration 91, loss = 0.18696020\n",
      "Iteration 92, loss = 0.17512044\n",
      "Iteration 93, loss = 0.18406201\n",
      "Iteration 94, loss = 0.17522299\n",
      "Iteration 95, loss = 0.18398440\n",
      "Iteration 96, loss = 0.19170806\n",
      "Iteration 97, loss = 0.19416391\n",
      "Iteration 98, loss = 0.17905474\n",
      "Iteration 99, loss = 0.16896517\n",
      "Iteration 100, loss = 0.17240081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.50115893\n",
      "Iteration 2, loss = 0.84049456\n",
      "Iteration 3, loss = 0.68313839\n",
      "Iteration 4, loss = 0.64028247\n",
      "Iteration 5, loss = 0.60058450\n",
      "Iteration 6, loss = 0.75005639\n",
      "Iteration 7, loss = 0.56055903\n",
      "Iteration 8, loss = 0.46794254\n",
      "Iteration 9, loss = 0.45333073\n",
      "Iteration 10, loss = 0.41731068\n",
      "Iteration 11, loss = 0.36518300\n",
      "Iteration 12, loss = 0.40907866\n",
      "Iteration 13, loss = 0.39301659\n",
      "Iteration 14, loss = 0.36768134\n",
      "Iteration 15, loss = 0.31313754\n",
      "Iteration 16, loss = 0.40004784\n",
      "Iteration 17, loss = 0.28085279\n",
      "Iteration 18, loss = 0.42683353\n",
      "Iteration 19, loss = 0.33674626\n",
      "Iteration 20, loss = 0.37347421\n",
      "Iteration 21, loss = 0.31085984\n",
      "Iteration 22, loss = 0.27130599\n",
      "Iteration 23, loss = 0.27565067\n",
      "Iteration 24, loss = 0.21843474\n",
      "Iteration 25, loss = 0.21877438\n",
      "Iteration 26, loss = 0.23601246\n",
      "Iteration 27, loss = 0.25313337\n",
      "Iteration 28, loss = 0.30292373\n",
      "Iteration 29, loss = 0.45008674\n",
      "Iteration 30, loss = 0.29970810\n",
      "Iteration 31, loss = 0.29430081\n",
      "Iteration 32, loss = 0.30831572\n",
      "Iteration 33, loss = 0.35446457\n",
      "Iteration 34, loss = 0.35933975\n",
      "Iteration 35, loss = 0.23335922\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 36, loss = 0.18440590\n",
      "Iteration 37, loss = 0.14536085\n",
      "Iteration 38, loss = 0.13904309\n",
      "Iteration 39, loss = 0.13535369\n",
      "Iteration 40, loss = 0.12310074\n",
      "Iteration 41, loss = 0.11835447\n",
      "Iteration 42, loss = 0.11553092\n",
      "Iteration 43, loss = 0.11297659\n",
      "Iteration 44, loss = 0.11188536\n",
      "Iteration 45, loss = 0.10893792\n",
      "Iteration 46, loss = 0.10961734\n",
      "Iteration 47, loss = 0.11109629\n",
      "Iteration 48, loss = 0.10762666\n",
      "Iteration 49, loss = 0.10129167\n",
      "Iteration 50, loss = 0.10313023\n",
      "Iteration 51, loss = 0.09798787\n",
      "Iteration 52, loss = 0.09669907\n",
      "Iteration 53, loss = 0.09568361\n",
      "Iteration 54, loss = 0.09497249\n",
      "Iteration 55, loss = 0.09405624\n",
      "Iteration 56, loss = 0.09370128\n",
      "Iteration 57, loss = 0.09196307\n",
      "Iteration 58, loss = 0.09107995\n",
      "Iteration 59, loss = 0.09047969\n",
      "Iteration 60, loss = 0.09004556\n",
      "Iteration 61, loss = 0.08942691\n",
      "Iteration 62, loss = 0.08904868\n",
      "Iteration 63, loss = 0.09011416\n",
      "Iteration 64, loss = 0.08911420\n",
      "Iteration 65, loss = 0.08846314\n",
      "Iteration 66, loss = 0.08782262\n",
      "Iteration 67, loss = 0.08882720\n",
      "Iteration 68, loss = 0.08652016\n",
      "Iteration 69, loss = 0.08586208\n",
      "Iteration 70, loss = 0.08548339\n",
      "Iteration 71, loss = 0.08519877\n",
      "Iteration 72, loss = 0.08538235\n",
      "Iteration 73, loss = 0.08420879\n",
      "Iteration 74, loss = 0.08576663\n",
      "Iteration 75, loss = 0.08704275\n",
      "Iteration 76, loss = 0.08327349\n",
      "Iteration 77, loss = 0.08264011\n",
      "Iteration 78, loss = 0.08210861\n",
      "Iteration 79, loss = 0.08173903\n",
      "Iteration 80, loss = 0.08143270\n",
      "Iteration 81, loss = 0.08112816\n",
      "Iteration 82, loss = 0.08086331\n",
      "Iteration 83, loss = 0.08059199\n",
      "Iteration 84, loss = 0.08063076\n",
      "Iteration 85, loss = 0.08027322\n",
      "Iteration 86, loss = 0.07993449\n",
      "Iteration 87, loss = 0.07963155\n",
      "Iteration 88, loss = 0.07928263\n",
      "Iteration 89, loss = 0.07854687\n",
      "Iteration 90, loss = 0.07804080\n",
      "Iteration 91, loss = 0.07782343\n",
      "Iteration 92, loss = 0.07754392\n",
      "Iteration 93, loss = 0.07727085\n",
      "Iteration 94, loss = 0.07705932\n",
      "Iteration 95, loss = 0.07683743\n",
      "Iteration 96, loss = 0.07662250\n",
      "Iteration 97, loss = 0.07637962\n",
      "Iteration 98, loss = 0.07610287\n",
      "Iteration 99, loss = 0.07586275\n",
      "Iteration 100, loss = 0.07555094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.30422082\n",
      "Iteration 2, loss = 0.73901445\n",
      "Iteration 3, loss = 0.62319357\n",
      "Iteration 4, loss = 0.54147685\n",
      "Iteration 5, loss = 0.47047853\n",
      "Iteration 6, loss = 0.48607000\n",
      "Iteration 7, loss = 0.41746889\n",
      "Iteration 8, loss = 0.45767500\n",
      "Iteration 9, loss = 0.51422577\n",
      "Iteration 10, loss = 0.34852118\n",
      "Iteration 11, loss = 0.42916721\n",
      "Iteration 12, loss = 0.57241173\n",
      "Iteration 13, loss = 0.54460628\n",
      "Iteration 14, loss = 0.42723007\n",
      "Iteration 15, loss = 0.36463016\n",
      "Iteration 16, loss = 0.29558210\n",
      "Iteration 17, loss = 0.42010594\n",
      "Iteration 18, loss = 0.34281445\n",
      "Iteration 19, loss = 0.21217077\n",
      "Iteration 20, loss = 0.21017771\n",
      "Iteration 21, loss = 0.19909506\n",
      "Iteration 22, loss = 0.24293272\n",
      "Iteration 23, loss = 0.31183367\n",
      "Iteration 24, loss = 0.31961356\n",
      "Iteration 25, loss = 0.28867074\n",
      "Iteration 26, loss = 0.32717826\n",
      "Iteration 27, loss = 0.21018387\n",
      "Iteration 28, loss = 0.22131014\n",
      "Iteration 29, loss = 0.29875966\n",
      "Iteration 30, loss = 0.36348193\n",
      "Iteration 31, loss = 0.26831250\n",
      "Iteration 32, loss = 0.25814346\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 33, loss = 0.21404921\n",
      "Iteration 34, loss = 0.17891675\n",
      "Iteration 35, loss = 0.16311852\n",
      "Iteration 36, loss = 0.16567421\n",
      "Iteration 37, loss = 0.14310113\n",
      "Iteration 38, loss = 0.13224657\n",
      "Iteration 39, loss = 0.13133336\n",
      "Iteration 40, loss = 0.12719169\n",
      "Iteration 41, loss = 0.12400924\n",
      "Iteration 42, loss = 0.12176222\n",
      "Iteration 43, loss = 0.11713552\n",
      "Iteration 44, loss = 0.11513748\n",
      "Iteration 45, loss = 0.11144872\n",
      "Iteration 46, loss = 0.11044747\n",
      "Iteration 47, loss = 0.10941550\n",
      "Iteration 48, loss = 0.10761424\n",
      "Iteration 49, loss = 0.10592257\n",
      "Iteration 50, loss = 0.10511438\n",
      "Iteration 51, loss = 0.10424999\n",
      "Iteration 52, loss = 0.10667125\n",
      "Iteration 53, loss = 0.10264139\n",
      "Iteration 54, loss = 0.10352136\n",
      "Iteration 55, loss = 0.10069530\n",
      "Iteration 56, loss = 0.10084063\n",
      "Iteration 57, loss = 0.10165797\n",
      "Iteration 58, loss = 0.09827825\n",
      "Iteration 59, loss = 0.09652099\n",
      "Iteration 60, loss = 0.09556447\n",
      "Iteration 61, loss = 0.09499724\n",
      "Iteration 62, loss = 0.09368274\n",
      "Iteration 63, loss = 0.09250570\n",
      "Iteration 64, loss = 0.09187081\n",
      "Iteration 65, loss = 0.09139074\n",
      "Iteration 66, loss = 0.09086767\n",
      "Iteration 67, loss = 0.09054985\n",
      "Iteration 68, loss = 0.09012628\n",
      "Iteration 69, loss = 0.08972263\n",
      "Iteration 70, loss = 0.08935749\n",
      "Iteration 71, loss = 0.08904172\n",
      "Iteration 72, loss = 0.08885082\n",
      "Iteration 73, loss = 0.08848442\n",
      "Iteration 74, loss = 0.08826681\n",
      "Iteration 75, loss = 0.08788102\n",
      "Iteration 76, loss = 0.08735748\n",
      "Iteration 77, loss = 0.08660894\n",
      "Iteration 78, loss = 0.08626518\n",
      "Iteration 79, loss = 0.08590997\n",
      "Iteration 80, loss = 0.08532366\n",
      "Iteration 81, loss = 0.08457981\n",
      "Iteration 82, loss = 0.08427447\n",
      "Iteration 83, loss = 0.09253351\n",
      "Iteration 84, loss = 0.08641717\n",
      "Iteration 85, loss = 0.08231269\n",
      "Iteration 86, loss = 0.08258313\n",
      "Iteration 87, loss = 0.08150428\n",
      "Iteration 88, loss = 0.08062460\n",
      "Iteration 89, loss = 0.08041776\n",
      "Iteration 90, loss = 0.08188692\n",
      "Iteration 91, loss = 0.09313723\n",
      "Iteration 92, loss = 0.07937629\n",
      "Iteration 93, loss = 0.07891236\n",
      "Iteration 94, loss = 0.07867728\n",
      "Iteration 95, loss = 0.07848607\n",
      "Iteration 96, loss = 0.07823834\n",
      "Iteration 97, loss = 0.07807699\n",
      "Iteration 98, loss = 0.07769264\n",
      "Iteration 99, loss = 0.07767231\n",
      "Iteration 100, loss = 0.07747434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.43187406\n",
      "Iteration 2, loss = 0.75319177\n",
      "Iteration 3, loss = 0.58294095\n",
      "Iteration 4, loss = 0.51510183\n",
      "Iteration 5, loss = 0.53137763\n",
      "Iteration 6, loss = 0.53009187\n",
      "Iteration 7, loss = 0.37460175\n",
      "Iteration 8, loss = 0.41293724\n",
      "Iteration 9, loss = 0.40883403\n",
      "Iteration 10, loss = 0.42271634\n",
      "Iteration 11, loss = 0.33517613\n",
      "Iteration 12, loss = 0.31951207\n",
      "Iteration 13, loss = 0.27363223\n",
      "Iteration 14, loss = 0.37196997\n",
      "Iteration 15, loss = 0.34048524\n",
      "Iteration 16, loss = 0.30029686\n",
      "Iteration 17, loss = 0.27955124\n",
      "Iteration 18, loss = 0.25607691\n",
      "Iteration 19, loss = 0.23431150\n",
      "Iteration 20, loss = 0.16239474\n",
      "Iteration 21, loss = 0.29796025\n",
      "Iteration 22, loss = 0.25161636\n",
      "Iteration 23, loss = 0.25542098\n",
      "Iteration 24, loss = 0.16733884\n",
      "Iteration 25, loss = 0.22245598\n",
      "Iteration 26, loss = 0.24706492\n",
      "Iteration 27, loss = 0.20380371\n",
      "Iteration 28, loss = 0.24696314\n",
      "Iteration 29, loss = 0.25384889\n",
      "Iteration 30, loss = 0.19922778\n",
      "Iteration 31, loss = 0.26787931\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 32, loss = 0.20543990\n",
      "Iteration 33, loss = 0.15668780\n",
      "Iteration 34, loss = 0.15001219\n",
      "Iteration 35, loss = 0.12488085\n",
      "Iteration 36, loss = 0.10577701\n",
      "Iteration 37, loss = 0.08935399\n",
      "Iteration 38, loss = 0.08089097\n",
      "Iteration 39, loss = 0.07482332\n",
      "Iteration 40, loss = 0.07556820\n",
      "Iteration 41, loss = 0.07021678\n",
      "Iteration 42, loss = 0.06768915\n",
      "Iteration 43, loss = 0.06825538\n",
      "Iteration 44, loss = 0.06530504\n",
      "Iteration 45, loss = 0.06348043\n",
      "Iteration 46, loss = 0.06231371\n",
      "Iteration 47, loss = 0.06171562\n",
      "Iteration 48, loss = 0.06044725\n",
      "Iteration 49, loss = 0.05979230\n",
      "Iteration 50, loss = 0.05877520\n",
      "Iteration 51, loss = 0.05800112\n",
      "Iteration 52, loss = 0.05661363\n",
      "Iteration 53, loss = 0.05592198\n",
      "Iteration 54, loss = 0.05740413\n",
      "Iteration 55, loss = 0.05459264\n",
      "Iteration 56, loss = 0.05242350\n",
      "Iteration 57, loss = 0.05193749\n",
      "Iteration 58, loss = 0.04943776\n",
      "Iteration 59, loss = 0.05010353\n",
      "Iteration 60, loss = 0.04877943\n",
      "Iteration 61, loss = 0.04813545\n",
      "Iteration 62, loss = 0.04771265\n",
      "Iteration 63, loss = 0.04739362\n",
      "Iteration 64, loss = 0.04702051\n",
      "Iteration 65, loss = 0.04673690\n",
      "Iteration 66, loss = 0.04638572\n",
      "Iteration 67, loss = 0.04610955\n",
      "Iteration 68, loss = 0.04578107\n",
      "Iteration 69, loss = 0.04553348\n",
      "Iteration 70, loss = 0.04525988\n",
      "Iteration 71, loss = 0.04498084\n",
      "Iteration 72, loss = 0.04473969\n",
      "Iteration 73, loss = 0.04448225\n",
      "Iteration 74, loss = 0.04424356\n",
      "Iteration 75, loss = 0.04394227\n",
      "Iteration 76, loss = 0.04373857\n",
      "Iteration 77, loss = 0.04350918\n",
      "Iteration 78, loss = 0.04331524\n",
      "Iteration 79, loss = 0.04307973\n",
      "Iteration 80, loss = 0.04282335\n",
      "Iteration 81, loss = 0.04249025\n",
      "Iteration 82, loss = 0.04226208\n",
      "Iteration 83, loss = 0.04206816\n",
      "Iteration 84, loss = 0.04183709\n",
      "Iteration 85, loss = 0.04158781\n",
      "Iteration 86, loss = 0.04161132\n",
      "Iteration 87, loss = 0.04048250\n",
      "Iteration 88, loss = 0.03983084\n",
      "Iteration 89, loss = 0.03915641\n",
      "Iteration 90, loss = 0.03879360\n",
      "Iteration 91, loss = 0.03847065\n",
      "Iteration 92, loss = 0.03818459\n",
      "Iteration 93, loss = 0.03790974\n",
      "Iteration 94, loss = 0.03767748\n",
      "Iteration 95, loss = 0.03744707\n",
      "Iteration 96, loss = 0.03724297\n",
      "Iteration 97, loss = 0.03704860\n",
      "Iteration 98, loss = 0.03679067\n",
      "Iteration 99, loss = 0.03666864\n",
      "Iteration 100, loss = 0.03622305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.53097159\n",
      "Iteration 2, loss = 0.90811616\n",
      "Iteration 3, loss = 0.63211051\n",
      "Iteration 4, loss = 0.71663505\n",
      "Iteration 5, loss = 0.47535494\n",
      "Iteration 6, loss = 0.44653381\n",
      "Iteration 7, loss = 0.50964935\n",
      "Iteration 8, loss = 0.46967648\n",
      "Iteration 9, loss = 0.39949030\n",
      "Iteration 10, loss = 0.39941610\n",
      "Iteration 11, loss = 0.44381804\n",
      "Iteration 12, loss = 0.31965560\n",
      "Iteration 13, loss = 0.34441612\n",
      "Iteration 14, loss = 0.40656824\n",
      "Iteration 15, loss = 0.30100504\n",
      "Iteration 16, loss = 0.33536512\n",
      "Iteration 17, loss = 0.42414750\n",
      "Iteration 18, loss = 0.38056683\n",
      "Iteration 19, loss = 0.45058584\n",
      "Iteration 20, loss = 0.35762919\n",
      "Iteration 21, loss = 0.32776830\n",
      "Iteration 22, loss = 0.33208445\n",
      "Iteration 23, loss = 0.28809871\n",
      "Iteration 24, loss = 0.28968358\n",
      "Iteration 25, loss = 0.21912308\n",
      "Iteration 26, loss = 0.25284666\n",
      "Iteration 27, loss = 0.22447360\n",
      "Iteration 28, loss = 0.28146733\n",
      "Iteration 29, loss = 0.28105314\n",
      "Iteration 30, loss = 0.23013416\n",
      "Iteration 31, loss = 0.27161059\n",
      "Iteration 32, loss = 0.27487498\n",
      "Iteration 33, loss = 0.26865069\n",
      "Iteration 34, loss = 0.20918695\n",
      "Iteration 35, loss = 0.16280356\n",
      "Iteration 36, loss = 0.19572359\n",
      "Iteration 37, loss = 0.23234265\n",
      "Iteration 38, loss = 0.25300893\n",
      "Iteration 39, loss = 0.26925844\n",
      "Iteration 40, loss = 0.31077402\n",
      "Iteration 41, loss = 0.24883599\n",
      "Iteration 42, loss = 0.25511111\n",
      "Iteration 43, loss = 0.21336813\n",
      "Iteration 44, loss = 0.21886178\n",
      "Iteration 45, loss = 0.19059577\n",
      "Iteration 46, loss = 0.18332892\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 47, loss = 0.13690624\n",
      "Iteration 48, loss = 0.12355757\n",
      "Iteration 49, loss = 0.11571299\n",
      "Iteration 50, loss = 0.11192916\n",
      "Iteration 51, loss = 0.10725635\n",
      "Iteration 52, loss = 0.10483095\n",
      "Iteration 53, loss = 0.10270038\n",
      "Iteration 54, loss = 0.10174684\n",
      "Iteration 55, loss = 0.09561307\n",
      "Iteration 56, loss = 0.09577659\n",
      "Iteration 57, loss = 0.09230439\n",
      "Iteration 58, loss = 0.09230093\n",
      "Iteration 59, loss = 0.09133341\n",
      "Iteration 60, loss = 0.09003439\n",
      "Iteration 61, loss = 0.08987705\n",
      "Iteration 62, loss = 0.08837406\n",
      "Iteration 63, loss = 0.08846322\n",
      "Iteration 64, loss = 0.08753717\n",
      "Iteration 65, loss = 0.08711967\n",
      "Iteration 66, loss = 0.08679945\n",
      "Iteration 67, loss = 0.08491962\n",
      "Iteration 68, loss = 0.08320538\n",
      "Iteration 69, loss = 0.08254077\n",
      "Iteration 70, loss = 0.08061971\n",
      "Iteration 71, loss = 0.08090757\n",
      "Iteration 72, loss = 0.07883198\n",
      "Iteration 73, loss = 0.07687271\n",
      "Iteration 74, loss = 0.08273035\n",
      "Iteration 75, loss = 0.07847748\n",
      "Iteration 76, loss = 0.07481581\n",
      "Iteration 77, loss = 0.07370774\n",
      "Iteration 78, loss = 0.07463017\n",
      "Iteration 79, loss = 0.07331625\n",
      "Iteration 80, loss = 0.07155034\n",
      "Iteration 81, loss = 0.07100310\n",
      "Iteration 82, loss = 0.07062900\n",
      "Iteration 83, loss = 0.06949278\n",
      "Iteration 84, loss = 0.06943762\n",
      "Iteration 85, loss = 0.06883386\n",
      "Iteration 86, loss = 0.06819376\n",
      "Iteration 87, loss = 0.06771666\n",
      "Iteration 88, loss = 0.06773897\n",
      "Iteration 89, loss = 0.06812874\n",
      "Iteration 90, loss = 0.06659955\n",
      "Iteration 91, loss = 0.06574792\n",
      "Iteration 92, loss = 0.06583159\n",
      "Iteration 93, loss = 0.06504183\n",
      "Iteration 94, loss = 0.06489718\n",
      "Iteration 95, loss = 0.06412015\n",
      "Iteration 96, loss = 0.06365176\n",
      "Iteration 97, loss = 0.06365633\n",
      "Iteration 98, loss = 0.06336710\n",
      "Iteration 99, loss = 0.06288331\n",
      "Iteration 100, loss = 0.06234525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.63415649\n",
      "Iteration 2, loss = 0.84719144\n",
      "Iteration 3, loss = 0.75691092\n",
      "Iteration 4, loss = 0.65403824\n",
      "Iteration 5, loss = 0.54734963\n",
      "Iteration 6, loss = 0.57531999\n",
      "Iteration 7, loss = 0.35539481\n",
      "Iteration 8, loss = 0.35250158\n",
      "Iteration 9, loss = 0.40042245\n",
      "Iteration 10, loss = 0.32860366\n",
      "Iteration 11, loss = 0.28592496\n",
      "Iteration 12, loss = 0.34460724\n",
      "Iteration 13, loss = 0.41143563\n",
      "Iteration 14, loss = 0.57128493\n",
      "Iteration 15, loss = 0.53130357\n",
      "Iteration 16, loss = 0.39809264\n",
      "Iteration 17, loss = 0.38730593\n",
      "Iteration 18, loss = 0.35591111\n",
      "Iteration 19, loss = 0.34449348\n",
      "Iteration 20, loss = 0.29110911\n",
      "Iteration 21, loss = 0.39243170\n",
      "Iteration 22, loss = 0.34980848\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 23, loss = 0.24965817\n",
      "Iteration 24, loss = 0.21651076\n",
      "Iteration 25, loss = 0.20994613\n",
      "Iteration 26, loss = 0.17510373\n",
      "Iteration 27, loss = 0.16073579\n",
      "Iteration 28, loss = 0.15817830\n",
      "Iteration 29, loss = 0.15279008\n",
      "Iteration 30, loss = 0.14812252\n",
      "Iteration 31, loss = 0.13718038\n",
      "Iteration 32, loss = 0.13167646\n",
      "Iteration 33, loss = 0.12775763\n",
      "Iteration 34, loss = 0.12472413\n",
      "Iteration 35, loss = 0.11886410\n",
      "Iteration 36, loss = 0.11515467\n",
      "Iteration 37, loss = 0.11340639\n",
      "Iteration 38, loss = 0.11191288\n",
      "Iteration 39, loss = 0.11312084\n",
      "Iteration 40, loss = 0.11029710\n",
      "Iteration 41, loss = 0.10958491\n",
      "Iteration 42, loss = 0.10891291\n",
      "Iteration 43, loss = 0.10948466\n",
      "Iteration 44, loss = 0.11025295\n",
      "Iteration 45, loss = 0.10574187\n",
      "Iteration 46, loss = 0.10732451\n",
      "Iteration 47, loss = 0.10444600\n",
      "Iteration 48, loss = 0.10063898\n",
      "Iteration 49, loss = 0.10111151\n",
      "Iteration 50, loss = 0.10056627\n",
      "Iteration 51, loss = 0.09757166\n",
      "Iteration 52, loss = 0.10070861\n",
      "Iteration 53, loss = 0.11482070\n",
      "Iteration 54, loss = 0.10557759\n",
      "Iteration 55, loss = 0.10234218\n",
      "Iteration 56, loss = 0.09454774\n",
      "Iteration 57, loss = 0.09379664\n",
      "Iteration 58, loss = 0.09162404\n",
      "Iteration 59, loss = 0.09146077\n",
      "Iteration 60, loss = 0.08930049\n",
      "Iteration 61, loss = 0.08923248\n",
      "Iteration 62, loss = 0.08863065\n",
      "Iteration 63, loss = 0.08801821\n",
      "Iteration 64, loss = 0.08718597\n",
      "Iteration 65, loss = 0.08788754\n",
      "Iteration 66, loss = 0.08591140\n",
      "Iteration 67, loss = 0.08625274\n",
      "Iteration 68, loss = 0.08610315\n",
      "Iteration 69, loss = 0.08474994\n",
      "Iteration 70, loss = 0.08494957\n",
      "Iteration 71, loss = 0.08377717\n",
      "Iteration 72, loss = 0.08464604\n",
      "Iteration 73, loss = 0.08365170\n",
      "Iteration 74, loss = 0.08266083\n",
      "Iteration 75, loss = 0.08285570\n",
      "Iteration 76, loss = 0.08165962\n",
      "Iteration 77, loss = 0.08146990\n",
      "Iteration 78, loss = 0.08094090\n",
      "Iteration 79, loss = 0.08098846\n",
      "Iteration 80, loss = 0.08040701\n",
      "Iteration 81, loss = 0.07972865\n",
      "Iteration 82, loss = 0.07937822\n",
      "Iteration 83, loss = 0.07898411\n",
      "Iteration 84, loss = 0.07880413\n",
      "Iteration 85, loss = 0.07847455\n",
      "Iteration 86, loss = 0.07823188\n",
      "Iteration 87, loss = 0.07778143\n",
      "Iteration 88, loss = 0.07753179\n",
      "Iteration 89, loss = 0.07742514\n",
      "Iteration 90, loss = 0.07768216\n",
      "Iteration 91, loss = 0.07702540\n",
      "Iteration 92, loss = 0.07717637\n",
      "Iteration 93, loss = 0.07635958\n",
      "Iteration 94, loss = 0.07590253\n",
      "Iteration 95, loss = 0.07561300\n",
      "Iteration 96, loss = 0.07556680\n",
      "Iteration 97, loss = 0.07517304\n",
      "Iteration 98, loss = 0.07491825\n",
      "Iteration 99, loss = 0.07457567\n",
      "Iteration 100, loss = 0.07454973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.28924057\n",
      "Iteration 2, loss = 0.69735199\n",
      "Iteration 3, loss = 0.52392543\n",
      "Iteration 4, loss = 0.51875054\n",
      "Iteration 5, loss = 0.54374995\n",
      "Iteration 6, loss = 0.46248508\n",
      "Iteration 7, loss = 0.43374321\n",
      "Iteration 8, loss = 0.38227525\n",
      "Iteration 9, loss = 0.32224998\n",
      "Iteration 10, loss = 0.30884384\n",
      "Iteration 11, loss = 0.36740558\n",
      "Iteration 12, loss = 0.45593626\n",
      "Iteration 13, loss = 0.56771051\n",
      "Iteration 14, loss = 0.43475279\n",
      "Iteration 15, loss = 0.35389492\n",
      "Iteration 16, loss = 0.30818771\n",
      "Iteration 17, loss = 0.31066671\n",
      "Iteration 18, loss = 0.31652328\n",
      "Iteration 19, loss = 0.31245218\n",
      "Iteration 20, loss = 0.41840148\n",
      "Iteration 21, loss = 0.31801021\n",
      "Iteration 22, loss = 0.44716770\n",
      "Iteration 23, loss = 0.29051345\n",
      "Iteration 24, loss = 0.21747431\n",
      "Iteration 25, loss = 0.24526125\n",
      "Iteration 26, loss = 0.30291001\n",
      "Iteration 27, loss = 0.25158726\n",
      "Iteration 28, loss = 0.21266695\n",
      "Iteration 29, loss = 0.26859874\n",
      "Iteration 30, loss = 0.32534769\n",
      "Iteration 31, loss = 0.25465830\n",
      "Iteration 32, loss = 0.28770762\n",
      "Iteration 33, loss = 0.48750854\n",
      "Iteration 34, loss = 0.44437240\n",
      "Iteration 35, loss = 0.43768932\n",
      "Iteration 36, loss = 0.41031809\n",
      "Iteration 37, loss = 0.43923613\n",
      "Iteration 38, loss = 0.29084284\n",
      "Iteration 39, loss = 0.28369273\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 40, loss = 0.22971351\n",
      "Iteration 41, loss = 0.19258183\n",
      "Iteration 42, loss = 0.18771775\n",
      "Iteration 43, loss = 0.18157259\n",
      "Iteration 44, loss = 0.17905823\n",
      "Iteration 45, loss = 0.17764541\n",
      "Iteration 46, loss = 0.18181780\n",
      "Iteration 47, loss = 0.16628718\n",
      "Iteration 48, loss = 0.15949888\n",
      "Iteration 49, loss = 0.15767195\n",
      "Iteration 50, loss = 0.15361217\n",
      "Iteration 51, loss = 0.15610672\n",
      "Iteration 52, loss = 0.16022382\n",
      "Iteration 53, loss = 0.14515909\n",
      "Iteration 54, loss = 0.14456744\n",
      "Iteration 55, loss = 0.13410733\n",
      "Iteration 56, loss = 0.13702408\n",
      "Iteration 57, loss = 0.12880542\n",
      "Iteration 58, loss = 0.13454048\n",
      "Iteration 59, loss = 0.13241049\n",
      "Iteration 60, loss = 0.13377704\n",
      "Iteration 61, loss = 0.12951074\n",
      "Iteration 62, loss = 0.12882501\n",
      "Iteration 63, loss = 0.12704122\n",
      "Iteration 64, loss = 0.12903309\n",
      "Iteration 65, loss = 0.12601462\n",
      "Iteration 66, loss = 0.13403593\n",
      "Iteration 67, loss = 0.13459119\n",
      "Iteration 68, loss = 0.12905307\n",
      "Iteration 69, loss = 0.11875323\n",
      "Iteration 70, loss = 0.12054603\n",
      "Iteration 71, loss = 0.11860846\n",
      "Iteration 72, loss = 0.12253413\n",
      "Iteration 73, loss = 0.11930330\n",
      "Iteration 74, loss = 0.11527298\n",
      "Iteration 75, loss = 0.11444788\n",
      "Iteration 76, loss = 0.11274523\n",
      "Iteration 77, loss = 0.11273728\n",
      "Iteration 78, loss = 0.11181608\n",
      "Iteration 79, loss = 0.11109954\n",
      "Iteration 80, loss = 0.11066353\n",
      "Iteration 81, loss = 0.10997510\n",
      "Iteration 82, loss = 0.11023233\n",
      "Iteration 83, loss = 0.10943617\n",
      "Iteration 84, loss = 0.10906446\n",
      "Iteration 85, loss = 0.10812274\n",
      "Iteration 86, loss = 0.10967868\n",
      "Iteration 87, loss = 0.10861400\n",
      "Iteration 88, loss = 0.10710383\n",
      "Iteration 89, loss = 0.10698628\n",
      "Iteration 90, loss = 0.10960828\n",
      "Iteration 91, loss = 0.10736068\n",
      "Iteration 92, loss = 0.10661730\n",
      "Iteration 93, loss = 0.10525458\n",
      "Iteration 94, loss = 0.10499228\n",
      "Iteration 95, loss = 0.10451673\n",
      "Iteration 96, loss = 0.10540650\n",
      "Iteration 97, loss = 0.10753403\n",
      "Iteration 98, loss = 0.10430951\n",
      "Iteration 99, loss = 0.10351077\n",
      "Iteration 100, loss = 0.10614936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.87781929\n",
      "Iteration 2, loss = 1.18640640\n",
      "Iteration 3, loss = 0.93589978\n",
      "Iteration 4, loss = 0.69229949\n",
      "Iteration 5, loss = 0.55656186\n",
      "Iteration 6, loss = 0.67983734\n",
      "Iteration 7, loss = 0.62022862\n",
      "Iteration 8, loss = 0.60476312\n",
      "Iteration 9, loss = 0.47526569\n",
      "Iteration 10, loss = 0.43650788\n",
      "Iteration 11, loss = 0.41927157\n",
      "Iteration 12, loss = 0.45810574\n",
      "Iteration 13, loss = 0.50374858\n",
      "Iteration 14, loss = 0.37965134\n",
      "Iteration 15, loss = 0.41470965\n",
      "Iteration 16, loss = 0.39214809\n",
      "Iteration 17, loss = 0.33804367\n",
      "Iteration 18, loss = 0.33743749\n",
      "Iteration 19, loss = 0.38159553\n",
      "Iteration 20, loss = 0.36319398\n",
      "Iteration 21, loss = 0.36373818\n",
      "Iteration 22, loss = 0.29921309\n",
      "Iteration 23, loss = 0.44813345\n",
      "Iteration 24, loss = 0.39204936\n",
      "Iteration 25, loss = 0.26041221\n",
      "Iteration 26, loss = 0.29620910\n",
      "Iteration 27, loss = 0.44661937\n",
      "Iteration 28, loss = 0.46858191\n",
      "Iteration 29, loss = 0.52704133\n",
      "Iteration 30, loss = 0.39452334\n",
      "Iteration 31, loss = 0.36058720\n",
      "Iteration 32, loss = 0.48012634\n",
      "Iteration 33, loss = 0.43112611\n",
      "Iteration 34, loss = 0.46649472\n",
      "Iteration 35, loss = 0.44681938\n",
      "Iteration 36, loss = 0.34843105\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 37, loss = 0.34967052\n",
      "Iteration 38, loss = 0.26398194\n",
      "Iteration 39, loss = 0.23516508\n",
      "Iteration 40, loss = 0.20923766\n",
      "Iteration 41, loss = 0.19970658\n",
      "Iteration 42, loss = 0.19290325\n",
      "Iteration 43, loss = 0.18126417\n",
      "Iteration 44, loss = 0.17633028\n",
      "Iteration 45, loss = 0.17913451\n",
      "Iteration 46, loss = 0.17415600\n",
      "Iteration 47, loss = 0.16686749\n",
      "Iteration 48, loss = 0.17049660\n",
      "Iteration 49, loss = 0.16473768\n",
      "Iteration 50, loss = 0.16443568\n",
      "Iteration 51, loss = 0.16029064\n",
      "Iteration 52, loss = 0.15710791\n",
      "Iteration 53, loss = 0.15700041\n",
      "Iteration 54, loss = 0.15331974\n",
      "Iteration 55, loss = 0.15487217\n",
      "Iteration 56, loss = 0.14902940\n",
      "Iteration 57, loss = 0.14472793\n",
      "Iteration 58, loss = 0.14156915\n",
      "Iteration 59, loss = 0.13518137\n",
      "Iteration 60, loss = 0.13356053\n",
      "Iteration 61, loss = 0.13029434\n",
      "Iteration 62, loss = 0.13527001\n",
      "Iteration 63, loss = 0.13050135\n",
      "Iteration 64, loss = 0.12870618\n",
      "Iteration 65, loss = 0.12724045\n",
      "Iteration 66, loss = 0.12509000\n",
      "Iteration 67, loss = 0.12289819\n",
      "Iteration 68, loss = 0.12203210\n",
      "Iteration 69, loss = 0.12108005\n",
      "Iteration 70, loss = 0.12028087\n",
      "Iteration 71, loss = 0.11866399\n",
      "Iteration 72, loss = 0.11766025\n",
      "Iteration 73, loss = 0.12527951\n",
      "Iteration 74, loss = 0.11815219\n",
      "Iteration 75, loss = 0.11530048\n",
      "Iteration 76, loss = 0.11407582\n",
      "Iteration 77, loss = 0.11516759\n",
      "Iteration 78, loss = 0.11418936\n",
      "Iteration 79, loss = 0.11276799\n",
      "Iteration 80, loss = 0.11237752\n",
      "Iteration 81, loss = 0.11279529\n",
      "Iteration 82, loss = 0.11373287\n",
      "Iteration 83, loss = 0.11248745\n",
      "Iteration 84, loss = 0.11007083\n",
      "Iteration 85, loss = 0.10953805\n",
      "Iteration 86, loss = 0.10795810\n",
      "Iteration 87, loss = 0.10827031\n",
      "Iteration 88, loss = 0.10938673\n",
      "Iteration 89, loss = 0.10560068\n",
      "Iteration 90, loss = 0.10686399\n",
      "Iteration 91, loss = 0.10530864\n",
      "Iteration 92, loss = 0.10422644\n",
      "Iteration 93, loss = 0.10488665\n",
      "Iteration 94, loss = 0.10158384\n",
      "Iteration 95, loss = 0.10111737\n",
      "Iteration 96, loss = 0.10029133\n",
      "Iteration 97, loss = 0.09946119\n",
      "Iteration 98, loss = 0.09946847\n",
      "Iteration 99, loss = 0.10437120\n",
      "Iteration 100, loss = 0.09710190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.34026222\n",
      "Iteration 2, loss = 0.72347899\n",
      "Iteration 3, loss = 0.68161615\n",
      "Iteration 4, loss = 0.66087388\n",
      "Iteration 5, loss = 0.70456862\n",
      "Iteration 6, loss = 0.73338956\n",
      "Iteration 7, loss = 0.61872851\n",
      "Iteration 8, loss = 0.55547575\n",
      "Iteration 9, loss = 0.55765984\n",
      "Iteration 10, loss = 0.57014524\n",
      "Iteration 11, loss = 0.51481886\n",
      "Iteration 12, loss = 0.51265062\n",
      "Iteration 13, loss = 0.54770253\n",
      "Iteration 14, loss = 0.53417178\n",
      "Iteration 15, loss = 0.59245658\n",
      "Iteration 16, loss = 0.46732407\n",
      "Iteration 17, loss = 0.42860149\n",
      "Iteration 18, loss = 0.33928931\n",
      "Iteration 19, loss = 0.37186652\n",
      "Iteration 20, loss = 0.34516296\n",
      "Iteration 21, loss = 0.40256681\n",
      "Iteration 22, loss = 0.33267029\n",
      "Iteration 23, loss = 0.27696944\n",
      "Iteration 24, loss = 0.30592420\n",
      "Iteration 25, loss = 0.25074022\n",
      "Iteration 26, loss = 0.29913860\n",
      "Iteration 27, loss = 0.26701639\n",
      "Iteration 28, loss = 0.28975687\n",
      "Iteration 29, loss = 0.26783965\n",
      "Iteration 30, loss = 0.27042215\n",
      "Iteration 31, loss = 0.29642443\n",
      "Iteration 32, loss = 0.26819155\n",
      "Iteration 33, loss = 0.27461263\n",
      "Iteration 34, loss = 0.26728749\n",
      "Iteration 35, loss = 0.28452697\n",
      "Iteration 36, loss = 0.31006219\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 37, loss = 0.22099399\n",
      "Iteration 38, loss = 0.17818122\n",
      "Iteration 39, loss = 0.14916422\n",
      "Iteration 40, loss = 0.14335921\n",
      "Iteration 41, loss = 0.13130714\n",
      "Iteration 42, loss = 0.12193891\n",
      "Iteration 43, loss = 0.11651394\n",
      "Iteration 44, loss = 0.11349334\n",
      "Iteration 45, loss = 0.10791556\n",
      "Iteration 46, loss = 0.10818539\n",
      "Iteration 47, loss = 0.10681536\n",
      "Iteration 48, loss = 0.10764612\n",
      "Iteration 49, loss = 0.10045664\n",
      "Iteration 50, loss = 0.09977121\n",
      "Iteration 51, loss = 0.10055247\n",
      "Iteration 52, loss = 0.10005826\n",
      "Iteration 53, loss = 0.09303604\n",
      "Iteration 54, loss = 0.09193706\n",
      "Iteration 55, loss = 0.09119129\n",
      "Iteration 56, loss = 0.09119657\n",
      "Iteration 57, loss = 0.09223063\n",
      "Iteration 58, loss = 0.09747337\n",
      "Iteration 59, loss = 0.09160460\n",
      "Iteration 60, loss = 0.08977687\n",
      "Iteration 61, loss = 0.09205424\n",
      "Iteration 62, loss = 0.08506160\n",
      "Iteration 63, loss = 0.08482921\n",
      "Iteration 64, loss = 0.08360014\n",
      "Iteration 65, loss = 0.08248743\n",
      "Iteration 66, loss = 0.08221353\n",
      "Iteration 67, loss = 0.08165532\n",
      "Iteration 68, loss = 0.08059619\n",
      "Iteration 69, loss = 0.08121691\n",
      "Iteration 70, loss = 0.07974257\n",
      "Iteration 71, loss = 0.08618900\n",
      "Iteration 72, loss = 0.07941286\n",
      "Iteration 73, loss = 0.07753586\n",
      "Iteration 74, loss = 0.07708829\n",
      "Iteration 75, loss = 0.07656932\n",
      "Iteration 76, loss = 0.07622459\n",
      "Iteration 77, loss = 0.07575074\n",
      "Iteration 78, loss = 0.07565269\n",
      "Iteration 79, loss = 0.07507986\n",
      "Iteration 80, loss = 0.07467262\n",
      "Iteration 81, loss = 0.07447377\n",
      "Iteration 82, loss = 0.07406517\n",
      "Iteration 83, loss = 0.07368607\n",
      "Iteration 84, loss = 0.07343072\n",
      "Iteration 85, loss = 0.07306803\n",
      "Iteration 86, loss = 0.07273827\n",
      "Iteration 87, loss = 0.07247885\n",
      "Iteration 88, loss = 0.07218948\n",
      "Iteration 89, loss = 0.07181044\n",
      "Iteration 90, loss = 0.07156556\n",
      "Iteration 91, loss = 0.07132097\n",
      "Iteration 92, loss = 0.07097658\n",
      "Iteration 93, loss = 0.07057340\n",
      "Iteration 94, loss = 0.07038768\n",
      "Iteration 95, loss = 0.07016421\n",
      "Iteration 96, loss = 0.06992831\n",
      "Iteration 97, loss = 0.06968379\n",
      "Iteration 98, loss = 0.06948062\n",
      "Iteration 99, loss = 0.06935475\n",
      "Iteration 100, loss = 0.06910497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.59604794\n",
      "Iteration 2, loss = 0.83956623\n",
      "Iteration 3, loss = 0.64341958\n",
      "Iteration 4, loss = 0.54874171\n",
      "Iteration 5, loss = 0.65825527\n",
      "Iteration 6, loss = 0.57245631\n",
      "Iteration 7, loss = 0.44485642\n",
      "Iteration 8, loss = 0.44644487\n",
      "Iteration 9, loss = 0.47804415\n",
      "Iteration 10, loss = 0.58152834\n",
      "Iteration 11, loss = 0.38554260\n",
      "Iteration 12, loss = 0.34789969\n",
      "Iteration 13, loss = 0.39181112\n",
      "Iteration 14, loss = 0.41099057\n",
      "Iteration 15, loss = 0.42181222\n",
      "Iteration 16, loss = 0.33254399\n",
      "Iteration 17, loss = 0.37306021\n",
      "Iteration 18, loss = 0.30698730\n",
      "Iteration 19, loss = 0.27251340\n",
      "Iteration 20, loss = 0.25276877\n",
      "Iteration 21, loss = 0.32692514\n",
      "Iteration 22, loss = 0.24926850\n",
      "Iteration 23, loss = 0.22017563\n",
      "Iteration 24, loss = 0.25985896\n",
      "Iteration 25, loss = 0.37179185\n",
      "Iteration 26, loss = 0.42098683\n",
      "Iteration 27, loss = 0.30329216\n",
      "Iteration 28, loss = 0.26377162\n",
      "Iteration 29, loss = 0.38953649\n",
      "Iteration 30, loss = 0.35174145\n",
      "Iteration 31, loss = 0.37592414\n",
      "Iteration 32, loss = 0.35923299\n",
      "Iteration 33, loss = 0.34543453\n",
      "Iteration 34, loss = 0.26661962\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 35, loss = 0.24894576\n",
      "Iteration 36, loss = 0.20458844\n",
      "Iteration 37, loss = 0.18081105\n",
      "Iteration 38, loss = 0.17502705\n",
      "Iteration 39, loss = 0.18131972\n",
      "Iteration 40, loss = 0.13973186\n",
      "Iteration 41, loss = 0.14646526\n",
      "Iteration 42, loss = 0.13961470\n",
      "Iteration 43, loss = 0.13427393\n",
      "Iteration 44, loss = 0.13097004\n",
      "Iteration 45, loss = 0.13040542\n",
      "Iteration 46, loss = 0.12831117\n",
      "Iteration 47, loss = 0.12135660\n",
      "Iteration 48, loss = 0.11982823\n",
      "Iteration 49, loss = 0.11787921\n",
      "Iteration 50, loss = 0.11782932\n",
      "Iteration 51, loss = 0.11484710\n",
      "Iteration 52, loss = 0.11488324\n",
      "Iteration 53, loss = 0.11106003\n",
      "Iteration 54, loss = 0.10970342\n",
      "Iteration 55, loss = 0.11028171\n",
      "Iteration 56, loss = 0.10903237\n",
      "Iteration 57, loss = 0.10765878\n",
      "Iteration 58, loss = 0.10589556\n",
      "Iteration 59, loss = 0.10518343\n",
      "Iteration 60, loss = 0.10395929\n",
      "Iteration 61, loss = 0.10316788\n",
      "Iteration 62, loss = 0.10222334\n",
      "Iteration 63, loss = 0.10134201\n",
      "Iteration 64, loss = 0.10129096\n",
      "Iteration 65, loss = 0.10032901\n",
      "Iteration 66, loss = 0.10001409\n",
      "Iteration 67, loss = 0.09952963\n",
      "Iteration 68, loss = 0.09922829\n",
      "Iteration 69, loss = 0.09860133\n",
      "Iteration 70, loss = 0.09785429\n",
      "Iteration 71, loss = 0.09781451\n",
      "Iteration 72, loss = 0.09762205\n",
      "Iteration 73, loss = 0.09705203\n",
      "Iteration 74, loss = 0.09547460\n",
      "Iteration 75, loss = 0.09559183\n",
      "Iteration 76, loss = 0.09556440\n",
      "Iteration 77, loss = 0.09343898\n",
      "Iteration 78, loss = 0.09194799\n",
      "Iteration 79, loss = 0.09216922\n",
      "Iteration 80, loss = 0.09075704\n",
      "Iteration 81, loss = 0.09040242\n",
      "Iteration 82, loss = 0.08951345\n",
      "Iteration 83, loss = 0.08914378\n",
      "Iteration 84, loss = 0.09200269\n",
      "Iteration 85, loss = 0.09038820\n",
      "Iteration 86, loss = 0.08907548\n",
      "Iteration 87, loss = 0.09118844\n",
      "Iteration 88, loss = 0.08960462\n",
      "Iteration 89, loss = 0.08722903\n",
      "Iteration 90, loss = 0.08493342\n",
      "Iteration 91, loss = 0.08447296\n",
      "Iteration 92, loss = 0.08295750\n",
      "Iteration 93, loss = 0.08254469\n",
      "Iteration 94, loss = 0.08208912\n",
      "Iteration 95, loss = 0.08179846\n",
      "Iteration 96, loss = 0.08075965\n",
      "Iteration 97, loss = 0.08121129\n",
      "Iteration 98, loss = 0.07947075\n",
      "Iteration 99, loss = 0.07938848\n",
      "Iteration 100, loss = 0.07831892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.53667413\n",
      "Iteration 2, loss = 0.74611504\n",
      "Iteration 3, loss = 0.57467319\n",
      "Iteration 4, loss = 0.60684970\n",
      "Iteration 5, loss = 0.53008526\n",
      "Iteration 6, loss = 0.41816589\n",
      "Iteration 7, loss = 0.49489510\n",
      "Iteration 8, loss = 0.43764215\n",
      "Iteration 9, loss = 0.37081624\n",
      "Iteration 10, loss = 0.36071507\n",
      "Iteration 11, loss = 0.40401803\n",
      "Iteration 12, loss = 0.37248625\n",
      "Iteration 13, loss = 0.43940104\n",
      "Iteration 14, loss = 0.45553603\n",
      "Iteration 15, loss = 0.37044298\n",
      "Iteration 16, loss = 0.42870047\n",
      "Iteration 17, loss = 0.39842206\n",
      "Iteration 18, loss = 0.29866122\n",
      "Iteration 19, loss = 0.37653171\n",
      "Iteration 20, loss = 0.40173186\n",
      "Iteration 21, loss = 0.36927462\n",
      "Iteration 22, loss = 0.33247724\n",
      "Iteration 23, loss = 0.29764640\n",
      "Iteration 24, loss = 0.29424115\n",
      "Iteration 25, loss = 0.32084179\n",
      "Iteration 26, loss = 0.35986068\n",
      "Iteration 27, loss = 0.29220491\n",
      "Iteration 28, loss = 0.35302086\n",
      "Iteration 29, loss = 0.21647200\n",
      "Iteration 30, loss = 0.28690168\n",
      "Iteration 31, loss = 0.28396280\n",
      "Iteration 32, loss = 0.30212419\n",
      "Iteration 33, loss = 0.25827276\n",
      "Iteration 34, loss = 0.28644798\n",
      "Iteration 35, loss = 0.25085337\n",
      "Iteration 36, loss = 0.30645695\n",
      "Iteration 37, loss = 0.29774947\n",
      "Iteration 38, loss = 0.26947247\n",
      "Iteration 39, loss = 0.24368674\n",
      "Iteration 40, loss = 0.28460406\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 41, loss = 0.28120043\n",
      "Iteration 42, loss = 0.21690073\n",
      "Iteration 43, loss = 0.19729304\n",
      "Iteration 44, loss = 0.17758306\n",
      "Iteration 45, loss = 0.17335518\n",
      "Iteration 46, loss = 0.18133214\n",
      "Iteration 47, loss = 0.17494808\n",
      "Iteration 48, loss = 0.15729615\n",
      "Iteration 49, loss = 0.16193159\n",
      "Iteration 50, loss = 0.15545338\n",
      "Iteration 51, loss = 0.15343937\n",
      "Iteration 52, loss = 0.15389380\n",
      "Iteration 53, loss = 0.14616252\n",
      "Iteration 54, loss = 0.14304539\n",
      "Iteration 55, loss = 0.14033732\n",
      "Iteration 56, loss = 0.14431125\n",
      "Iteration 57, loss = 0.14512231\n",
      "Iteration 58, loss = 0.14249451\n",
      "Iteration 59, loss = 0.14237088\n",
      "Iteration 60, loss = 0.14251958\n",
      "Iteration 61, loss = 0.14296619\n",
      "Iteration 62, loss = 0.14077141\n",
      "Iteration 63, loss = 0.13873923\n",
      "Iteration 64, loss = 0.13721262\n",
      "Iteration 65, loss = 0.13813699\n",
      "Iteration 66, loss = 0.13431992\n",
      "Iteration 67, loss = 0.13489814\n",
      "Iteration 68, loss = 0.13171000\n",
      "Iteration 69, loss = 0.12484124\n",
      "Iteration 70, loss = 0.12882561\n",
      "Iteration 71, loss = 0.12365679\n",
      "Iteration 72, loss = 0.12308793\n",
      "Iteration 73, loss = 0.12253446\n",
      "Iteration 74, loss = 0.12152982\n",
      "Iteration 75, loss = 0.12185627\n",
      "Iteration 76, loss = 0.12146331\n",
      "Iteration 77, loss = 0.12071330\n",
      "Iteration 78, loss = 0.11944385\n",
      "Iteration 79, loss = 0.11979481\n",
      "Iteration 80, loss = 0.11906271\n",
      "Iteration 81, loss = 0.11914335\n",
      "Iteration 82, loss = 0.11938737\n",
      "Iteration 83, loss = 0.11958412\n",
      "Iteration 84, loss = 0.11707102\n",
      "Iteration 85, loss = 0.11708723\n",
      "Iteration 86, loss = 0.12829329\n",
      "Iteration 87, loss = 0.11761595\n",
      "Iteration 88, loss = 0.11613509\n",
      "Iteration 89, loss = 0.11728825\n",
      "Iteration 90, loss = 0.11531232\n",
      "Iteration 91, loss = 0.11401089\n",
      "Iteration 92, loss = 0.11212126\n",
      "Iteration 93, loss = 0.11084470\n",
      "Iteration 94, loss = 0.11199537\n",
      "Iteration 95, loss = 0.10847340\n",
      "Iteration 96, loss = 0.10490815\n",
      "Iteration 97, loss = 0.10140481\n",
      "Iteration 98, loss = 0.10036518\n",
      "Iteration 99, loss = 0.09941709\n",
      "Iteration 100, loss = 0.10056713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.63014444\n",
      "Iteration 2, loss = 0.97190496\n",
      "Iteration 3, loss = 0.69565736\n",
      "Iteration 4, loss = 0.57575195\n",
      "Iteration 5, loss = 0.63198906\n",
      "Iteration 6, loss = 0.52451290\n",
      "Iteration 7, loss = 0.49714536\n",
      "Iteration 8, loss = 0.44428198\n",
      "Iteration 9, loss = 0.48221688\n",
      "Iteration 10, loss = 0.40122242\n",
      "Iteration 11, loss = 0.39885390\n",
      "Iteration 12, loss = 0.39581922\n",
      "Iteration 13, loss = 0.33381985\n",
      "Iteration 14, loss = 0.32944119\n",
      "Iteration 15, loss = 0.25169218\n",
      "Iteration 16, loss = 0.30454413\n",
      "Iteration 17, loss = 0.33417279\n",
      "Iteration 18, loss = 0.35407679\n",
      "Iteration 19, loss = 0.25800230\n",
      "Iteration 20, loss = 0.27354420\n",
      "Iteration 21, loss = 0.21581765\n",
      "Iteration 22, loss = 0.20735601\n",
      "Iteration 23, loss = 0.27494918\n",
      "Iteration 24, loss = 0.40941409\n",
      "Iteration 25, loss = 0.35958664\n",
      "Iteration 26, loss = 0.45813811\n",
      "Iteration 27, loss = 0.29171540\n",
      "Iteration 28, loss = 0.32378144\n",
      "Iteration 29, loss = 0.36802285\n",
      "Iteration 30, loss = 0.23143839\n",
      "Iteration 31, loss = 0.25241375\n",
      "Iteration 32, loss = 0.30245724\n",
      "Iteration 33, loss = 0.21983935\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 34, loss = 0.19795536\n",
      "Iteration 35, loss = 0.17993815\n",
      "Iteration 36, loss = 0.15471124\n",
      "Iteration 37, loss = 0.15850380\n",
      "Iteration 38, loss = 0.14732125\n",
      "Iteration 39, loss = 0.14220817\n",
      "Iteration 40, loss = 0.13480159\n",
      "Iteration 41, loss = 0.13935904\n",
      "Iteration 42, loss = 0.13746051\n",
      "Iteration 43, loss = 0.13016163\n",
      "Iteration 44, loss = 0.12270849\n",
      "Iteration 45, loss = 0.11506160\n",
      "Iteration 46, loss = 0.10758852\n",
      "Iteration 47, loss = 0.10650654\n",
      "Iteration 48, loss = 0.11607274\n",
      "Iteration 49, loss = 0.11643325\n",
      "Iteration 50, loss = 0.11092843\n",
      "Iteration 51, loss = 0.09985188\n",
      "Iteration 52, loss = 0.09666702\n",
      "Iteration 53, loss = 0.10153973\n",
      "Iteration 54, loss = 0.09802018\n",
      "Iteration 55, loss = 0.09245882\n",
      "Iteration 56, loss = 0.09116314\n",
      "Iteration 57, loss = 0.08976645\n",
      "Iteration 58, loss = 0.08946188\n",
      "Iteration 59, loss = 0.08821379\n",
      "Iteration 60, loss = 0.08733512\n",
      "Iteration 61, loss = 0.08667082\n",
      "Iteration 62, loss = 0.08598336\n",
      "Iteration 63, loss = 0.08671555\n",
      "Iteration 64, loss = 0.08793561\n",
      "Iteration 65, loss = 0.08609845\n",
      "Iteration 66, loss = 0.08502613\n",
      "Iteration 67, loss = 0.08501523\n",
      "Iteration 68, loss = 0.08466403\n",
      "Iteration 69, loss = 0.08415751\n",
      "Iteration 70, loss = 0.08371597\n",
      "Iteration 71, loss = 0.08311196\n",
      "Iteration 72, loss = 0.08319367\n",
      "Iteration 73, loss = 0.08269461\n",
      "Iteration 74, loss = 0.08125253\n",
      "Iteration 75, loss = 0.08295947\n",
      "Iteration 76, loss = 0.08200159\n",
      "Iteration 77, loss = 0.08007065\n",
      "Iteration 78, loss = 0.07957503\n",
      "Iteration 79, loss = 0.07935382\n",
      "Iteration 80, loss = 0.07913254\n",
      "Iteration 81, loss = 0.07883486\n",
      "Iteration 82, loss = 0.07849994\n",
      "Iteration 83, loss = 0.07809038\n",
      "Iteration 84, loss = 0.07781930\n",
      "Iteration 85, loss = 0.07758898\n",
      "Iteration 86, loss = 0.07719093\n",
      "Iteration 87, loss = 0.07702307\n",
      "Iteration 88, loss = 0.07650244\n",
      "Iteration 89, loss = 0.07653443\n",
      "Iteration 90, loss = 0.07606336\n",
      "Iteration 91, loss = 0.07537194\n",
      "Iteration 92, loss = 0.07568670\n",
      "Iteration 93, loss = 0.07488550\n",
      "Iteration 94, loss = 0.07441551\n",
      "Iteration 95, loss = 0.07412035\n",
      "Iteration 96, loss = 0.07392359\n",
      "Iteration 97, loss = 0.07366186\n",
      "Iteration 98, loss = 0.07336916\n",
      "Iteration 99, loss = 0.07309873\n",
      "Iteration 100, loss = 0.07289126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.51376520\n",
      "Iteration 2, loss = 0.78484528\n",
      "Iteration 3, loss = 0.63120354\n",
      "Iteration 4, loss = 0.60107143\n",
      "Iteration 5, loss = 0.52384386\n",
      "Iteration 6, loss = 0.41745845\n",
      "Iteration 7, loss = 0.37619228\n",
      "Iteration 8, loss = 0.39136914\n",
      "Iteration 9, loss = 0.40906350\n",
      "Iteration 10, loss = 0.41626100\n",
      "Iteration 11, loss = 0.35148149\n",
      "Iteration 12, loss = 0.34850522\n",
      "Iteration 13, loss = 0.53947467\n",
      "Iteration 14, loss = 0.46723151\n",
      "Iteration 15, loss = 0.36080011\n",
      "Iteration 16, loss = 0.39510917\n",
      "Iteration 17, loss = 0.38318966\n",
      "Iteration 18, loss = 0.35425693\n",
      "Iteration 19, loss = 0.29172828\n",
      "Iteration 20, loss = 0.34117449\n",
      "Iteration 21, loss = 0.34187266\n",
      "Iteration 22, loss = 0.34533397\n",
      "Iteration 23, loss = 0.28818914\n",
      "Iteration 24, loss = 0.35209938\n",
      "Iteration 25, loss = 0.35812312\n",
      "Iteration 26, loss = 0.32089729\n",
      "Iteration 27, loss = 0.25445960\n",
      "Iteration 28, loss = 0.24413837\n",
      "Iteration 29, loss = 0.20707378\n",
      "Iteration 30, loss = 0.24240933\n",
      "Iteration 31, loss = 0.22506328\n",
      "Iteration 32, loss = 0.25883685\n",
      "Iteration 33, loss = 0.28732663\n",
      "Iteration 34, loss = 0.28729097\n",
      "Iteration 35, loss = 0.28274224\n",
      "Iteration 36, loss = 0.19189764\n",
      "Iteration 37, loss = 0.17925695\n",
      "Iteration 38, loss = 0.25450147\n",
      "Iteration 39, loss = 0.17759641\n",
      "Iteration 40, loss = 0.25855520\n",
      "Iteration 41, loss = 0.19309094\n",
      "Iteration 42, loss = 0.25341861\n",
      "Iteration 43, loss = 0.16454912\n",
      "Iteration 44, loss = 0.18119289\n",
      "Iteration 45, loss = 0.15306241\n",
      "Iteration 46, loss = 0.19293426\n",
      "Iteration 47, loss = 0.24287605\n",
      "Iteration 48, loss = 0.20735581\n",
      "Iteration 49, loss = 0.16802918\n",
      "Iteration 50, loss = 0.18990423\n",
      "Iteration 51, loss = 0.17550957\n",
      "Iteration 52, loss = 0.17232396\n",
      "Iteration 53, loss = 0.21581843\n",
      "Iteration 54, loss = 0.17290129\n",
      "Iteration 55, loss = 0.14416001\n",
      "Iteration 56, loss = 0.17676196\n",
      "Iteration 57, loss = 0.21726342\n",
      "Iteration 58, loss = 0.20237434\n",
      "Iteration 59, loss = 0.11512891\n",
      "Iteration 60, loss = 0.09728767\n",
      "Iteration 61, loss = 0.10173858\n",
      "Iteration 62, loss = 0.07743756\n",
      "Iteration 63, loss = 0.10547436\n",
      "Iteration 64, loss = 0.10469793\n",
      "Iteration 65, loss = 0.14673582\n",
      "Iteration 66, loss = 0.09048556\n",
      "Iteration 67, loss = 0.10134044\n",
      "Iteration 68, loss = 0.16734228\n",
      "Iteration 69, loss = 0.18675994\n",
      "Iteration 70, loss = 0.15236817\n",
      "Iteration 71, loss = 0.12053435\n",
      "Iteration 72, loss = 0.17401448\n",
      "Iteration 73, loss = 0.15574490\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 74, loss = 0.18648063\n",
      "Iteration 75, loss = 0.15000155\n",
      "Iteration 76, loss = 0.13228928\n",
      "Iteration 77, loss = 0.11500874\n",
      "Iteration 78, loss = 0.10813237\n",
      "Iteration 79, loss = 0.09775936\n",
      "Iteration 80, loss = 0.09203343\n",
      "Iteration 81, loss = 0.08416313\n",
      "Iteration 82, loss = 0.07629692\n",
      "Iteration 83, loss = 0.06999950\n",
      "Iteration 84, loss = 0.05860898\n",
      "Iteration 85, loss = 0.05407285\n",
      "Iteration 86, loss = 0.05241238\n",
      "Iteration 87, loss = 0.04988735\n",
      "Iteration 88, loss = 0.04872697\n",
      "Iteration 89, loss = 0.04827559\n",
      "Iteration 90, loss = 0.04756352\n",
      "Iteration 91, loss = 0.04699248\n",
      "Iteration 92, loss = 0.04651168\n",
      "Iteration 93, loss = 0.04624187\n",
      "Iteration 94, loss = 0.04592313\n",
      "Iteration 95, loss = 0.04529214\n",
      "Iteration 96, loss = 0.04480391\n",
      "Iteration 97, loss = 0.04389426\n",
      "Iteration 98, loss = 0.04321473\n",
      "Iteration 99, loss = 0.04266200\n",
      "Iteration 100, loss = 0.04203915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.56608128\n",
      "Iteration 2, loss = 0.86935317\n",
      "Iteration 3, loss = 0.72399638\n",
      "Iteration 4, loss = 0.59974941\n",
      "Iteration 5, loss = 0.61210217\n",
      "Iteration 6, loss = 0.80715264\n",
      "Iteration 7, loss = 0.54892192\n",
      "Iteration 8, loss = 0.50185323\n",
      "Iteration 9, loss = 0.47910538\n",
      "Iteration 10, loss = 0.48430289\n",
      "Iteration 11, loss = 0.64390670\n",
      "Iteration 12, loss = 0.53919985\n",
      "Iteration 13, loss = 0.58034871\n",
      "Iteration 14, loss = 0.40318654\n",
      "Iteration 15, loss = 0.39725215\n",
      "Iteration 16, loss = 0.32898934\n",
      "Iteration 17, loss = 0.33961620\n",
      "Iteration 18, loss = 0.31742064\n",
      "Iteration 19, loss = 0.32239182\n",
      "Iteration 20, loss = 0.35096349\n",
      "Iteration 21, loss = 0.28176642\n",
      "Iteration 22, loss = 0.29222938\n",
      "Iteration 23, loss = 0.31005580\n",
      "Iteration 24, loss = 0.28746882\n",
      "Iteration 25, loss = 0.29912913\n",
      "Iteration 26, loss = 0.28789218\n",
      "Iteration 27, loss = 0.25013497\n",
      "Iteration 28, loss = 0.27435245\n",
      "Iteration 29, loss = 0.34123352\n",
      "Iteration 30, loss = 0.24811392\n",
      "Iteration 31, loss = 0.25076650\n",
      "Iteration 32, loss = 0.26866275\n",
      "Iteration 33, loss = 0.40874716\n",
      "Iteration 34, loss = 0.30531751\n",
      "Iteration 35, loss = 0.27013857\n",
      "Iteration 36, loss = 0.24258247\n",
      "Iteration 37, loss = 0.23488972\n",
      "Iteration 38, loss = 0.18142734\n",
      "Iteration 39, loss = 0.18035828\n",
      "Iteration 40, loss = 0.16024041\n",
      "Iteration 41, loss = 0.18579794\n",
      "Iteration 42, loss = 0.15832161\n",
      "Iteration 43, loss = 0.19734939\n",
      "Iteration 44, loss = 0.18667462\n",
      "Iteration 45, loss = 0.15863304\n",
      "Iteration 46, loss = 0.14129592\n",
      "Iteration 47, loss = 0.13730834\n",
      "Iteration 48, loss = 0.14098430\n",
      "Iteration 49, loss = 0.18821586\n",
      "Iteration 50, loss = 0.15886596\n",
      "Iteration 51, loss = 0.11767730\n",
      "Iteration 52, loss = 0.20139600\n",
      "Iteration 53, loss = 0.22584887\n",
      "Iteration 54, loss = 0.17193199\n",
      "Iteration 55, loss = 0.17154243\n",
      "Iteration 56, loss = 0.22707522\n",
      "Iteration 57, loss = 0.23815806\n",
      "Iteration 58, loss = 0.19952095\n",
      "Iteration 59, loss = 0.17457122\n",
      "Iteration 60, loss = 0.15316286\n",
      "Iteration 61, loss = 0.18960941\n",
      "Iteration 62, loss = 0.18652936\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 63, loss = 0.12355435\n",
      "Iteration 64, loss = 0.09771917\n",
      "Iteration 65, loss = 0.09000146\n",
      "Iteration 66, loss = 0.08971735\n",
      "Iteration 67, loss = 0.09455018\n",
      "Iteration 68, loss = 0.08835182\n",
      "Iteration 69, loss = 0.08538546\n",
      "Iteration 70, loss = 0.08121899\n",
      "Iteration 71, loss = 0.08321239\n",
      "Iteration 72, loss = 0.08034318\n",
      "Iteration 73, loss = 0.07708913\n",
      "Iteration 74, loss = 0.07598666\n",
      "Iteration 75, loss = 0.07606025\n",
      "Iteration 76, loss = 0.07767572\n",
      "Iteration 77, loss = 0.07277356\n",
      "Iteration 78, loss = 0.07332897\n",
      "Iteration 79, loss = 0.07232492\n",
      "Iteration 80, loss = 0.07320160\n",
      "Iteration 81, loss = 0.07045106\n",
      "Iteration 82, loss = 0.06993144\n",
      "Iteration 83, loss = 0.06920031\n",
      "Iteration 84, loss = 0.06844353\n",
      "Iteration 85, loss = 0.06779718\n",
      "Iteration 86, loss = 0.06727037\n",
      "Iteration 87, loss = 0.06671373\n",
      "Iteration 88, loss = 0.06590144\n",
      "Iteration 89, loss = 0.06529263\n",
      "Iteration 90, loss = 0.06509271\n",
      "Iteration 91, loss = 0.06477203\n",
      "Iteration 92, loss = 0.06445671\n",
      "Iteration 93, loss = 0.06419056\n",
      "Iteration 94, loss = 0.06388484\n",
      "Iteration 95, loss = 0.06351725\n",
      "Iteration 96, loss = 0.06324480\n",
      "Iteration 97, loss = 0.06312432\n",
      "Iteration 98, loss = 0.06270163\n",
      "Iteration 99, loss = 0.06257248\n",
      "Iteration 100, loss = 0.06251564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.30112001\n",
      "Iteration 2, loss = 0.66169601\n",
      "Iteration 3, loss = 0.52942525\n",
      "Iteration 4, loss = 0.49426130\n",
      "Iteration 5, loss = 0.35631375\n",
      "Iteration 6, loss = 0.42342720\n",
      "Iteration 7, loss = 0.32184150\n",
      "Iteration 8, loss = 0.30636445\n",
      "Iteration 9, loss = 0.28468606\n",
      "Iteration 10, loss = 0.25947695\n",
      "Iteration 11, loss = 0.32377811\n",
      "Iteration 12, loss = 0.31275184\n",
      "Iteration 13, loss = 0.33386601\n",
      "Iteration 14, loss = 0.42424431\n",
      "Iteration 15, loss = 0.29493702\n",
      "Iteration 16, loss = 0.28300202\n",
      "Iteration 17, loss = 0.22745090\n",
      "Iteration 18, loss = 0.25324643\n",
      "Iteration 19, loss = 0.37868630\n",
      "Iteration 20, loss = 0.35556403\n",
      "Iteration 21, loss = 0.39597548\n",
      "Iteration 22, loss = 0.31649035\n",
      "Iteration 23, loss = 0.32047571\n",
      "Iteration 24, loss = 0.21392534\n",
      "Iteration 25, loss = 0.30348036\n",
      "Iteration 26, loss = 0.36120307\n",
      "Iteration 27, loss = 0.31371383\n",
      "Iteration 28, loss = 0.25205911\n",
      "Iteration 29, loss = 0.25640949\n",
      "Iteration 30, loss = 0.27321062\n",
      "Iteration 31, loss = 0.22761552\n",
      "Iteration 32, loss = 0.20760599\n",
      "Iteration 33, loss = 0.18021864\n",
      "Iteration 34, loss = 0.20559197\n",
      "Iteration 35, loss = 0.17835796\n",
      "Iteration 36, loss = 0.19172596\n",
      "Iteration 37, loss = 0.19125651\n",
      "Iteration 38, loss = 0.28384807\n",
      "Iteration 39, loss = 0.26766872\n",
      "Iteration 40, loss = 0.25406880\n",
      "Iteration 41, loss = 0.17998203\n",
      "Iteration 42, loss = 0.17352349\n",
      "Iteration 43, loss = 0.19082827\n",
      "Iteration 44, loss = 0.23866064\n",
      "Iteration 45, loss = 0.21113322\n",
      "Iteration 46, loss = 0.22795427\n",
      "Iteration 47, loss = 0.25943352\n",
      "Iteration 48, loss = 0.22786958\n",
      "Iteration 49, loss = 0.22840568\n",
      "Iteration 50, loss = 0.15592205\n",
      "Iteration 51, loss = 0.20185684\n",
      "Iteration 52, loss = 0.22774113\n",
      "Iteration 53, loss = 0.21431409\n",
      "Iteration 54, loss = 0.19460421\n",
      "Iteration 55, loss = 0.20758164\n",
      "Iteration 56, loss = 0.14192188\n",
      "Iteration 57, loss = 0.17774170\n",
      "Iteration 58, loss = 0.17914533\n",
      "Iteration 59, loss = 0.13626262\n",
      "Iteration 60, loss = 0.18899204\n",
      "Iteration 61, loss = 0.16261479\n",
      "Iteration 62, loss = 0.19431039\n",
      "Iteration 63, loss = 0.21036817\n",
      "Iteration 64, loss = 0.19549745\n",
      "Iteration 65, loss = 0.16479253\n",
      "Iteration 66, loss = 0.15212965\n",
      "Iteration 67, loss = 0.12759696\n",
      "Iteration 68, loss = 0.09931884\n",
      "Iteration 69, loss = 0.17750983\n",
      "Iteration 70, loss = 0.21032682\n",
      "Iteration 71, loss = 0.23182690\n",
      "Iteration 72, loss = 0.23154424\n",
      "Iteration 73, loss = 0.19139452\n",
      "Iteration 74, loss = 0.16590121\n",
      "Iteration 75, loss = 0.19563824\n",
      "Iteration 76, loss = 0.23859290\n",
      "Iteration 77, loss = 0.22863659\n",
      "Iteration 78, loss = 0.23958379\n",
      "Iteration 79, loss = 0.19640867\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 80, loss = 0.14583295\n",
      "Iteration 81, loss = 0.13545882\n",
      "Iteration 82, loss = 0.13170955\n",
      "Iteration 83, loss = 0.12525993\n",
      "Iteration 84, loss = 0.11106141\n",
      "Iteration 85, loss = 0.11384224\n",
      "Iteration 86, loss = 0.11086339\n",
      "Iteration 87, loss = 0.10791049\n",
      "Iteration 88, loss = 0.09904250\n",
      "Iteration 89, loss = 0.09678044\n",
      "Iteration 90, loss = 0.09465871\n",
      "Iteration 91, loss = 0.09396944\n",
      "Iteration 92, loss = 0.09276597\n",
      "Iteration 93, loss = 0.09406054\n",
      "Iteration 94, loss = 0.08994015\n",
      "Iteration 95, loss = 0.08957244\n",
      "Iteration 96, loss = 0.08871903\n",
      "Iteration 97, loss = 0.08622155\n",
      "Iteration 98, loss = 0.08610771\n",
      "Iteration 99, loss = 0.08464745\n",
      "Iteration 100, loss = 0.08408719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.40447225\n",
      "Iteration 2, loss = 0.84228657\n",
      "Iteration 3, loss = 0.58280523\n",
      "Iteration 4, loss = 0.47414351\n",
      "Iteration 5, loss = 0.42181286\n",
      "Iteration 6, loss = 0.41187985\n",
      "Iteration 7, loss = 0.34698406\n",
      "Iteration 8, loss = 0.44160231\n",
      "Iteration 9, loss = 0.47353841\n",
      "Iteration 10, loss = 0.45400233\n",
      "Iteration 11, loss = 0.36375405\n",
      "Iteration 12, loss = 0.41469677\n",
      "Iteration 13, loss = 0.43280841\n",
      "Iteration 14, loss = 0.32195222\n",
      "Iteration 15, loss = 0.50506922\n",
      "Iteration 16, loss = 0.38511848\n",
      "Iteration 17, loss = 0.33927299\n",
      "Iteration 18, loss = 0.25286368\n",
      "Iteration 19, loss = 0.32815632\n",
      "Iteration 20, loss = 0.32939903\n",
      "Iteration 21, loss = 0.24379269\n",
      "Iteration 22, loss = 0.26887833\n",
      "Iteration 23, loss = 0.29088174\n",
      "Iteration 24, loss = 0.32719957\n",
      "Iteration 25, loss = 0.28204922\n",
      "Iteration 26, loss = 0.29805003\n",
      "Iteration 27, loss = 0.23062765\n",
      "Iteration 28, loss = 0.25561282\n",
      "Iteration 29, loss = 0.21751165\n",
      "Iteration 30, loss = 0.24646066\n",
      "Iteration 31, loss = 0.26567279\n",
      "Iteration 32, loss = 0.24360864\n",
      "Iteration 33, loss = 0.25315361\n",
      "Iteration 34, loss = 0.28673512\n",
      "Iteration 35, loss = 0.19602063\n",
      "Iteration 36, loss = 0.16174325\n",
      "Iteration 37, loss = 0.29096088\n",
      "Iteration 38, loss = 0.28745375\n",
      "Iteration 39, loss = 0.28485646\n",
      "Iteration 40, loss = 0.25660397\n",
      "Iteration 41, loss = 0.31585435\n",
      "Iteration 42, loss = 0.27595098\n",
      "Iteration 43, loss = 0.22256974\n",
      "Iteration 44, loss = 0.26260963\n",
      "Iteration 45, loss = 0.24998056\n",
      "Iteration 46, loss = 0.25167114\n",
      "Iteration 47, loss = 0.22368093\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 48, loss = 0.18880280\n",
      "Iteration 49, loss = 0.15972142\n",
      "Iteration 50, loss = 0.13847432\n",
      "Iteration 51, loss = 0.13009909\n",
      "Iteration 52, loss = 0.12830740\n",
      "Iteration 53, loss = 0.12022565\n",
      "Iteration 54, loss = 0.11611112\n",
      "Iteration 55, loss = 0.11257606\n",
      "Iteration 56, loss = 0.11027428\n",
      "Iteration 57, loss = 0.10832021\n",
      "Iteration 58, loss = 0.10525853\n",
      "Iteration 59, loss = 0.10481783\n",
      "Iteration 60, loss = 0.10203937\n",
      "Iteration 61, loss = 0.10115114\n",
      "Iteration 62, loss = 0.10016411\n",
      "Iteration 63, loss = 0.09944168\n",
      "Iteration 64, loss = 0.09843377\n",
      "Iteration 65, loss = 0.09815339\n",
      "Iteration 66, loss = 0.09873529\n",
      "Iteration 67, loss = 0.09967189\n",
      "Iteration 68, loss = 0.09380224\n",
      "Iteration 69, loss = 0.09247568\n",
      "Iteration 70, loss = 0.08924175\n",
      "Iteration 71, loss = 0.09727304\n",
      "Iteration 72, loss = 0.09109379\n",
      "Iteration 73, loss = 0.08699100\n",
      "Iteration 74, loss = 0.08759420\n",
      "Iteration 75, loss = 0.08577257\n",
      "Iteration 76, loss = 0.08507833\n",
      "Iteration 77, loss = 0.08466824\n",
      "Iteration 78, loss = 0.08443297\n",
      "Iteration 79, loss = 0.08356430\n",
      "Iteration 80, loss = 0.08559699\n",
      "Iteration 81, loss = 0.08479811\n",
      "Iteration 82, loss = 0.08426016\n",
      "Iteration 83, loss = 0.08167553\n",
      "Iteration 84, loss = 0.08092865\n",
      "Iteration 85, loss = 0.08052152\n",
      "Iteration 86, loss = 0.08018943\n",
      "Iteration 87, loss = 0.07927182\n",
      "Iteration 88, loss = 0.07864990\n",
      "Iteration 89, loss = 0.07841002\n",
      "Iteration 90, loss = 0.07805988\n",
      "Iteration 91, loss = 0.07754702\n",
      "Iteration 92, loss = 0.07693557\n",
      "Iteration 93, loss = 0.07676436\n",
      "Iteration 94, loss = 0.07528624\n",
      "Iteration 95, loss = 0.07503094\n",
      "Iteration 96, loss = 0.07464393\n",
      "Iteration 97, loss = 0.07452685\n",
      "Iteration 98, loss = 0.07412619\n",
      "Iteration 99, loss = 0.07379099\n",
      "Iteration 100, loss = 0.07350404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.52278019\n",
      "Iteration 2, loss = 0.84779570\n",
      "Iteration 3, loss = 0.72803874\n",
      "Iteration 4, loss = 0.56634216\n",
      "Iteration 5, loss = 0.46621780\n",
      "Iteration 6, loss = 0.50143246\n",
      "Iteration 7, loss = 0.42105255\n",
      "Iteration 8, loss = 0.44827647\n",
      "Iteration 9, loss = 0.34569300\n",
      "Iteration 10, loss = 0.26275904\n",
      "Iteration 11, loss = 0.38939673\n",
      "Iteration 12, loss = 0.33407689\n",
      "Iteration 13, loss = 0.39363039\n",
      "Iteration 14, loss = 0.35924386\n",
      "Iteration 15, loss = 0.31203809\n",
      "Iteration 16, loss = 0.35584013\n",
      "Iteration 17, loss = 0.45992281\n",
      "Iteration 18, loss = 0.46362365\n",
      "Iteration 19, loss = 0.43423458\n",
      "Iteration 20, loss = 0.33456385\n",
      "Iteration 21, loss = 0.30065731\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 22, loss = 0.20475431\n",
      "Iteration 23, loss = 0.16454463\n",
      "Iteration 24, loss = 0.15665628\n",
      "Iteration 25, loss = 0.15067873\n",
      "Iteration 26, loss = 0.14805300\n",
      "Iteration 27, loss = 0.13457457\n",
      "Iteration 28, loss = 0.13566643\n",
      "Iteration 29, loss = 0.13227125\n",
      "Iteration 30, loss = 0.12115026\n",
      "Iteration 31, loss = 0.11888151\n",
      "Iteration 32, loss = 0.11894979\n",
      "Iteration 33, loss = 0.11480470\n",
      "Iteration 34, loss = 0.10858818\n",
      "Iteration 35, loss = 0.10892385\n",
      "Iteration 36, loss = 0.10570857\n",
      "Iteration 37, loss = 0.10197751\n",
      "Iteration 38, loss = 0.10267728\n",
      "Iteration 39, loss = 0.09954996\n",
      "Iteration 40, loss = 0.09627887\n",
      "Iteration 41, loss = 0.09731066\n",
      "Iteration 42, loss = 0.09866030\n",
      "Iteration 43, loss = 0.09409393\n",
      "Iteration 44, loss = 0.09256601\n",
      "Iteration 45, loss = 0.09079354\n",
      "Iteration 46, loss = 0.09985262\n",
      "Iteration 47, loss = 0.09088528\n",
      "Iteration 48, loss = 0.08814344\n",
      "Iteration 49, loss = 0.08932233\n",
      "Iteration 50, loss = 0.08700232\n",
      "Iteration 51, loss = 0.08454673\n",
      "Iteration 52, loss = 0.08520833\n",
      "Iteration 53, loss = 0.08405254\n",
      "Iteration 54, loss = 0.08352318\n",
      "Iteration 55, loss = 0.08190929\n",
      "Iteration 56, loss = 0.08107086\n",
      "Iteration 57, loss = 0.08065676\n",
      "Iteration 58, loss = 0.07994872\n",
      "Iteration 59, loss = 0.07962817\n",
      "Iteration 60, loss = 0.07900347\n",
      "Iteration 61, loss = 0.08008067\n",
      "Iteration 62, loss = 0.07858561\n",
      "Iteration 63, loss = 0.07665504\n",
      "Iteration 64, loss = 0.07581473\n",
      "Iteration 65, loss = 0.07484490\n",
      "Iteration 66, loss = 0.07426865\n",
      "Iteration 67, loss = 0.07399827\n",
      "Iteration 68, loss = 0.07349799\n",
      "Iteration 69, loss = 0.07336428\n",
      "Iteration 70, loss = 0.07222063\n",
      "Iteration 71, loss = 0.07180162\n",
      "Iteration 72, loss = 0.07151669\n",
      "Iteration 73, loss = 0.07105606\n",
      "Iteration 74, loss = 0.07065216\n",
      "Iteration 75, loss = 0.07262845\n",
      "Iteration 76, loss = 0.06978057\n",
      "Iteration 77, loss = 0.06968263\n",
      "Iteration 78, loss = 0.07057704\n",
      "Iteration 79, loss = 0.06855071\n",
      "Iteration 80, loss = 0.06802989\n",
      "Iteration 81, loss = 0.06727960\n",
      "Iteration 82, loss = 0.06678198\n",
      "Iteration 83, loss = 0.06646302\n",
      "Iteration 84, loss = 0.06615485\n",
      "Iteration 85, loss = 0.06576766\n",
      "Iteration 86, loss = 0.06553663\n",
      "Iteration 87, loss = 0.06518084\n",
      "Iteration 88, loss = 0.06493954\n",
      "Iteration 89, loss = 0.06465845\n",
      "Iteration 90, loss = 0.06438472\n",
      "Iteration 91, loss = 0.06407154\n",
      "Iteration 92, loss = 0.06397156\n",
      "Iteration 93, loss = 0.06363100\n",
      "Iteration 94, loss = 0.06344727\n",
      "Iteration 95, loss = 0.06318846\n",
      "Iteration 96, loss = 0.06295420\n",
      "Iteration 97, loss = 0.06276866\n",
      "Iteration 98, loss = 0.06253730\n",
      "Iteration 99, loss = 0.06234831\n",
      "Iteration 100, loss = 0.06211745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.36533034\n",
      "Iteration 2, loss = 0.71834458\n",
      "Iteration 3, loss = 0.57268307\n",
      "Iteration 4, loss = 0.44213120\n",
      "Iteration 5, loss = 0.42874199\n",
      "Iteration 6, loss = 0.39985644\n",
      "Iteration 7, loss = 0.40162974\n",
      "Iteration 8, loss = 0.36250379\n",
      "Iteration 9, loss = 0.31026984\n",
      "Iteration 10, loss = 0.45772177\n",
      "Iteration 11, loss = 0.35660546\n",
      "Iteration 12, loss = 0.36203650\n",
      "Iteration 13, loss = 0.31959030\n",
      "Iteration 14, loss = 0.30197560\n",
      "Iteration 15, loss = 0.26808525\n",
      "Iteration 16, loss = 0.27946539\n",
      "Iteration 17, loss = 0.26364476\n",
      "Iteration 18, loss = 0.29785884\n",
      "Iteration 19, loss = 0.24223483\n",
      "Iteration 20, loss = 0.22983253\n",
      "Iteration 21, loss = 0.19848150\n",
      "Iteration 22, loss = 0.39433199\n",
      "Iteration 23, loss = 0.38108528\n",
      "Iteration 24, loss = 0.31780770\n",
      "Iteration 25, loss = 0.39356055\n",
      "Iteration 26, loss = 0.53865586\n",
      "Iteration 27, loss = 0.30987108\n",
      "Iteration 28, loss = 0.27413400\n",
      "Iteration 29, loss = 0.39684791\n",
      "Iteration 30, loss = 0.45388245\n",
      "Iteration 31, loss = 0.36109930\n",
      "Iteration 32, loss = 0.25132614\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 33, loss = 0.22454475\n",
      "Iteration 34, loss = 0.19119421\n",
      "Iteration 35, loss = 0.17284727\n",
      "Iteration 36, loss = 0.15361631\n",
      "Iteration 37, loss = 0.14771237\n",
      "Iteration 38, loss = 0.13934552\n",
      "Iteration 39, loss = 0.13407171\n",
      "Iteration 40, loss = 0.12304208\n",
      "Iteration 41, loss = 0.12339335\n",
      "Iteration 42, loss = 0.12109471\n",
      "Iteration 43, loss = 0.11315465\n",
      "Iteration 44, loss = 0.10969007\n",
      "Iteration 45, loss = 0.10770415\n",
      "Iteration 46, loss = 0.10849446\n",
      "Iteration 47, loss = 0.11410289\n",
      "Iteration 48, loss = 0.10765567\n",
      "Iteration 49, loss = 0.10305686\n",
      "Iteration 50, loss = 0.10113104\n",
      "Iteration 51, loss = 0.09702600\n",
      "Iteration 52, loss = 0.09727859\n",
      "Iteration 53, loss = 0.09545916\n",
      "Iteration 54, loss = 0.09552058\n",
      "Iteration 55, loss = 0.09509952\n",
      "Iteration 56, loss = 0.09306678\n",
      "Iteration 57, loss = 0.09226369\n",
      "Iteration 58, loss = 0.09187435\n",
      "Iteration 59, loss = 0.09134184\n",
      "Iteration 60, loss = 0.09105084\n",
      "Iteration 61, loss = 0.09044825\n",
      "Iteration 62, loss = 0.09005534\n",
      "Iteration 63, loss = 0.08963051\n",
      "Iteration 64, loss = 0.08935473\n",
      "Iteration 65, loss = 0.08881883\n",
      "Iteration 66, loss = 0.08835345\n",
      "Iteration 67, loss = 0.08796652\n",
      "Iteration 68, loss = 0.08770749\n",
      "Iteration 69, loss = 0.08797624\n",
      "Iteration 70, loss = 0.08688741\n",
      "Iteration 71, loss = 0.08638588\n",
      "Iteration 72, loss = 0.08554452\n",
      "Iteration 73, loss = 0.08519015\n",
      "Iteration 74, loss = 0.08484027\n",
      "Iteration 75, loss = 0.08450366\n",
      "Iteration 76, loss = 0.08421071\n",
      "Iteration 77, loss = 0.08383236\n",
      "Iteration 78, loss = 0.08359742\n",
      "Iteration 79, loss = 0.08325453\n",
      "Iteration 80, loss = 0.08301553\n",
      "Iteration 81, loss = 0.08268614\n",
      "Iteration 82, loss = 0.08245382\n",
      "Iteration 83, loss = 0.08212616\n",
      "Iteration 84, loss = 0.08192507\n",
      "Iteration 85, loss = 0.08170135\n",
      "Iteration 86, loss = 0.08149349\n",
      "Iteration 87, loss = 0.08112465\n",
      "Iteration 88, loss = 0.08097560\n",
      "Iteration 89, loss = 0.08068590\n",
      "Iteration 90, loss = 0.08045882\n",
      "Iteration 91, loss = 0.08029186\n",
      "Iteration 92, loss = 0.08009585\n",
      "Iteration 93, loss = 0.07980125\n",
      "Iteration 94, loss = 0.07967101\n",
      "Iteration 95, loss = 0.07944213\n",
      "Iteration 96, loss = 0.07915595\n",
      "Iteration 97, loss = 0.07904417\n",
      "Iteration 98, loss = 0.07879695\n",
      "Iteration 99, loss = 0.07858562\n",
      "Iteration 100, loss = 0.07835397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.60738548\n",
      "Iteration 2, loss = 0.82496233\n",
      "Iteration 3, loss = 0.66134795\n",
      "Iteration 4, loss = 0.49999171\n",
      "Iteration 5, loss = 0.58324753\n",
      "Iteration 6, loss = 0.52272224\n",
      "Iteration 7, loss = 0.43847818\n",
      "Iteration 8, loss = 0.48006980\n",
      "Iteration 9, loss = 0.46170678\n",
      "Iteration 10, loss = 0.44107536\n",
      "Iteration 11, loss = 0.46995874\n",
      "Iteration 12, loss = 0.43277790\n",
      "Iteration 13, loss = 0.48309974\n",
      "Iteration 14, loss = 0.47643245\n",
      "Iteration 15, loss = 0.44679827\n",
      "Iteration 16, loss = 0.53052583\n",
      "Iteration 17, loss = 0.42110914\n",
      "Iteration 18, loss = 0.33676304\n",
      "Iteration 19, loss = 0.37095819\n",
      "Iteration 20, loss = 0.35786105\n",
      "Iteration 21, loss = 0.39469717\n",
      "Iteration 22, loss = 0.38486037\n",
      "Iteration 23, loss = 0.32875063\n",
      "Iteration 24, loss = 0.37792598\n",
      "Iteration 25, loss = 0.30957176\n",
      "Iteration 26, loss = 0.32597799\n",
      "Iteration 27, loss = 0.32212198\n",
      "Iteration 28, loss = 0.30060133\n",
      "Iteration 29, loss = 0.27599928\n",
      "Iteration 30, loss = 0.34647891\n",
      "Iteration 31, loss = 0.29051875\n",
      "Iteration 32, loss = 0.23006882\n",
      "Iteration 33, loss = 0.42720967\n",
      "Iteration 34, loss = 0.55615563\n",
      "Iteration 35, loss = 0.39696338\n",
      "Iteration 36, loss = 0.31074515\n",
      "Iteration 37, loss = 0.25123429\n",
      "Iteration 38, loss = 0.26094826\n",
      "Iteration 39, loss = 0.22096545\n",
      "Iteration 40, loss = 0.23158405\n",
      "Iteration 41, loss = 0.29409329\n",
      "Iteration 42, loss = 0.24819119\n",
      "Iteration 43, loss = 0.19583903\n",
      "Iteration 44, loss = 0.19150903\n",
      "Iteration 45, loss = 0.20153628\n",
      "Iteration 46, loss = 0.23696522\n",
      "Iteration 47, loss = 0.21046297\n",
      "Iteration 48, loss = 0.17130225\n",
      "Iteration 49, loss = 0.27112725\n",
      "Iteration 50, loss = 0.32433632\n",
      "Iteration 51, loss = 0.27194568\n",
      "Iteration 52, loss = 0.24341456\n",
      "Iteration 53, loss = 0.30892408\n",
      "Iteration 54, loss = 0.29915848\n",
      "Iteration 55, loss = 0.43603070\n",
      "Iteration 56, loss = 0.28518414\n",
      "Iteration 57, loss = 0.30227031\n",
      "Iteration 58, loss = 0.24787525\n",
      "Iteration 59, loss = 0.16927036\n",
      "Iteration 60, loss = 0.19297724\n",
      "Iteration 61, loss = 0.16675187\n",
      "Iteration 62, loss = 0.22570862\n",
      "Iteration 63, loss = 0.20465056\n",
      "Iteration 64, loss = 0.31303590\n",
      "Iteration 65, loss = 0.22118680\n",
      "Iteration 66, loss = 0.21563097\n",
      "Iteration 67, loss = 0.28037887\n",
      "Iteration 68, loss = 0.36575362\n",
      "Iteration 69, loss = 0.23327333\n",
      "Iteration 70, loss = 0.25485050\n",
      "Iteration 71, loss = 0.20874008\n",
      "Iteration 72, loss = 0.22606105\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 73, loss = 0.19536852\n",
      "Iteration 74, loss = 0.17128453\n",
      "Iteration 75, loss = 0.16529558\n",
      "Iteration 76, loss = 0.14553448\n",
      "Iteration 77, loss = 0.14217048\n",
      "Iteration 78, loss = 0.13505444\n",
      "Iteration 79, loss = 0.12181061\n",
      "Iteration 80, loss = 0.11418069\n",
      "Iteration 81, loss = 0.10826378\n",
      "Iteration 82, loss = 0.10247901\n",
      "Iteration 83, loss = 0.09873395\n",
      "Iteration 84, loss = 0.09837374\n",
      "Iteration 85, loss = 0.09471506\n",
      "Iteration 86, loss = 0.08540141\n",
      "Iteration 87, loss = 0.08324669\n",
      "Iteration 88, loss = 0.08212451\n",
      "Iteration 89, loss = 0.08134511\n",
      "Iteration 90, loss = 0.07909341\n",
      "Iteration 91, loss = 0.07692873\n",
      "Iteration 92, loss = 0.07881620\n",
      "Iteration 93, loss = 0.07813903\n",
      "Iteration 94, loss = 0.07320555\n",
      "Iteration 95, loss = 0.07306963\n",
      "Iteration 96, loss = 0.07202109\n",
      "Iteration 97, loss = 0.07186229\n",
      "Iteration 98, loss = 0.06988429\n",
      "Iteration 99, loss = 0.06962266\n",
      "Iteration 100, loss = 0.07003458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.36311114\n",
      "Iteration 2, loss = 0.75019321\n",
      "Iteration 3, loss = 0.59412272\n",
      "Iteration 4, loss = 0.42815161\n",
      "Iteration 5, loss = 0.45367519\n",
      "Iteration 6, loss = 0.39839043\n",
      "Iteration 7, loss = 0.37110129\n",
      "Iteration 8, loss = 0.42197536\n",
      "Iteration 9, loss = 0.42413571\n",
      "Iteration 10, loss = 0.37379191\n",
      "Iteration 11, loss = 0.30413315\n",
      "Iteration 12, loss = 0.36396547\n",
      "Iteration 13, loss = 0.27269002\n",
      "Iteration 14, loss = 0.26789052\n",
      "Iteration 15, loss = 0.28814519\n",
      "Iteration 16, loss = 0.25227041\n",
      "Iteration 17, loss = 0.24703972\n",
      "Iteration 18, loss = 0.30176534\n",
      "Iteration 19, loss = 0.29295707\n",
      "Iteration 20, loss = 0.34234078\n",
      "Iteration 21, loss = 0.43844647\n",
      "Iteration 22, loss = 0.44167861\n",
      "Iteration 23, loss = 0.31406385\n",
      "Iteration 24, loss = 0.24028034\n",
      "Iteration 25, loss = 0.32712277\n",
      "Iteration 26, loss = 0.36928678\n",
      "Iteration 27, loss = 0.23820184\n",
      "Iteration 28, loss = 0.26707023\n",
      "Iteration 29, loss = 0.24669428\n",
      "Iteration 30, loss = 0.18246703\n",
      "Iteration 31, loss = 0.18725044\n",
      "Iteration 32, loss = 0.23282085\n",
      "Iteration 33, loss = 0.18634838\n",
      "Iteration 34, loss = 0.18095915\n",
      "Iteration 35, loss = 0.14752036\n",
      "Iteration 36, loss = 0.15359329\n",
      "Iteration 37, loss = 0.22121984\n",
      "Iteration 38, loss = 0.20665988\n",
      "Iteration 39, loss = 0.12705174\n",
      "Iteration 40, loss = 0.12690799\n",
      "Iteration 41, loss = 0.14027987\n",
      "Iteration 42, loss = 0.14130619\n",
      "Iteration 43, loss = 0.13315974\n",
      "Iteration 44, loss = 0.14368845\n",
      "Iteration 45, loss = 0.13899924\n",
      "Iteration 46, loss = 0.13511100\n",
      "Iteration 47, loss = 0.20315058\n",
      "Iteration 48, loss = 0.11859975\n",
      "Iteration 49, loss = 0.13920511\n",
      "Iteration 50, loss = 0.14757889\n",
      "Iteration 51, loss = 0.17489796\n",
      "Iteration 52, loss = 0.23340829\n",
      "Iteration 53, loss = 0.20737969\n",
      "Iteration 54, loss = 0.19869164\n",
      "Iteration 55, loss = 0.20008962\n",
      "Iteration 56, loss = 0.15953248\n",
      "Iteration 57, loss = 0.12595954\n",
      "Iteration 58, loss = 0.17244756\n",
      "Iteration 59, loss = 0.12948607\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 60, loss = 0.11369119\n",
      "Iteration 61, loss = 0.09021903\n",
      "Iteration 62, loss = 0.08221483\n",
      "Iteration 63, loss = 0.07490528\n",
      "Iteration 64, loss = 0.07034310\n",
      "Iteration 65, loss = 0.06866034\n",
      "Iteration 66, loss = 0.06569196\n",
      "Iteration 67, loss = 0.06394182\n",
      "Iteration 68, loss = 0.06269842\n",
      "Iteration 69, loss = 0.06028604\n",
      "Iteration 70, loss = 0.06094400\n",
      "Iteration 71, loss = 0.05956650\n",
      "Iteration 72, loss = 0.05781417\n",
      "Iteration 73, loss = 0.05681084\n",
      "Iteration 74, loss = 0.05622379\n",
      "Iteration 75, loss = 0.05470050\n",
      "Iteration 76, loss = 0.05388021\n",
      "Iteration 77, loss = 0.05534338\n",
      "Iteration 78, loss = 0.05220138\n",
      "Iteration 79, loss = 0.05263758\n",
      "Iteration 80, loss = 0.05025310\n",
      "Iteration 81, loss = 0.04965394\n",
      "Iteration 82, loss = 0.04909904\n",
      "Iteration 83, loss = 0.04859082\n",
      "Iteration 84, loss = 0.04828216\n",
      "Iteration 85, loss = 0.04780988\n",
      "Iteration 86, loss = 0.04751830\n",
      "Iteration 87, loss = 0.04716682\n",
      "Iteration 88, loss = 0.04681328\n",
      "Iteration 89, loss = 0.04650721\n",
      "Iteration 90, loss = 0.04620449\n",
      "Iteration 91, loss = 0.04586660\n",
      "Iteration 92, loss = 0.04573286\n",
      "Iteration 93, loss = 0.04546441\n",
      "Iteration 94, loss = 0.04514361\n",
      "Iteration 95, loss = 0.04450996\n",
      "Iteration 96, loss = 0.04415309\n",
      "Iteration 97, loss = 0.04397773\n",
      "Iteration 98, loss = 0.04389444\n",
      "Iteration 99, loss = 0.04345940\n",
      "Iteration 100, loss = 0.04321574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.40242211\n",
      "Iteration 2, loss = 0.72017986\n",
      "Iteration 3, loss = 0.71130496\n",
      "Iteration 4, loss = 0.51202407\n",
      "Iteration 5, loss = 0.52274688\n",
      "Iteration 6, loss = 0.51171490\n",
      "Iteration 7, loss = 0.54559552\n",
      "Iteration 8, loss = 0.50418506\n",
      "Iteration 9, loss = 0.46889342\n",
      "Iteration 10, loss = 0.39348258\n",
      "Iteration 11, loss = 0.33744775\n",
      "Iteration 12, loss = 0.38439324\n",
      "Iteration 13, loss = 0.44961391\n",
      "Iteration 14, loss = 0.35582881\n",
      "Iteration 15, loss = 0.50374376\n",
      "Iteration 16, loss = 0.32031923\n",
      "Iteration 17, loss = 0.35640896\n",
      "Iteration 18, loss = 0.34458595\n",
      "Iteration 19, loss = 0.43035885\n",
      "Iteration 20, loss = 0.41285999\n",
      "Iteration 21, loss = 0.29565859\n",
      "Iteration 22, loss = 0.39396367\n",
      "Iteration 23, loss = 0.36638482\n",
      "Iteration 24, loss = 0.37678778\n",
      "Iteration 25, loss = 0.31167460\n",
      "Iteration 26, loss = 0.23434984\n",
      "Iteration 27, loss = 0.31217919\n",
      "Iteration 28, loss = 0.31056857\n",
      "Iteration 29, loss = 0.27341222\n",
      "Iteration 30, loss = 0.28827861\n",
      "Iteration 31, loss = 0.28618210\n",
      "Iteration 32, loss = 0.39409303\n",
      "Iteration 33, loss = 0.26686703\n",
      "Iteration 34, loss = 0.31705224\n",
      "Iteration 35, loss = 0.35469183\n",
      "Iteration 36, loss = 0.34636895\n",
      "Iteration 37, loss = 0.22964531\n",
      "Iteration 38, loss = 0.23065762\n",
      "Iteration 39, loss = 0.26344061\n",
      "Iteration 40, loss = 0.25136938\n",
      "Iteration 41, loss = 0.21658385\n",
      "Iteration 42, loss = 0.34421831\n",
      "Iteration 43, loss = 0.33793527\n",
      "Iteration 44, loss = 0.30371346\n",
      "Iteration 45, loss = 0.24265974\n",
      "Iteration 46, loss = 0.25864549\n",
      "Iteration 47, loss = 0.25653015\n",
      "Iteration 48, loss = 0.24323918\n",
      "Iteration 49, loss = 0.24509751\n",
      "Iteration 50, loss = 0.24147821\n",
      "Iteration 51, loss = 0.23646252\n",
      "Iteration 52, loss = 0.17459067\n",
      "Iteration 53, loss = 0.15765982\n",
      "Iteration 54, loss = 0.15157042\n",
      "Iteration 55, loss = 0.15853456\n",
      "Iteration 56, loss = 0.18844249\n",
      "Iteration 57, loss = 0.15557232\n",
      "Iteration 58, loss = 0.15812117\n",
      "Iteration 59, loss = 0.16588442\n",
      "Iteration 60, loss = 0.19915325\n",
      "Iteration 61, loss = 0.19759524\n",
      "Iteration 62, loss = 0.21278665\n",
      "Iteration 63, loss = 0.19599094\n",
      "Iteration 64, loss = 0.20092358\n",
      "Iteration 65, loss = 0.20716828\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 66, loss = 0.16140120\n",
      "Iteration 67, loss = 0.13310024\n",
      "Iteration 68, loss = 0.12144719\n",
      "Iteration 69, loss = 0.11986353\n",
      "Iteration 70, loss = 0.11574382\n",
      "Iteration 71, loss = 0.11282875\n",
      "Iteration 72, loss = 0.11363308\n",
      "Iteration 73, loss = 0.11378330\n",
      "Iteration 74, loss = 0.10806714\n",
      "Iteration 75, loss = 0.10624029\n",
      "Iteration 76, loss = 0.10458193\n",
      "Iteration 77, loss = 0.10307886\n",
      "Iteration 78, loss = 0.10044681\n",
      "Iteration 79, loss = 0.09526045\n",
      "Iteration 80, loss = 0.09411424\n",
      "Iteration 81, loss = 0.09346942\n",
      "Iteration 82, loss = 0.09230352\n",
      "Iteration 83, loss = 0.09134022\n",
      "Iteration 84, loss = 0.09034977\n",
      "Iteration 85, loss = 0.08965824\n",
      "Iteration 86, loss = 0.08967505\n",
      "Iteration 87, loss = 0.08878449\n",
      "Iteration 88, loss = 0.08776003\n",
      "Iteration 89, loss = 0.08739972\n",
      "Iteration 90, loss = 0.08696395\n",
      "Iteration 91, loss = 0.08650321\n",
      "Iteration 92, loss = 0.08576437\n",
      "Iteration 93, loss = 0.08588519\n",
      "Iteration 94, loss = 0.08493194\n",
      "Iteration 95, loss = 0.08437984\n",
      "Iteration 96, loss = 0.08418956\n",
      "Iteration 97, loss = 0.08387231\n",
      "Iteration 98, loss = 0.08326616\n",
      "Iteration 99, loss = 0.08295004\n",
      "Iteration 100, loss = 0.08339629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.34145555\n",
      "Iteration 2, loss = 0.84653465\n",
      "Iteration 3, loss = 0.66345027\n",
      "Iteration 4, loss = 0.53520983\n",
      "Iteration 5, loss = 0.52684630\n",
      "Iteration 6, loss = 0.46033699\n",
      "Iteration 7, loss = 0.43811672\n",
      "Iteration 8, loss = 0.30785506\n",
      "Iteration 9, loss = 0.40993517\n",
      "Iteration 10, loss = 0.32837644\n",
      "Iteration 11, loss = 0.35153513\n",
      "Iteration 12, loss = 0.27585216\n",
      "Iteration 13, loss = 0.24298298\n",
      "Iteration 14, loss = 0.31557805\n",
      "Iteration 15, loss = 0.44571893\n",
      "Iteration 16, loss = 0.41706528\n",
      "Iteration 17, loss = 0.25423977\n",
      "Iteration 18, loss = 0.28970609\n",
      "Iteration 19, loss = 0.26648383\n",
      "Iteration 20, loss = 0.19853102\n",
      "Iteration 21, loss = 0.32650404\n",
      "Iteration 22, loss = 0.25292379\n",
      "Iteration 23, loss = 0.29427131\n",
      "Iteration 24, loss = 0.20209679\n",
      "Iteration 25, loss = 0.24408020\n",
      "Iteration 26, loss = 0.21197482\n",
      "Iteration 27, loss = 0.25424945\n",
      "Iteration 28, loss = 0.27214458\n",
      "Iteration 29, loss = 0.28289971\n",
      "Iteration 30, loss = 0.31300568\n",
      "Iteration 31, loss = 0.22431994\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 32, loss = 0.18667453\n",
      "Iteration 33, loss = 0.13604325\n",
      "Iteration 34, loss = 0.11813985\n",
      "Iteration 35, loss = 0.11053551\n",
      "Iteration 36, loss = 0.09474582\n",
      "Iteration 37, loss = 0.08923461\n",
      "Iteration 38, loss = 0.09258555\n",
      "Iteration 39, loss = 0.08769066\n",
      "Iteration 40, loss = 0.08460804\n",
      "Iteration 41, loss = 0.08190735\n",
      "Iteration 42, loss = 0.08111999\n",
      "Iteration 43, loss = 0.07842763\n",
      "Iteration 44, loss = 0.07658983\n",
      "Iteration 45, loss = 0.07626228\n",
      "Iteration 46, loss = 0.07430334\n",
      "Iteration 47, loss = 0.07331817\n",
      "Iteration 48, loss = 0.07226499\n",
      "Iteration 49, loss = 0.07178902\n",
      "Iteration 50, loss = 0.06839514\n",
      "Iteration 51, loss = 0.06747765\n",
      "Iteration 52, loss = 0.06658443\n",
      "Iteration 53, loss = 0.06602609\n",
      "Iteration 54, loss = 0.06553687\n",
      "Iteration 55, loss = 0.06486349\n",
      "Iteration 56, loss = 0.06459095\n",
      "Iteration 57, loss = 0.06382123\n",
      "Iteration 58, loss = 0.06345172\n",
      "Iteration 59, loss = 0.06530047\n",
      "Iteration 60, loss = 0.06398736\n",
      "Iteration 61, loss = 0.06179397\n",
      "Iteration 62, loss = 0.05998220\n",
      "Iteration 63, loss = 0.05956247\n",
      "Iteration 64, loss = 0.05918324\n",
      "Iteration 65, loss = 0.05871895\n",
      "Iteration 66, loss = 0.05841669\n",
      "Iteration 67, loss = 0.05888540\n",
      "Iteration 68, loss = 0.05844936\n",
      "Iteration 69, loss = 0.05762983\n",
      "Iteration 70, loss = 0.05733739\n",
      "Iteration 71, loss = 0.05690006\n",
      "Iteration 72, loss = 0.06175747\n",
      "Iteration 73, loss = 0.05676777\n",
      "Iteration 74, loss = 0.05643619\n",
      "Iteration 75, loss = 0.05611635\n",
      "Iteration 76, loss = 0.05583435\n",
      "Iteration 77, loss = 0.05552031\n",
      "Iteration 78, loss = 0.05522873\n",
      "Iteration 79, loss = 0.05502317\n",
      "Iteration 80, loss = 0.05475509\n",
      "Iteration 81, loss = 0.05377500\n",
      "Iteration 82, loss = 0.05444287\n",
      "Iteration 83, loss = 0.05372041\n",
      "Iteration 84, loss = 0.05438910\n",
      "Iteration 85, loss = 0.05063476\n",
      "Iteration 86, loss = 0.05047936\n",
      "Iteration 87, loss = 0.04941307\n",
      "Iteration 88, loss = 0.04830728\n",
      "Iteration 89, loss = 0.05217118\n",
      "Iteration 90, loss = 0.04800459\n",
      "Iteration 91, loss = 0.04678775\n",
      "Iteration 92, loss = 0.04636610\n",
      "Iteration 93, loss = 0.04570537\n",
      "Iteration 94, loss = 0.04547150\n",
      "Iteration 95, loss = 0.04464179\n",
      "Iteration 96, loss = 0.04430581\n",
      "Iteration 97, loss = 0.04388942\n",
      "Iteration 98, loss = 0.04351269\n",
      "Iteration 99, loss = 0.04328803\n",
      "Iteration 100, loss = 0.04297614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import time\n",
    "score_matrice_train=np.zeros((10,5))\n",
    "score_matrice_test=np.zeros((10,5))\n",
    "time_matricefit=np.zeros((10,5))\n",
    "time_matrice=np.zeros((10,5))\n",
    "compte_columns=0\n",
    "for hiden_layer in range(12,25,3):\n",
    "    for compteur_ligne in range(10):\n",
    "        model = MLPClassifier(hidden_layer_sizes=hiden_layer,activation='tanh',solver='sgd', batch_size=1, alpha=0, learning_rate='adaptive', verbose=0,max_iter=100)\n",
    "\n",
    "       #j'entraine mon model et calcul du temps\n",
    "        start_time=time.time()\n",
    "        model.fit(X_train,y_train)\n",
    "        timefit=time.time() - start_time\n",
    "        time_matricefit[compteur_ligne][compte_columns]=timefit\n",
    "        \n",
    "        \n",
    "        #time sur le predict \n",
    "        start_time2=time.time()\n",
    "        model.predict(X_test)\n",
    "        timepredict=time.time() - start_time2\n",
    "        time_matrice[compteur_ligne][compte_columns]=timepredict\n",
    "        #je recupere les resultats pour le train\n",
    "        score_matrice_train[compteur_ligne][compte_columns]=model.score(X_train,y_train)\n",
    "        #je récupere les résultat pour la base de test\n",
    "        score_matrice_test[compteur_ligne][compte_columns]=model.score(X_test,y_test)\n",
    "\n",
    "    \n",
    "    #je passe a la colonne suivante    \n",
    "    compte_columns=compte_columns+1\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save de mes datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Sauvegarde des tableau de numpy en df pour eviter de les refaires a chauqe fois. random state = 42 on aura toujours les même resultats\n",
    "df_score_matrice_train = pd.DataFrame(data=score_matrice_train,columns=['C=12','C=15','C=18','C=21','C=24'])\n",
    "df_score_matrice_test = pd.DataFrame(data=score_matrice_test,columns=['C=12','C=15','C=18','C=21','C=24'])\n",
    "df_timefit_matrice = pd.DataFrame(data=time_matricefit,columns=['C=12','C=15','C=18','C=21','C=24'])\n",
    "df_time_matrice = pd.DataFrame(data=time_matrice,columns=['C=12','C=15','C=18','C=21','C=24'])\n",
    "\n",
    "#Export en PDF pour save en local et eviter le recalcul\n",
    "df_score_matrice_train.to_excel(\"df_score_matrice_train.xlsx\") \n",
    "df_score_matrice_test.to_excel(\"df_score_matrice_test.xlsx\") \n",
    "df_timefit_matrice.to_excel(\"df_timefit_matrice.xlsx\") \n",
    "df_time_matrice.to_excel(\"df_time_matrice.xlsx\") \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "je recupere mes datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_matrice_train=[]\n",
    "score_matrice_test=[]\n",
    "timefit_matrice=[]\n",
    "time_matrice=[]\n",
    "#recuperation des données\n",
    "score_matrice_train=pd.read_excel('df_score_matrice_train.xlsx', index_col=0)  \n",
    "score_matrice_test=pd.read_excel('df_score_matrice_test.xlsx', index_col=0)  \n",
    "timefit_matrice=pd.read_excel('df_timefit_matrice.xlsx', index_col=0) \n",
    "time_matrice=pd.read_excel('df_time_matrice.xlsx', index_col=0) \n",
    "#transformation en np\n",
    "score_matrice_train=np.array(score_matrice_train)\n",
    "score_matrice_test=np.array(score_matrice_test)\n",
    "timefit_matrice=np.array(timefit_matrice)\n",
    "time_matrice=np.array(time_matrice)\n",
    "#calcule de la moyenne du score pour chaque colonne\n",
    "accurancy_train=[]\n",
    "accurancy_test=[]\n",
    "accurancy_timefit=[]\n",
    "accurancy_time=[]\n",
    "for i in range(5):\n",
    "    accurancy_train.append(score_matrice_train[:,i].mean())\n",
    "    accurancy_test.append(score_matrice_test[:,i].mean())\n",
    "    accurancy_timefit.append(timefit_matrice[:,i].mean())\n",
    "    accurancy_time.append(time_matrice[:,i].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa55d50bc40>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=[\"C=12\",\"C=15\",\"C=18\",\"C=21\",\"C=24\"]\n",
    "scoretrain = plt.plot(x,accurancy_train, color = 'blue')\n",
    "scoretest = plt.plot(x,accurancy_test, color = 'green')\n",
    "\n",
    "plt.legend()\n",
    "plt.legend(scoretest, ['scoretest'])\n",
    "\n",
    "ax2 = plt.gca().twinx()\n",
    "time = ax2.plot(x,accurancy_time, color = 'red')\n",
    "\n",
    "\n",
    "plt.legend(time, ['time']) \n",
    "#reste a fix la legend\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous choissions C=15 au vu de nos resultat en effet avec ce choix nous minimisons le temps et minimisons la variance (resilience de notre model)entre test et train. Néanmoins notre taux d'erreur reste important pour un developement a grande echelle sachant que aujoutdhui le taux de reconnaisance est proche de 100 pourcent. En effet la moyenne du score entre 15 et 24 est negligeable comparé au temps de calcul."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin d’améliorer les performances en généralisation du réseau de neurones, on se propose de mettre en oeuvre un apprentissage avec arrêt précoce (early_stopping) par cross-validation.\n",
    "Changer les paramètres du réseau pour séparer la base d’apprentissage précédente en deux sets : apprentissage (80%) et validation croisée (20% : validation _fraction=0.2).\n",
    "Entraîner un réseau de neurones avec arrêt par cross-validation (fonction fit). Optimiser le nombre de neurones cachés C : répéter 10 fois l’apprentissage et calculer la moyenne et l’écart-type des taux en apprentissage et en validation afin de minimiser le biais et la variance (fonction score). Comparer avec les résultats obtenus précédemment. Conclure.\n",
    "Conserver les poids du réseau optimal. Donner la matrice de confusion sur la base de test. Comparer les résultats obtenus avec ceux de l’algorithme des k-plus-proches-voisins en termes de\n",
    "taux de reconnaissance et de temps de classification. Régler k sur une base de validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "je décide d'aller de 3 en 3 pour les neuronnes car sinon c'est bcp trop long en temps de calcul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.72968197\n",
      "Iteration 2, loss = 1.15222807\n",
      "Iteration 3, loss = 0.90589267\n",
      "Iteration 4, loss = 0.88844267\n",
      "Iteration 5, loss = 0.67933264\n",
      "Iteration 6, loss = 0.81334319\n",
      "Iteration 7, loss = 0.93147162\n",
      "Iteration 8, loss = 0.79741062\n",
      "Iteration 9, loss = 0.68907592\n",
      "Iteration 10, loss = 0.70029454\n",
      "Iteration 11, loss = 0.64039893\n",
      "Iteration 12, loss = 0.77396772\n",
      "Iteration 13, loss = 0.68177141\n",
      "Iteration 14, loss = 0.61224136\n",
      "Iteration 15, loss = 0.59380505\n",
      "Iteration 16, loss = 0.51309936\n",
      "Iteration 17, loss = 0.68401502\n",
      "Iteration 18, loss = 0.66998012\n",
      "Iteration 19, loss = 0.62059855\n",
      "Iteration 20, loss = 0.58739584\n",
      "Iteration 21, loss = 0.70001118\n",
      "Iteration 22, loss = 0.53078396\n",
      "Iteration 23, loss = 0.56448197\n",
      "Iteration 24, loss = 0.59902135\n",
      "Iteration 25, loss = 0.54584939\n",
      "Iteration 26, loss = 0.60702882\n",
      "Iteration 27, loss = 0.56200295\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 28, loss = 0.45749682\n",
      "Iteration 29, loss = 0.42301839\n",
      "Iteration 30, loss = 0.42114447\n",
      "Iteration 31, loss = 0.40366066\n",
      "Iteration 32, loss = 0.39639459\n",
      "Iteration 33, loss = 0.39555002\n",
      "Iteration 34, loss = 0.37901211\n",
      "Iteration 35, loss = 0.37828414\n",
      "Iteration 36, loss = 0.36996131\n",
      "Iteration 37, loss = 0.35215490\n",
      "Iteration 38, loss = 0.34664813\n",
      "Iteration 39, loss = 0.34527858\n",
      "Iteration 40, loss = 0.33955978\n",
      "Iteration 41, loss = 0.34724821\n",
      "Iteration 42, loss = 0.33762715\n",
      "Iteration 43, loss = 0.33912875\n",
      "Iteration 44, loss = 0.34826908\n",
      "Iteration 45, loss = 0.33402272\n",
      "Iteration 46, loss = 0.32748975\n",
      "Iteration 47, loss = 0.32801766\n",
      "Iteration 48, loss = 0.33404501\n",
      "Iteration 49, loss = 0.32457919\n",
      "Iteration 50, loss = 0.33499328\n",
      "Iteration 51, loss = 0.32188917\n",
      "Iteration 52, loss = 0.31244570\n",
      "Iteration 53, loss = 0.31850733\n",
      "Iteration 54, loss = 0.31663260\n",
      "Iteration 55, loss = 0.33372932\n",
      "Iteration 56, loss = 0.32571367\n",
      "Iteration 57, loss = 0.31429870\n",
      "Iteration 58, loss = 0.31079334\n",
      "Iteration 59, loss = 0.29729972\n",
      "Iteration 60, loss = 0.30006236\n",
      "Iteration 61, loss = 0.29715962\n",
      "Iteration 62, loss = 0.29426528\n",
      "Iteration 63, loss = 0.29484805\n",
      "Iteration 64, loss = 0.28716255\n",
      "Iteration 65, loss = 0.29413020\n",
      "Iteration 66, loss = 0.30360370\n",
      "Iteration 67, loss = 0.30832185\n",
      "Iteration 68, loss = 0.28880994\n",
      "Iteration 69, loss = 0.28907922\n",
      "Iteration 70, loss = 0.27856673\n",
      "Iteration 71, loss = 0.27965614\n",
      "Iteration 72, loss = 0.29000139\n",
      "Iteration 73, loss = 0.27645364\n",
      "Iteration 74, loss = 0.28412168\n",
      "Iteration 75, loss = 0.29706057\n",
      "Iteration 76, loss = 0.29373030\n",
      "Iteration 77, loss = 0.28299980\n",
      "Iteration 78, loss = 0.28606405\n",
      "Iteration 79, loss = 0.27885849\n",
      "Iteration 80, loss = 0.27753377\n",
      "Iteration 81, loss = 0.27115846\n",
      "Iteration 82, loss = 0.28782706\n",
      "Iteration 83, loss = 0.30022944\n",
      "Iteration 84, loss = 0.27681873\n",
      "Iteration 85, loss = 0.26909496\n",
      "Iteration 86, loss = 0.29603413\n",
      "Iteration 87, loss = 0.25307163\n",
      "Iteration 88, loss = 0.25268615\n",
      "Iteration 89, loss = 0.24556326\n",
      "Iteration 90, loss = 0.25723808\n",
      "Iteration 91, loss = 0.24813172\n",
      "Iteration 92, loss = 0.25103673\n",
      "Iteration 93, loss = 0.24397405\n",
      "Iteration 94, loss = 0.24016687\n",
      "Iteration 95, loss = 0.23971285\n",
      "Iteration 96, loss = 0.24115518\n",
      "Iteration 97, loss = 0.24685680\n",
      "Iteration 98, loss = 0.24040004\n",
      "Iteration 99, loss = 0.23489907\n",
      "Iteration 100, loss = 0.23757053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.62699449\n",
      "Iteration 2, loss = 1.18151612\n",
      "Iteration 3, loss = 0.96079905\n",
      "Iteration 4, loss = 0.93072826\n",
      "Iteration 5, loss = 0.92819904\n",
      "Iteration 6, loss = 0.88741442\n",
      "Iteration 7, loss = 0.71180122\n",
      "Iteration 8, loss = 0.73530915\n",
      "Iteration 9, loss = 0.62987344\n",
      "Iteration 10, loss = 0.76529410\n",
      "Iteration 11, loss = 0.65112547\n",
      "Iteration 12, loss = 0.62296781\n",
      "Iteration 13, loss = 0.53477312\n",
      "Iteration 14, loss = 0.49277006\n",
      "Iteration 15, loss = 0.62468412\n",
      "Iteration 16, loss = 0.58411777\n",
      "Iteration 17, loss = 0.48270498\n",
      "Iteration 18, loss = 0.50175018\n",
      "Iteration 19, loss = 0.53902243\n",
      "Iteration 20, loss = 0.50822606\n",
      "Iteration 21, loss = 0.48431733\n",
      "Iteration 22, loss = 0.62345884\n",
      "Iteration 23, loss = 0.44260344\n",
      "Iteration 24, loss = 0.42512713\n",
      "Iteration 25, loss = 0.44659620\n",
      "Iteration 26, loss = 0.39376448\n",
      "Iteration 27, loss = 0.41075357\n",
      "Iteration 28, loss = 0.43109475\n",
      "Iteration 29, loss = 0.32675744\n",
      "Iteration 30, loss = 0.30637356\n",
      "Iteration 31, loss = 0.33877882\n",
      "Iteration 32, loss = 0.33789489\n",
      "Iteration 33, loss = 0.43355822\n",
      "Iteration 34, loss = 0.38049555\n",
      "Iteration 35, loss = 0.47901210\n",
      "Iteration 36, loss = 0.33929588\n",
      "Iteration 37, loss = 0.40320837\n",
      "Iteration 38, loss = 0.44977837\n",
      "Iteration 39, loss = 0.36940026\n",
      "Iteration 40, loss = 0.31702267\n",
      "Iteration 41, loss = 0.35936029\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 42, loss = 0.30076575\n",
      "Iteration 43, loss = 0.26576991\n",
      "Iteration 44, loss = 0.25227585\n",
      "Iteration 45, loss = 0.25820173\n",
      "Iteration 46, loss = 0.25288715\n",
      "Iteration 47, loss = 0.24029241\n",
      "Iteration 48, loss = 0.23267947\n",
      "Iteration 49, loss = 0.22740810\n",
      "Iteration 50, loss = 0.23222284\n",
      "Iteration 51, loss = 0.22589908\n",
      "Iteration 52, loss = 0.22330164\n",
      "Iteration 53, loss = 0.22310748\n",
      "Iteration 54, loss = 0.22210274\n",
      "Iteration 55, loss = 0.21708198\n",
      "Iteration 56, loss = 0.21511219\n",
      "Iteration 57, loss = 0.20480330\n",
      "Iteration 58, loss = 0.20223666\n",
      "Iteration 59, loss = 0.20182584\n",
      "Iteration 60, loss = 0.22181060\n",
      "Iteration 61, loss = 0.21911702\n",
      "Iteration 62, loss = 0.21340643\n",
      "Iteration 63, loss = 0.20209959\n",
      "Iteration 64, loss = 0.19919956\n",
      "Iteration 65, loss = 0.19874157\n",
      "Iteration 66, loss = 0.19566758\n",
      "Iteration 67, loss = 0.19286014\n",
      "Iteration 68, loss = 0.19137489\n",
      "Iteration 69, loss = 0.18949466\n",
      "Iteration 70, loss = 0.18665876\n",
      "Iteration 71, loss = 0.17989854\n",
      "Iteration 72, loss = 0.17930676\n",
      "Iteration 73, loss = 0.17799452\n",
      "Iteration 74, loss = 0.17873674\n",
      "Iteration 75, loss = 0.18147762\n",
      "Iteration 76, loss = 0.17611954\n",
      "Iteration 77, loss = 0.17638809\n",
      "Iteration 78, loss = 0.17553502\n",
      "Iteration 79, loss = 0.17451535\n",
      "Iteration 80, loss = 0.17358029\n",
      "Iteration 81, loss = 0.17135480\n",
      "Iteration 82, loss = 0.17101858\n",
      "Iteration 83, loss = 0.17066620\n",
      "Iteration 84, loss = 0.17030942\n",
      "Iteration 85, loss = 0.16983652\n",
      "Iteration 86, loss = 0.16939970\n",
      "Iteration 87, loss = 0.16926586\n",
      "Iteration 88, loss = 0.16888436\n",
      "Iteration 89, loss = 0.16859305\n",
      "Iteration 90, loss = 0.16829162\n",
      "Iteration 91, loss = 0.16798911\n",
      "Iteration 92, loss = 0.16775553\n",
      "Iteration 93, loss = 0.16758119\n",
      "Iteration 94, loss = 0.16748350\n",
      "Iteration 95, loss = 0.16859387\n",
      "Iteration 96, loss = 0.17745728\n",
      "Iteration 97, loss = 0.18312371\n",
      "Iteration 98, loss = 0.17705998\n",
      "Iteration 99, loss = 0.18777618\n",
      "Iteration 100, loss = 0.17810216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.66961627\n",
      "Iteration 2, loss = 1.05851441\n",
      "Iteration 3, loss = 0.91438328\n",
      "Iteration 4, loss = 0.73359843\n",
      "Iteration 5, loss = 0.66550455\n",
      "Iteration 6, loss = 0.72177558\n",
      "Iteration 7, loss = 0.52537351\n",
      "Iteration 8, loss = 0.63037633\n",
      "Iteration 9, loss = 0.62941831\n",
      "Iteration 10, loss = 0.57537396\n",
      "Iteration 11, loss = 0.62732901\n",
      "Iteration 12, loss = 0.59590320\n",
      "Iteration 13, loss = 0.51874794\n",
      "Iteration 14, loss = 0.63034040\n",
      "Iteration 15, loss = 0.53474989\n",
      "Iteration 16, loss = 0.38763570\n",
      "Iteration 17, loss = 0.39701823\n",
      "Iteration 18, loss = 0.37717742\n",
      "Iteration 19, loss = 0.46004100\n",
      "Iteration 20, loss = 0.52949407\n",
      "Iteration 21, loss = 0.64921372\n",
      "Iteration 22, loss = 0.40658870\n",
      "Iteration 23, loss = 0.38363770\n",
      "Iteration 24, loss = 0.48260657\n",
      "Iteration 25, loss = 0.45983415\n",
      "Iteration 26, loss = 0.44159877\n",
      "Iteration 27, loss = 0.40540296\n",
      "Iteration 28, loss = 0.41718666\n",
      "Iteration 29, loss = 0.40964748\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 30, loss = 0.29825883\n",
      "Iteration 31, loss = 0.28081066\n",
      "Iteration 32, loss = 0.29833004\n",
      "Iteration 33, loss = 0.27113231\n",
      "Iteration 34, loss = 0.24698341\n",
      "Iteration 35, loss = 0.24235238\n",
      "Iteration 36, loss = 0.23470878\n",
      "Iteration 37, loss = 0.23290731\n",
      "Iteration 38, loss = 0.22851980\n",
      "Iteration 39, loss = 0.23176185\n",
      "Iteration 40, loss = 0.22508593\n",
      "Iteration 41, loss = 0.21687098\n",
      "Iteration 42, loss = 0.23283735\n",
      "Iteration 43, loss = 0.22363010\n",
      "Iteration 44, loss = 0.22244847\n",
      "Iteration 45, loss = 0.22273431\n",
      "Iteration 46, loss = 0.21999891\n",
      "Iteration 47, loss = 0.21938657\n",
      "Iteration 48, loss = 0.21634110\n",
      "Iteration 49, loss = 0.20752094\n",
      "Iteration 50, loss = 0.22921154\n",
      "Iteration 51, loss = 0.21015715\n",
      "Iteration 52, loss = 0.20881182\n",
      "Iteration 53, loss = 0.21446591\n",
      "Iteration 54, loss = 0.21506324\n",
      "Iteration 55, loss = 0.21100124\n",
      "Iteration 56, loss = 0.21178122\n",
      "Iteration 57, loss = 0.21311318\n",
      "Iteration 58, loss = 0.19806290\n",
      "Iteration 59, loss = 0.19483417\n",
      "Iteration 60, loss = 0.19392841\n",
      "Iteration 61, loss = 0.19633079\n",
      "Iteration 62, loss = 0.20543227\n",
      "Iteration 63, loss = 0.20019162\n",
      "Iteration 64, loss = 0.19279342\n",
      "Iteration 65, loss = 0.19195660\n",
      "Iteration 66, loss = 0.19019164\n",
      "Iteration 67, loss = 0.18865302\n",
      "Iteration 68, loss = 0.18959519\n",
      "Iteration 69, loss = 0.18761506\n",
      "Iteration 70, loss = 0.18785395\n",
      "Iteration 71, loss = 0.19302951\n",
      "Iteration 72, loss = 0.19441044\n",
      "Iteration 73, loss = 0.18752732\n",
      "Iteration 74, loss = 0.18300606\n",
      "Iteration 75, loss = 0.18202444\n",
      "Iteration 76, loss = 0.18242280\n",
      "Iteration 77, loss = 0.18515084\n",
      "Iteration 78, loss = 0.17948461\n",
      "Iteration 79, loss = 0.17986848\n",
      "Iteration 80, loss = 0.18061324\n",
      "Iteration 81, loss = 0.17915895\n",
      "Iteration 82, loss = 0.19148869\n",
      "Iteration 83, loss = 0.18631364\n",
      "Iteration 84, loss = 0.18644633\n",
      "Iteration 85, loss = 0.18723410\n",
      "Iteration 86, loss = 0.18281401\n",
      "Iteration 87, loss = 0.18977038\n",
      "Iteration 88, loss = 0.18266543\n",
      "Iteration 89, loss = 0.17553580\n",
      "Iteration 90, loss = 0.17712860\n",
      "Iteration 91, loss = 0.17476040\n",
      "Iteration 92, loss = 0.17415748\n",
      "Iteration 93, loss = 0.17306697\n",
      "Iteration 94, loss = 0.17227214\n",
      "Iteration 95, loss = 0.17195873\n",
      "Iteration 96, loss = 0.17203833\n",
      "Iteration 97, loss = 0.17188505\n",
      "Iteration 98, loss = 0.17160775\n",
      "Iteration 99, loss = 0.17054434\n",
      "Iteration 100, loss = 0.17047768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.04477058\n",
      "Iteration 2, loss = 1.45244496\n",
      "Iteration 3, loss = 1.18332768\n",
      "Iteration 4, loss = 1.06039408\n",
      "Iteration 5, loss = 0.82863049\n",
      "Iteration 6, loss = 0.88146829\n",
      "Iteration 7, loss = 0.83317473\n",
      "Iteration 8, loss = 0.74035335\n",
      "Iteration 9, loss = 0.72991003\n",
      "Iteration 10, loss = 0.68399960\n",
      "Iteration 11, loss = 0.67011995\n",
      "Iteration 12, loss = 0.64232552\n",
      "Iteration 13, loss = 0.72585486\n",
      "Iteration 14, loss = 0.77388466\n",
      "Iteration 15, loss = 0.70012981\n",
      "Iteration 16, loss = 0.57761253\n",
      "Iteration 17, loss = 0.67421853\n",
      "Iteration 18, loss = 0.53603288\n",
      "Iteration 19, loss = 0.53628498\n",
      "Iteration 20, loss = 0.50236212\n",
      "Iteration 21, loss = 0.50143001\n",
      "Iteration 22, loss = 0.50430465\n",
      "Iteration 23, loss = 0.39001887\n",
      "Iteration 24, loss = 0.42174799\n",
      "Iteration 25, loss = 0.36934523\n",
      "Iteration 26, loss = 0.36091487\n",
      "Iteration 27, loss = 0.40949750\n",
      "Iteration 28, loss = 0.37536094\n",
      "Iteration 29, loss = 0.47053159\n",
      "Iteration 30, loss = 0.41147430\n",
      "Iteration 31, loss = 0.37892093\n",
      "Iteration 32, loss = 0.39979024\n",
      "Iteration 33, loss = 0.62301970\n",
      "Iteration 34, loss = 0.59343059\n",
      "Iteration 35, loss = 0.59753960\n",
      "Iteration 36, loss = 0.53173681\n",
      "Iteration 37, loss = 0.41620754\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 38, loss = 0.30822005\n",
      "Iteration 39, loss = 0.28357546\n",
      "Iteration 40, loss = 0.28913355\n",
      "Iteration 41, loss = 0.28854014\n",
      "Iteration 42, loss = 0.27400317\n",
      "Iteration 43, loss = 0.25109531\n",
      "Iteration 44, loss = 0.25150351\n",
      "Iteration 45, loss = 0.23957826\n",
      "Iteration 46, loss = 0.23920711\n",
      "Iteration 47, loss = 0.23689227\n",
      "Iteration 48, loss = 0.22870026\n",
      "Iteration 49, loss = 0.24235563\n",
      "Iteration 50, loss = 0.23084396\n",
      "Iteration 51, loss = 0.22706065\n",
      "Iteration 52, loss = 0.22020989\n",
      "Iteration 53, loss = 0.22440586\n",
      "Iteration 54, loss = 0.23648850\n",
      "Iteration 55, loss = 0.22911502\n",
      "Iteration 56, loss = 0.22016885\n",
      "Iteration 57, loss = 0.22155885\n",
      "Iteration 58, loss = 0.22888871\n",
      "Iteration 59, loss = 0.22996574\n",
      "Iteration 60, loss = 0.22266519\n",
      "Iteration 61, loss = 0.20610009\n",
      "Iteration 62, loss = 0.22433202\n",
      "Iteration 63, loss = 0.20423437\n",
      "Iteration 64, loss = 0.20880149\n",
      "Iteration 65, loss = 0.19539322\n",
      "Iteration 66, loss = 0.19732307\n",
      "Iteration 67, loss = 0.18481175\n",
      "Iteration 68, loss = 0.17906233\n",
      "Iteration 69, loss = 0.17793436\n",
      "Iteration 70, loss = 0.17769012\n",
      "Iteration 71, loss = 0.17588093\n",
      "Iteration 72, loss = 0.17942188\n",
      "Iteration 73, loss = 0.17811357\n",
      "Iteration 74, loss = 0.18208498\n",
      "Iteration 75, loss = 0.17816456\n",
      "Iteration 76, loss = 0.16887075\n",
      "Iteration 77, loss = 0.16867630\n",
      "Iteration 78, loss = 0.17004749\n",
      "Iteration 79, loss = 0.16748809\n",
      "Iteration 80, loss = 0.16760544\n",
      "Iteration 81, loss = 0.17316284\n",
      "Iteration 82, loss = 0.16621917\n",
      "Iteration 83, loss = 0.16103842\n",
      "Iteration 84, loss = 0.16393711\n",
      "Iteration 85, loss = 0.17138014\n",
      "Iteration 86, loss = 0.17208533\n",
      "Iteration 87, loss = 0.16009843\n",
      "Iteration 88, loss = 0.16431539\n",
      "Iteration 89, loss = 0.15940911\n",
      "Iteration 90, loss = 0.15885326\n",
      "Iteration 91, loss = 0.16055581\n",
      "Iteration 92, loss = 0.16792347\n",
      "Iteration 93, loss = 0.15872776\n",
      "Iteration 94, loss = 0.15819971\n",
      "Iteration 95, loss = 0.15770433\n",
      "Iteration 96, loss = 0.15704778\n",
      "Iteration 97, loss = 0.15686874\n",
      "Iteration 98, loss = 0.15665157\n",
      "Iteration 99, loss = 0.15623685\n",
      "Iteration 100, loss = 0.15596908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.56584919\n",
      "Iteration 2, loss = 1.00796280\n",
      "Iteration 3, loss = 0.96681658\n",
      "Iteration 4, loss = 0.70483924\n",
      "Iteration 5, loss = 0.56154899\n",
      "Iteration 6, loss = 0.67825727\n",
      "Iteration 7, loss = 0.65582866\n",
      "Iteration 8, loss = 0.62758609\n",
      "Iteration 9, loss = 0.55995705\n",
      "Iteration 10, loss = 0.45517827\n",
      "Iteration 11, loss = 0.46555390\n",
      "Iteration 12, loss = 0.49949511\n",
      "Iteration 13, loss = 0.51279384\n",
      "Iteration 14, loss = 0.37461513\n",
      "Iteration 15, loss = 0.40161344\n",
      "Iteration 16, loss = 0.44652946\n",
      "Iteration 17, loss = 0.40867325\n",
      "Iteration 18, loss = 0.29299868\n",
      "Iteration 19, loss = 0.42217405\n",
      "Iteration 20, loss = 0.51853672\n",
      "Iteration 21, loss = 0.42323229\n",
      "Iteration 22, loss = 0.48865983\n",
      "Iteration 23, loss = 0.37864475\n",
      "Iteration 24, loss = 0.37465094\n",
      "Iteration 25, loss = 0.36269422\n",
      "Iteration 26, loss = 0.28770492\n",
      "Iteration 27, loss = 0.37069372\n",
      "Iteration 28, loss = 0.30519074\n",
      "Iteration 29, loss = 0.30157866\n",
      "Iteration 30, loss = 0.26945895\n",
      "Iteration 31, loss = 0.34299179\n",
      "Iteration 32, loss = 0.32686928\n",
      "Iteration 33, loss = 0.25629971\n",
      "Iteration 34, loss = 0.25095097\n",
      "Iteration 35, loss = 0.27673901\n",
      "Iteration 36, loss = 0.32621903\n",
      "Iteration 37, loss = 0.31942647\n",
      "Iteration 38, loss = 0.27480107\n",
      "Iteration 39, loss = 0.31716506\n",
      "Iteration 40, loss = 0.33919777\n",
      "Iteration 41, loss = 0.26376780\n",
      "Iteration 42, loss = 0.24056431\n",
      "Iteration 43, loss = 0.21357757\n",
      "Iteration 44, loss = 0.34528435\n",
      "Iteration 45, loss = 0.23562852\n",
      "Iteration 46, loss = 0.30511532\n",
      "Iteration 47, loss = 0.22787078\n",
      "Iteration 48, loss = 0.27919949\n",
      "Iteration 49, loss = 0.25584780\n",
      "Iteration 50, loss = 0.30284564\n",
      "Iteration 51, loss = 0.27060098\n",
      "Iteration 52, loss = 0.31721376\n",
      "Iteration 53, loss = 0.35732359\n",
      "Iteration 54, loss = 0.31464446\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 55, loss = 0.21151041\n",
      "Iteration 56, loss = 0.17831976\n",
      "Iteration 57, loss = 0.16908182\n",
      "Iteration 58, loss = 0.16741115\n",
      "Iteration 59, loss = 0.15160023\n",
      "Iteration 60, loss = 0.16163692\n",
      "Iteration 61, loss = 0.15984173\n",
      "Iteration 62, loss = 0.16732135\n",
      "Iteration 63, loss = 0.15303869\n",
      "Iteration 64, loss = 0.14997651\n",
      "Iteration 65, loss = 0.13868346\n",
      "Iteration 66, loss = 0.13303001\n",
      "Iteration 67, loss = 0.13222264\n",
      "Iteration 68, loss = 0.13239709\n",
      "Iteration 69, loss = 0.13119444\n",
      "Iteration 70, loss = 0.12468922\n",
      "Iteration 71, loss = 0.12447310\n",
      "Iteration 72, loss = 0.12229106\n",
      "Iteration 73, loss = 0.12385525\n",
      "Iteration 74, loss = 0.12250096\n",
      "Iteration 75, loss = 0.11989155\n",
      "Iteration 76, loss = 0.12088988\n",
      "Iteration 77, loss = 0.11957894\n",
      "Iteration 78, loss = 0.11880203\n",
      "Iteration 79, loss = 0.11775272\n",
      "Iteration 80, loss = 0.11725808\n",
      "Iteration 81, loss = 0.11674019\n",
      "Iteration 82, loss = 0.11961841\n",
      "Iteration 83, loss = 0.11882618\n",
      "Iteration 84, loss = 0.11386437\n",
      "Iteration 85, loss = 0.11520500\n",
      "Iteration 86, loss = 0.12096439\n",
      "Iteration 87, loss = 0.11057303\n",
      "Iteration 88, loss = 0.10999366\n",
      "Iteration 89, loss = 0.11308136\n",
      "Iteration 90, loss = 0.11160036\n",
      "Iteration 91, loss = 0.11310141\n",
      "Iteration 92, loss = 0.11261199\n",
      "Iteration 93, loss = 0.11164009\n",
      "Iteration 94, loss = 0.10760173\n",
      "Iteration 95, loss = 0.10728784\n",
      "Iteration 96, loss = 0.10945736\n",
      "Iteration 97, loss = 0.10305290\n",
      "Iteration 98, loss = 0.11100951\n",
      "Iteration 99, loss = 0.10501709\n",
      "Iteration 100, loss = 0.10848854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.90254703\n",
      "Iteration 2, loss = 1.46266687\n",
      "Iteration 3, loss = 1.27960563\n",
      "Iteration 4, loss = 1.15748957\n",
      "Iteration 5, loss = 1.02927100\n",
      "Iteration 6, loss = 1.14180408\n",
      "Iteration 7, loss = 0.98208227\n",
      "Iteration 8, loss = 0.95107134\n",
      "Iteration 9, loss = 0.88369101\n",
      "Iteration 10, loss = 0.88016971\n",
      "Iteration 11, loss = 0.84364768\n",
      "Iteration 12, loss = 0.90072113\n",
      "Iteration 13, loss = 0.99198216\n",
      "Iteration 14, loss = 0.85425682\n",
      "Iteration 15, loss = 0.84549238\n",
      "Iteration 16, loss = 0.88806928\n",
      "Iteration 17, loss = 0.79223441\n",
      "Iteration 18, loss = 0.82690654\n",
      "Iteration 19, loss = 0.95857029\n",
      "Iteration 20, loss = 1.05043153\n",
      "Iteration 21, loss = 0.85071668\n",
      "Iteration 22, loss = 0.78903308\n",
      "Iteration 23, loss = 0.75832561\n",
      "Iteration 24, loss = 0.75187779\n",
      "Iteration 25, loss = 0.79209864\n",
      "Iteration 26, loss = 0.82700803\n",
      "Iteration 27, loss = 0.83506791\n",
      "Iteration 28, loss = 0.76942232\n",
      "Iteration 29, loss = 0.72009065\n",
      "Iteration 30, loss = 0.83936185\n",
      "Iteration 31, loss = 0.74532616\n",
      "Iteration 32, loss = 0.75748388\n",
      "Iteration 33, loss = 0.74765303\n",
      "Iteration 34, loss = 0.71770098\n",
      "Iteration 35, loss = 0.69400065\n",
      "Iteration 36, loss = 0.69151255\n",
      "Iteration 37, loss = 0.70883123\n",
      "Iteration 38, loss = 0.74347160\n",
      "Iteration 39, loss = 0.73360743\n",
      "Iteration 40, loss = 0.76309914\n",
      "Iteration 41, loss = 0.68875716\n",
      "Iteration 42, loss = 0.79958301\n",
      "Iteration 43, loss = 0.69400702\n",
      "Iteration 44, loss = 0.71988293\n",
      "Iteration 45, loss = 0.75421810\n",
      "Iteration 46, loss = 0.65627370\n",
      "Iteration 47, loss = 0.68532262\n",
      "Iteration 48, loss = 0.64104714\n",
      "Iteration 49, loss = 0.70603707\n",
      "Iteration 50, loss = 0.68042410\n",
      "Iteration 51, loss = 0.67253158\n",
      "Iteration 52, loss = 0.75041548\n",
      "Iteration 53, loss = 0.70078184\n",
      "Iteration 54, loss = 0.68403929\n",
      "Iteration 55, loss = 0.76274939\n",
      "Iteration 56, loss = 0.69100417\n",
      "Iteration 57, loss = 0.64911249\n",
      "Iteration 58, loss = 0.70812270\n",
      "Iteration 59, loss = 0.62123828\n",
      "Iteration 60, loss = 0.62785563\n",
      "Iteration 61, loss = 0.65782675\n",
      "Iteration 62, loss = 0.62400004\n",
      "Iteration 63, loss = 0.51140419\n",
      "Iteration 64, loss = 0.68244386\n",
      "Iteration 65, loss = 0.63779338\n",
      "Iteration 66, loss = 0.52899611\n",
      "Iteration 67, loss = 0.59743896\n",
      "Iteration 68, loss = 0.53926957\n",
      "Iteration 69, loss = 0.63054085\n",
      "Iteration 70, loss = 0.58526087\n",
      "Iteration 71, loss = 0.55427930\n",
      "Iteration 72, loss = 0.50409689\n",
      "Iteration 73, loss = 0.52604480\n",
      "Iteration 74, loss = 0.47281353\n",
      "Iteration 75, loss = 0.50629615\n",
      "Iteration 76, loss = 0.51773475\n",
      "Iteration 77, loss = 0.62499943\n",
      "Iteration 78, loss = 0.60327223\n",
      "Iteration 79, loss = 0.50951568\n",
      "Iteration 80, loss = 0.60568541\n",
      "Iteration 81, loss = 0.48430483\n",
      "Iteration 82, loss = 0.51225651\n",
      "Iteration 83, loss = 0.56902925\n",
      "Iteration 84, loss = 0.56450849\n",
      "Iteration 85, loss = 0.60033814\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 86, loss = 0.45863435\n",
      "Iteration 87, loss = 0.38512378\n",
      "Iteration 88, loss = 0.37541772\n",
      "Iteration 89, loss = 0.37352505\n",
      "Iteration 90, loss = 0.37161762\n",
      "Iteration 91, loss = 0.35954146\n",
      "Iteration 92, loss = 0.35244187\n",
      "Iteration 93, loss = 0.34916853\n",
      "Iteration 94, loss = 0.37855438\n",
      "Iteration 95, loss = 0.36699770\n",
      "Iteration 96, loss = 0.36232789\n",
      "Iteration 97, loss = 0.36058867\n",
      "Iteration 98, loss = 0.33958135\n",
      "Iteration 99, loss = 0.34329606\n",
      "Iteration 100, loss = 0.34758681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.79604983\n",
      "Iteration 2, loss = 1.19355400\n",
      "Iteration 3, loss = 0.80574006\n",
      "Iteration 4, loss = 0.75803549\n",
      "Iteration 5, loss = 0.66784501\n",
      "Iteration 6, loss = 0.53560647\n",
      "Iteration 7, loss = 0.66917590\n",
      "Iteration 8, loss = 0.61078714\n",
      "Iteration 9, loss = 0.53425820\n",
      "Iteration 10, loss = 0.54952857\n",
      "Iteration 11, loss = 0.42799668\n",
      "Iteration 12, loss = 0.47112340\n",
      "Iteration 13, loss = 0.46250551\n",
      "Iteration 14, loss = 0.46292096\n",
      "Iteration 15, loss = 0.38708344\n",
      "Iteration 16, loss = 0.47685892\n",
      "Iteration 17, loss = 0.45726568\n",
      "Iteration 18, loss = 0.43985010\n",
      "Iteration 19, loss = 0.36625466\n",
      "Iteration 20, loss = 0.36762352\n",
      "Iteration 21, loss = 0.36722854\n",
      "Iteration 22, loss = 0.39895428\n",
      "Iteration 23, loss = 0.39039855\n",
      "Iteration 24, loss = 0.48259705\n",
      "Iteration 25, loss = 0.41069632\n",
      "Iteration 26, loss = 0.44068303\n",
      "Iteration 27, loss = 0.35843269\n",
      "Iteration 28, loss = 0.30724741\n",
      "Iteration 29, loss = 0.33871727\n",
      "Iteration 30, loss = 0.32617705\n",
      "Iteration 31, loss = 0.33189281\n",
      "Iteration 32, loss = 0.25088152\n",
      "Iteration 33, loss = 0.44118784\n",
      "Iteration 34, loss = 0.34624641\n",
      "Iteration 35, loss = 0.30170188\n",
      "Iteration 36, loss = 0.30164361\n",
      "Iteration 37, loss = 0.30548798\n",
      "Iteration 38, loss = 0.27789440\n",
      "Iteration 39, loss = 0.30352190\n",
      "Iteration 40, loss = 0.29242554\n",
      "Iteration 41, loss = 0.24528242\n",
      "Iteration 42, loss = 0.25432385\n",
      "Iteration 43, loss = 0.38415463\n",
      "Iteration 44, loss = 0.40104369\n",
      "Iteration 45, loss = 0.28032487\n",
      "Iteration 46, loss = 0.29827153\n",
      "Iteration 47, loss = 0.26435774\n",
      "Iteration 48, loss = 0.24896006\n",
      "Iteration 49, loss = 0.22008832\n",
      "Iteration 50, loss = 0.23887009\n",
      "Iteration 51, loss = 0.31196736\n",
      "Iteration 52, loss = 0.24083271\n",
      "Iteration 53, loss = 0.28510699\n",
      "Iteration 54, loss = 0.20538506\n",
      "Iteration 55, loss = 0.24685716\n",
      "Iteration 56, loss = 0.19834331\n",
      "Iteration 57, loss = 0.18128771\n",
      "Iteration 58, loss = 0.27975282\n",
      "Iteration 59, loss = 0.27286319\n",
      "Iteration 60, loss = 0.24850865\n",
      "Iteration 61, loss = 0.26461802\n",
      "Iteration 62, loss = 0.24393795\n",
      "Iteration 63, loss = 0.20221059\n",
      "Iteration 64, loss = 0.21688298\n",
      "Iteration 65, loss = 0.24940029\n",
      "Iteration 66, loss = 0.25901263\n",
      "Iteration 67, loss = 0.23392502\n",
      "Iteration 68, loss = 0.21790101\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 69, loss = 0.19099016\n",
      "Iteration 70, loss = 0.15631904\n",
      "Iteration 71, loss = 0.14140504\n",
      "Iteration 72, loss = 0.14180680\n",
      "Iteration 73, loss = 0.12722427\n",
      "Iteration 74, loss = 0.11912158\n",
      "Iteration 75, loss = 0.12748812\n",
      "Iteration 76, loss = 0.11856714\n",
      "Iteration 77, loss = 0.11616963\n",
      "Iteration 78, loss = 0.11568322\n",
      "Iteration 79, loss = 0.11651063\n",
      "Iteration 80, loss = 0.11679915\n",
      "Iteration 81, loss = 0.11533849\n",
      "Iteration 82, loss = 0.11518591\n",
      "Iteration 83, loss = 0.11406948\n",
      "Iteration 84, loss = 0.11376752\n",
      "Iteration 85, loss = 0.11341238\n",
      "Iteration 86, loss = 0.11313163\n",
      "Iteration 87, loss = 0.11272473\n",
      "Iteration 88, loss = 0.11179585\n",
      "Iteration 89, loss = 0.11506685\n",
      "Iteration 90, loss = 0.11011358\n",
      "Iteration 91, loss = 0.11339028\n",
      "Iteration 92, loss = 0.10890837\n",
      "Iteration 93, loss = 0.10851798\n",
      "Iteration 94, loss = 0.10961992\n",
      "Iteration 95, loss = 0.10669156\n",
      "Iteration 96, loss = 0.10397518\n",
      "Iteration 97, loss = 0.10057351\n",
      "Iteration 98, loss = 0.10052216\n",
      "Iteration 99, loss = 0.10076562\n",
      "Iteration 100, loss = 0.10266797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.68795150\n",
      "Iteration 2, loss = 1.09237382\n",
      "Iteration 3, loss = 0.97330914\n",
      "Iteration 4, loss = 0.94694175\n",
      "Iteration 5, loss = 0.75539324\n",
      "Iteration 6, loss = 0.65671862\n",
      "Iteration 7, loss = 0.62203726\n",
      "Iteration 8, loss = 0.76498209\n",
      "Iteration 9, loss = 0.78990224\n",
      "Iteration 10, loss = 0.78637655\n",
      "Iteration 11, loss = 0.71220015\n",
      "Iteration 12, loss = 0.65383290\n",
      "Iteration 13, loss = 0.60532498\n",
      "Iteration 14, loss = 0.65221181\n",
      "Iteration 15, loss = 0.55896222\n",
      "Iteration 16, loss = 0.55062495\n",
      "Iteration 17, loss = 0.49319956\n",
      "Iteration 18, loss = 0.52676368\n",
      "Iteration 19, loss = 0.42054311\n",
      "Iteration 20, loss = 0.48608765\n",
      "Iteration 21, loss = 0.47672830\n",
      "Iteration 22, loss = 0.49992495\n",
      "Iteration 23, loss = 0.55598929\n",
      "Iteration 24, loss = 0.48738181\n",
      "Iteration 25, loss = 0.48037534\n",
      "Iteration 26, loss = 0.49381010\n",
      "Iteration 27, loss = 0.47158611\n",
      "Iteration 28, loss = 0.47714914\n",
      "Iteration 29, loss = 0.41970795\n",
      "Iteration 30, loss = 0.36374739\n",
      "Iteration 31, loss = 0.41492346\n",
      "Iteration 32, loss = 0.36640647\n",
      "Iteration 33, loss = 0.42363322\n",
      "Iteration 34, loss = 0.33171081\n",
      "Iteration 35, loss = 0.33350905\n",
      "Iteration 36, loss = 0.34954650\n",
      "Iteration 37, loss = 0.43493848\n",
      "Iteration 38, loss = 0.41156188\n",
      "Iteration 39, loss = 0.40612949\n",
      "Iteration 40, loss = 0.37509263\n",
      "Iteration 41, loss = 0.31307593\n",
      "Iteration 42, loss = 0.28402188\n",
      "Iteration 43, loss = 0.43091514\n",
      "Iteration 44, loss = 0.44798308\n",
      "Iteration 45, loss = 0.44566305\n",
      "Iteration 46, loss = 0.48675552\n",
      "Iteration 47, loss = 0.46322690\n",
      "Iteration 48, loss = 0.45634879\n",
      "Iteration 49, loss = 0.45956804\n",
      "Iteration 50, loss = 0.44322889\n",
      "Iteration 51, loss = 0.38456011\n",
      "Iteration 52, loss = 0.34527883\n",
      "Iteration 53, loss = 0.39325280\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 54, loss = 0.29385179\n",
      "Iteration 55, loss = 0.25673595\n",
      "Iteration 56, loss = 0.24329988\n",
      "Iteration 57, loss = 0.23774643\n",
      "Iteration 58, loss = 0.24007466\n",
      "Iteration 59, loss = 0.24245136\n",
      "Iteration 60, loss = 0.22581894\n",
      "Iteration 61, loss = 0.21952274\n",
      "Iteration 62, loss = 0.22076370\n",
      "Iteration 63, loss = 0.21620066\n",
      "Iteration 64, loss = 0.20370833\n",
      "Iteration 65, loss = 0.20603347\n",
      "Iteration 66, loss = 0.20021665\n",
      "Iteration 67, loss = 0.19780808\n",
      "Iteration 68, loss = 0.19846119\n",
      "Iteration 69, loss = 0.19537693\n",
      "Iteration 70, loss = 0.19473918\n",
      "Iteration 71, loss = 0.19399885\n",
      "Iteration 72, loss = 0.19551483\n",
      "Iteration 73, loss = 0.19278149\n",
      "Iteration 74, loss = 0.19301871\n",
      "Iteration 75, loss = 0.19284171\n",
      "Iteration 76, loss = 0.19137526\n",
      "Iteration 77, loss = 0.19069929\n",
      "Iteration 78, loss = 0.19039033\n",
      "Iteration 79, loss = 0.19820517\n",
      "Iteration 80, loss = 0.18962406\n",
      "Iteration 81, loss = 0.18655226\n",
      "Iteration 82, loss = 0.18606288\n",
      "Iteration 83, loss = 0.18558208\n",
      "Iteration 84, loss = 0.18591399\n",
      "Iteration 85, loss = 0.18512021\n",
      "Iteration 86, loss = 0.19068794\n",
      "Iteration 87, loss = 0.19160469\n",
      "Iteration 88, loss = 0.19389807\n",
      "Iteration 89, loss = 0.18228720\n",
      "Iteration 90, loss = 0.18142940\n",
      "Iteration 91, loss = 0.18111708\n",
      "Iteration 92, loss = 0.18129184\n",
      "Iteration 93, loss = 0.18214546\n",
      "Iteration 94, loss = 0.17921768\n",
      "Iteration 95, loss = 0.18291464\n",
      "Iteration 96, loss = 0.18068788\n",
      "Iteration 97, loss = 0.18070653\n",
      "Iteration 98, loss = 0.17896275\n",
      "Iteration 99, loss = 0.17836424\n",
      "Iteration 100, loss = 0.17768067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.80713514\n",
      "Iteration 2, loss = 1.25920303\n",
      "Iteration 3, loss = 1.10607382\n",
      "Iteration 4, loss = 1.10385448\n",
      "Iteration 5, loss = 1.07284402\n",
      "Iteration 6, loss = 1.02766943\n",
      "Iteration 7, loss = 0.95316981\n",
      "Iteration 8, loss = 0.93130031\n",
      "Iteration 9, loss = 0.80926311\n",
      "Iteration 10, loss = 0.73622488\n",
      "Iteration 11, loss = 0.78005058\n",
      "Iteration 12, loss = 0.77554657\n",
      "Iteration 13, loss = 0.62657881\n",
      "Iteration 14, loss = 0.70115716\n",
      "Iteration 15, loss = 0.61072533\n",
      "Iteration 16, loss = 0.61712271\n",
      "Iteration 17, loss = 0.55544963\n",
      "Iteration 18, loss = 0.64679591\n",
      "Iteration 19, loss = 0.58896936\n",
      "Iteration 20, loss = 0.70078569\n",
      "Iteration 21, loss = 0.55921066\n",
      "Iteration 22, loss = 0.51951203\n",
      "Iteration 23, loss = 0.57213353\n",
      "Iteration 24, loss = 0.59186588\n",
      "Iteration 25, loss = 0.41269756\n",
      "Iteration 26, loss = 0.47310057\n",
      "Iteration 27, loss = 0.42930796\n",
      "Iteration 28, loss = 0.36805177\n",
      "Iteration 29, loss = 0.47443538\n",
      "Iteration 30, loss = 0.59165551\n",
      "Iteration 31, loss = 0.39389564\n",
      "Iteration 32, loss = 0.42038140\n",
      "Iteration 33, loss = 0.40819988\n",
      "Iteration 34, loss = 0.41045335\n",
      "Iteration 35, loss = 0.38923972\n",
      "Iteration 36, loss = 0.36821889\n",
      "Iteration 37, loss = 0.33510771\n",
      "Iteration 38, loss = 0.28364148\n",
      "Iteration 39, loss = 0.34876803\n",
      "Iteration 40, loss = 0.31144142\n",
      "Iteration 41, loss = 0.31783199\n",
      "Iteration 42, loss = 0.37965552\n",
      "Iteration 43, loss = 0.35963883\n",
      "Iteration 44, loss = 0.34345058\n",
      "Iteration 45, loss = 0.50704978\n",
      "Iteration 46, loss = 0.47395974\n",
      "Iteration 47, loss = 0.40474866\n",
      "Iteration 48, loss = 0.44574378\n",
      "Iteration 49, loss = 0.43220505\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 50, loss = 0.35137957\n",
      "Iteration 51, loss = 0.31672300\n",
      "Iteration 52, loss = 0.27238955\n",
      "Iteration 53, loss = 0.26839246\n",
      "Iteration 54, loss = 0.23776562\n",
      "Iteration 55, loss = 0.22791469\n",
      "Iteration 56, loss = 0.22889037\n",
      "Iteration 57, loss = 0.23747619\n",
      "Iteration 58, loss = 0.23106851\n",
      "Iteration 59, loss = 0.23986442\n",
      "Iteration 60, loss = 0.23226093\n",
      "Iteration 61, loss = 0.21988044\n",
      "Iteration 62, loss = 0.22603005\n",
      "Iteration 63, loss = 0.22270247\n",
      "Iteration 64, loss = 0.21815477\n",
      "Iteration 65, loss = 0.22087989\n",
      "Iteration 66, loss = 0.21565758\n",
      "Iteration 67, loss = 0.21548690\n",
      "Iteration 68, loss = 0.21007809\n",
      "Iteration 69, loss = 0.20992895\n",
      "Iteration 70, loss = 0.20357346\n",
      "Iteration 71, loss = 0.20423061\n",
      "Iteration 72, loss = 0.20472168\n",
      "Iteration 73, loss = 0.19782152\n",
      "Iteration 74, loss = 0.19305781\n",
      "Iteration 75, loss = 0.19403393\n",
      "Iteration 76, loss = 0.19555623\n",
      "Iteration 77, loss = 0.19548556\n",
      "Iteration 78, loss = 0.19289465\n",
      "Iteration 79, loss = 0.19021359\n",
      "Iteration 80, loss = 0.18939426\n",
      "Iteration 81, loss = 0.18859701\n",
      "Iteration 82, loss = 0.18849895\n",
      "Iteration 83, loss = 0.18758746\n",
      "Iteration 84, loss = 0.18689991\n",
      "Iteration 85, loss = 0.18642513\n",
      "Iteration 86, loss = 0.19055924\n",
      "Iteration 87, loss = 0.18552894\n",
      "Iteration 88, loss = 0.18559338\n",
      "Iteration 89, loss = 0.18521201\n",
      "Iteration 90, loss = 0.18594615\n",
      "Iteration 91, loss = 0.18490354\n",
      "Iteration 92, loss = 0.18415862\n",
      "Iteration 93, loss = 0.18440587\n",
      "Iteration 94, loss = 0.18345007\n",
      "Iteration 95, loss = 0.18115301\n",
      "Iteration 96, loss = 0.17996324\n",
      "Iteration 97, loss = 0.17927126\n",
      "Iteration 98, loss = 0.17859411\n",
      "Iteration 99, loss = 0.17938068\n",
      "Iteration 100, loss = 0.17805439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.02271437\n",
      "Iteration 2, loss = 1.70830105\n",
      "Iteration 3, loss = 1.48160075\n",
      "Iteration 4, loss = 1.45013390\n",
      "Iteration 5, loss = 1.30325777\n",
      "Iteration 6, loss = 1.16609806\n",
      "Iteration 7, loss = 1.09516226\n",
      "Iteration 8, loss = 0.94179455\n",
      "Iteration 9, loss = 0.96076167\n",
      "Iteration 10, loss = 1.06113935\n",
      "Iteration 11, loss = 0.87741398\n",
      "Iteration 12, loss = 0.89802319\n",
      "Iteration 13, loss = 0.92470564\n",
      "Iteration 14, loss = 0.89548956\n",
      "Iteration 15, loss = 0.85431294\n",
      "Iteration 16, loss = 0.76617860\n",
      "Iteration 17, loss = 0.76199188\n",
      "Iteration 18, loss = 0.88677549\n",
      "Iteration 19, loss = 0.75780343\n",
      "Iteration 20, loss = 0.68573565\n",
      "Iteration 21, loss = 0.65448503\n",
      "Iteration 22, loss = 0.66001057\n",
      "Iteration 23, loss = 0.76885365\n",
      "Iteration 24, loss = 0.68012230\n",
      "Iteration 25, loss = 0.64147565\n",
      "Iteration 26, loss = 0.62935557\n",
      "Iteration 27, loss = 0.68530103\n",
      "Iteration 28, loss = 0.78311429\n",
      "Iteration 29, loss = 0.66510236\n",
      "Iteration 30, loss = 0.61716990\n",
      "Iteration 31, loss = 0.78552835\n",
      "Iteration 32, loss = 0.67705817\n",
      "Iteration 33, loss = 0.72182293\n",
      "Iteration 34, loss = 0.72879798\n",
      "Iteration 35, loss = 0.63185499\n",
      "Iteration 36, loss = 0.61390154\n",
      "Iteration 37, loss = 0.58118429\n",
      "Iteration 38, loss = 0.59619859\n",
      "Iteration 39, loss = 0.60916555\n",
      "Iteration 40, loss = 0.64961244\n",
      "Iteration 41, loss = 0.58711435\n",
      "Iteration 42, loss = 0.56857028\n",
      "Iteration 43, loss = 0.53135090\n",
      "Iteration 44, loss = 0.58010938\n",
      "Iteration 45, loss = 0.54941512\n",
      "Iteration 46, loss = 0.54825968\n",
      "Iteration 47, loss = 0.70129183\n",
      "Iteration 48, loss = 0.55595896\n",
      "Iteration 49, loss = 0.49813036\n",
      "Iteration 50, loss = 0.53996595\n",
      "Iteration 51, loss = 0.44933199\n",
      "Iteration 52, loss = 0.51355410\n",
      "Iteration 53, loss = 0.48943098\n",
      "Iteration 54, loss = 0.48047499\n",
      "Iteration 55, loss = 0.46762755\n",
      "Iteration 56, loss = 0.46413416\n",
      "Iteration 57, loss = 0.41727897\n",
      "Iteration 58, loss = 0.44698364\n",
      "Iteration 59, loss = 0.41280942\n",
      "Iteration 60, loss = 0.58935213\n",
      "Iteration 61, loss = 0.60446001\n",
      "Iteration 62, loss = 0.51547772\n",
      "Iteration 63, loss = 0.43120408\n",
      "Iteration 64, loss = 0.43839964\n",
      "Iteration 65, loss = 0.62608679\n",
      "Iteration 66, loss = 0.47386516\n",
      "Iteration 67, loss = 0.42859736\n",
      "Iteration 68, loss = 0.39719777\n",
      "Iteration 69, loss = 0.40594336\n",
      "Iteration 70, loss = 0.49702775\n",
      "Iteration 71, loss = 0.48379202\n",
      "Iteration 72, loss = 0.35091455\n",
      "Iteration 73, loss = 0.32534563\n",
      "Iteration 74, loss = 0.32831738\n",
      "Iteration 75, loss = 0.28517756\n",
      "Iteration 76, loss = 0.34728931\n",
      "Iteration 77, loss = 0.36942143\n",
      "Iteration 78, loss = 0.44023580\n",
      "Iteration 79, loss = 0.39596068\n",
      "Iteration 80, loss = 0.40581307\n",
      "Iteration 81, loss = 0.32109294\n",
      "Iteration 82, loss = 0.39379463\n",
      "Iteration 83, loss = 0.41265974\n",
      "Iteration 84, loss = 0.46063559\n",
      "Iteration 85, loss = 0.29834626\n",
      "Iteration 86, loss = 0.34413450\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 87, loss = 0.24487242\n",
      "Iteration 88, loss = 0.24072326\n",
      "Iteration 89, loss = 0.22813319\n",
      "Iteration 90, loss = 0.23129278\n",
      "Iteration 91, loss = 0.22621545\n",
      "Iteration 92, loss = 0.22345422\n",
      "Iteration 93, loss = 0.23087257\n",
      "Iteration 94, loss = 0.21489183\n",
      "Iteration 95, loss = 0.21424167\n",
      "Iteration 96, loss = 0.21782825\n",
      "Iteration 97, loss = 0.21755759\n",
      "Iteration 98, loss = 0.22478764\n",
      "Iteration 99, loss = 0.21206772\n",
      "Iteration 100, loss = 0.21171124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.68149230\n",
      "Iteration 2, loss = 1.01961446\n",
      "Iteration 3, loss = 0.88746183\n",
      "Iteration 4, loss = 0.68260861\n",
      "Iteration 5, loss = 0.64992917\n",
      "Iteration 6, loss = 0.74839435\n",
      "Iteration 7, loss = 0.61647198\n",
      "Iteration 8, loss = 0.62202702\n",
      "Iteration 9, loss = 0.50483692\n",
      "Iteration 10, loss = 0.56648965\n",
      "Iteration 11, loss = 0.48148988\n",
      "Iteration 12, loss = 0.44329612\n",
      "Iteration 13, loss = 0.56318854\n",
      "Iteration 14, loss = 0.45807046\n",
      "Iteration 15, loss = 0.42577462\n",
      "Iteration 16, loss = 0.51501842\n",
      "Iteration 17, loss = 0.47313559\n",
      "Iteration 18, loss = 0.44775142\n",
      "Iteration 19, loss = 0.46539488\n",
      "Iteration 20, loss = 0.48459579\n",
      "Iteration 21, loss = 0.46193548\n",
      "Iteration 22, loss = 0.41680597\n",
      "Iteration 23, loss = 0.35200958\n",
      "Iteration 24, loss = 0.37360321\n",
      "Iteration 25, loss = 0.37393720\n",
      "Iteration 26, loss = 0.47222580\n",
      "Iteration 27, loss = 0.37638798\n",
      "Iteration 28, loss = 0.40855230\n",
      "Iteration 29, loss = 0.46087209\n",
      "Iteration 30, loss = 0.34796789\n",
      "Iteration 31, loss = 0.45929894\n",
      "Iteration 32, loss = 0.34385737\n",
      "Iteration 33, loss = 0.41455412\n",
      "Iteration 34, loss = 0.37975481\n",
      "Iteration 35, loss = 0.38885645\n",
      "Iteration 36, loss = 0.40254575\n",
      "Iteration 37, loss = 0.35488439\n",
      "Iteration 38, loss = 0.43134493\n",
      "Iteration 39, loss = 0.50836335\n",
      "Iteration 40, loss = 0.45703860\n",
      "Iteration 41, loss = 0.35422709\n",
      "Iteration 42, loss = 0.27426240\n",
      "Iteration 43, loss = 0.28012954\n",
      "Iteration 44, loss = 0.37644468\n",
      "Iteration 45, loss = 0.32461611\n",
      "Iteration 46, loss = 0.31326560\n",
      "Iteration 47, loss = 0.32677140\n",
      "Iteration 48, loss = 0.27028007\n",
      "Iteration 49, loss = 0.27367191\n",
      "Iteration 50, loss = 0.26685933\n",
      "Iteration 51, loss = 0.22725717\n",
      "Iteration 52, loss = 0.35227748\n",
      "Iteration 53, loss = 0.48495735\n",
      "Iteration 54, loss = 0.47103939\n",
      "Iteration 55, loss = 0.32123158\n",
      "Iteration 56, loss = 0.31458149\n",
      "Iteration 57, loss = 0.28227083\n",
      "Iteration 58, loss = 0.21869117\n",
      "Iteration 59, loss = 0.32480101\n",
      "Iteration 60, loss = 0.29237191\n",
      "Iteration 61, loss = 0.31021165\n",
      "Iteration 62, loss = 0.24139992\n",
      "Iteration 63, loss = 0.21913745\n",
      "Iteration 64, loss = 0.17498558\n",
      "Iteration 65, loss = 0.21927855\n",
      "Iteration 66, loss = 0.27882574\n",
      "Iteration 67, loss = 0.27825504\n",
      "Iteration 68, loss = 0.21075518\n",
      "Iteration 69, loss = 0.20576651\n",
      "Iteration 70, loss = 0.26866483\n",
      "Iteration 71, loss = 0.33015260\n",
      "Iteration 72, loss = 0.33321201\n",
      "Iteration 73, loss = 0.20158339\n",
      "Iteration 74, loss = 0.20024927\n",
      "Iteration 75, loss = 0.23244744\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 76, loss = 0.16438305\n",
      "Iteration 77, loss = 0.15873174\n",
      "Iteration 78, loss = 0.15549379\n",
      "Iteration 79, loss = 0.15603034\n",
      "Iteration 80, loss = 0.15148197\n",
      "Iteration 81, loss = 0.15086540\n",
      "Iteration 82, loss = 0.15035774\n",
      "Iteration 83, loss = 0.14955334\n",
      "Iteration 84, loss = 0.14898297\n",
      "Iteration 85, loss = 0.14807959\n",
      "Iteration 86, loss = 0.14943553\n",
      "Iteration 87, loss = 0.14514532\n",
      "Iteration 88, loss = 0.14457479\n",
      "Iteration 89, loss = 0.14392987\n",
      "Iteration 90, loss = 0.14350545\n",
      "Iteration 91, loss = 0.14309168\n",
      "Iteration 92, loss = 0.14272495\n",
      "Iteration 93, loss = 0.14216196\n",
      "Iteration 94, loss = 0.14495719\n",
      "Iteration 95, loss = 0.14312114\n",
      "Iteration 96, loss = 0.14102518\n",
      "Iteration 97, loss = 0.13910779\n",
      "Iteration 98, loss = 0.13762891\n",
      "Iteration 99, loss = 0.13796068\n",
      "Iteration 100, loss = 0.13667368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.59654519\n",
      "Iteration 2, loss = 0.95003041\n",
      "Iteration 3, loss = 0.67354218\n",
      "Iteration 4, loss = 0.55547482\n",
      "Iteration 5, loss = 0.56395421\n",
      "Iteration 6, loss = 0.56334151\n",
      "Iteration 7, loss = 0.56049426\n",
      "Iteration 8, loss = 0.46019956\n",
      "Iteration 9, loss = 0.38191134\n",
      "Iteration 10, loss = 0.45857350\n",
      "Iteration 11, loss = 0.41710214\n",
      "Iteration 12, loss = 0.66244961\n",
      "Iteration 13, loss = 0.64162963\n",
      "Iteration 14, loss = 0.55711268\n",
      "Iteration 15, loss = 0.45127739\n",
      "Iteration 16, loss = 0.34111323\n",
      "Iteration 17, loss = 0.36890269\n",
      "Iteration 18, loss = 0.33252108\n",
      "Iteration 19, loss = 0.27836720\n",
      "Iteration 20, loss = 0.42142591\n",
      "Iteration 21, loss = 0.34335575\n",
      "Iteration 22, loss = 0.30882836\n",
      "Iteration 23, loss = 0.31447036\n",
      "Iteration 24, loss = 0.29283798\n",
      "Iteration 25, loss = 0.29168196\n",
      "Iteration 26, loss = 0.30538900\n",
      "Iteration 27, loss = 0.22482227\n",
      "Iteration 28, loss = 0.28641508\n",
      "Iteration 29, loss = 0.26264937\n",
      "Iteration 30, loss = 0.25596975\n",
      "Iteration 31, loss = 0.28418000\n",
      "Iteration 32, loss = 0.55150403\n",
      "Iteration 33, loss = 0.48505881\n",
      "Iteration 34, loss = 0.33515867\n",
      "Iteration 35, loss = 0.34753382\n",
      "Iteration 36, loss = 0.29498032\n",
      "Iteration 37, loss = 0.23729655\n",
      "Iteration 38, loss = 0.26736592\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 39, loss = 0.19081847\n",
      "Iteration 40, loss = 0.15854042\n",
      "Iteration 41, loss = 0.14339677\n",
      "Iteration 42, loss = 0.13387621\n",
      "Iteration 43, loss = 0.12057028\n",
      "Iteration 44, loss = 0.11443511\n",
      "Iteration 45, loss = 0.10594565\n",
      "Iteration 46, loss = 0.10348889\n",
      "Iteration 47, loss = 0.10035769\n",
      "Iteration 48, loss = 0.09879319\n",
      "Iteration 49, loss = 0.09558871\n",
      "Iteration 50, loss = 0.09229151\n",
      "Iteration 51, loss = 0.08800659\n",
      "Iteration 52, loss = 0.08718379\n",
      "Iteration 53, loss = 0.08526284\n",
      "Iteration 54, loss = 0.08281568\n",
      "Iteration 55, loss = 0.07958801\n",
      "Iteration 56, loss = 0.07959492\n",
      "Iteration 57, loss = 0.07869141\n",
      "Iteration 58, loss = 0.07955986\n",
      "Iteration 59, loss = 0.07963668\n",
      "Iteration 60, loss = 0.07505853\n",
      "Iteration 61, loss = 0.07840056\n",
      "Iteration 62, loss = 0.08139664\n",
      "Iteration 63, loss = 0.07359836\n",
      "Iteration 64, loss = 0.07416569\n",
      "Iteration 65, loss = 0.07234840\n",
      "Iteration 66, loss = 0.07193581\n",
      "Iteration 67, loss = 0.07095366\n",
      "Iteration 68, loss = 0.07033638\n",
      "Iteration 69, loss = 0.06977217\n",
      "Iteration 70, loss = 0.06955222\n",
      "Iteration 71, loss = 0.06921474\n",
      "Iteration 72, loss = 0.06882539\n",
      "Iteration 73, loss = 0.06855051\n",
      "Iteration 74, loss = 0.06826802\n",
      "Iteration 75, loss = 0.06791080\n",
      "Iteration 76, loss = 0.06771162\n",
      "Iteration 77, loss = 0.06723350\n",
      "Iteration 78, loss = 0.06713967\n",
      "Iteration 79, loss = 0.06660291\n",
      "Iteration 80, loss = 0.06596556\n",
      "Iteration 81, loss = 0.06583403\n",
      "Iteration 82, loss = 0.06536109\n",
      "Iteration 83, loss = 0.06492167\n",
      "Iteration 84, loss = 0.06473650\n",
      "Iteration 85, loss = 0.06439160\n",
      "Iteration 86, loss = 0.06426730\n",
      "Iteration 87, loss = 0.06383838\n",
      "Iteration 88, loss = 0.06368331\n",
      "Iteration 89, loss = 0.06338555\n",
      "Iteration 90, loss = 0.06315533\n",
      "Iteration 91, loss = 0.06302795\n",
      "Iteration 92, loss = 0.06279679\n",
      "Iteration 93, loss = 0.06263208\n",
      "Iteration 94, loss = 0.06239127\n",
      "Iteration 95, loss = 0.06225550\n",
      "Iteration 96, loss = 0.06209199\n",
      "Iteration 97, loss = 0.06197551\n",
      "Iteration 98, loss = 0.06176661\n",
      "Iteration 99, loss = 0.06157388\n",
      "Iteration 100, loss = 0.06137117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.78172031\n",
      "Iteration 2, loss = 1.03984492\n",
      "Iteration 3, loss = 0.73215049\n",
      "Iteration 4, loss = 0.72965465\n",
      "Iteration 5, loss = 0.67949058\n",
      "Iteration 6, loss = 0.49117869\n",
      "Iteration 7, loss = 0.54521825\n",
      "Iteration 8, loss = 0.48260732\n",
      "Iteration 9, loss = 0.50151811\n",
      "Iteration 10, loss = 0.33035245\n",
      "Iteration 11, loss = 0.36092872\n",
      "Iteration 12, loss = 0.43122161\n",
      "Iteration 13, loss = 0.44798630\n",
      "Iteration 14, loss = 0.39410251\n",
      "Iteration 15, loss = 0.43267983\n",
      "Iteration 16, loss = 0.33538922\n",
      "Iteration 17, loss = 0.24742675\n",
      "Iteration 18, loss = 0.28495967\n",
      "Iteration 19, loss = 0.32218306\n",
      "Iteration 20, loss = 0.37116480\n",
      "Iteration 21, loss = 0.39941815\n",
      "Iteration 22, loss = 0.38542401\n",
      "Iteration 23, loss = 0.34144460\n",
      "Iteration 24, loss = 0.41152905\n",
      "Iteration 25, loss = 0.27635013\n",
      "Iteration 26, loss = 0.30348540\n",
      "Iteration 27, loss = 0.32879112\n",
      "Iteration 28, loss = 0.34498201\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 29, loss = 0.30248737\n",
      "Iteration 30, loss = 0.23664178\n",
      "Iteration 31, loss = 0.20406655\n",
      "Iteration 32, loss = 0.17077977\n",
      "Iteration 33, loss = 0.16170983\n",
      "Iteration 34, loss = 0.16099362\n",
      "Iteration 35, loss = 0.15336534\n",
      "Iteration 36, loss = 0.15342932\n",
      "Iteration 37, loss = 0.14441171\n",
      "Iteration 38, loss = 0.13321919\n",
      "Iteration 39, loss = 0.12791480\n",
      "Iteration 40, loss = 0.13389470\n",
      "Iteration 41, loss = 0.12736505\n",
      "Iteration 42, loss = 0.12070198\n",
      "Iteration 43, loss = 0.11970752\n",
      "Iteration 44, loss = 0.11406778\n",
      "Iteration 45, loss = 0.10890521\n",
      "Iteration 46, loss = 0.11407012\n",
      "Iteration 47, loss = 0.11427535\n",
      "Iteration 48, loss = 0.10711789\n",
      "Iteration 49, loss = 0.10874668\n",
      "Iteration 50, loss = 0.10350793\n",
      "Iteration 51, loss = 0.10170295\n",
      "Iteration 52, loss = 0.10001854\n",
      "Iteration 53, loss = 0.09756004\n",
      "Iteration 54, loss = 0.09692940\n",
      "Iteration 55, loss = 0.09417186\n",
      "Iteration 56, loss = 0.09509710\n",
      "Iteration 57, loss = 0.09710386\n",
      "Iteration 58, loss = 0.09721871\n",
      "Iteration 59, loss = 0.09172965\n",
      "Iteration 60, loss = 0.09022996\n",
      "Iteration 61, loss = 0.09561254\n",
      "Iteration 62, loss = 0.09025623\n",
      "Iteration 63, loss = 0.08937660\n",
      "Iteration 64, loss = 0.08661657\n",
      "Iteration 65, loss = 0.08702559\n",
      "Iteration 66, loss = 0.08532904\n",
      "Iteration 67, loss = 0.08483189\n",
      "Iteration 68, loss = 0.08620995\n",
      "Iteration 69, loss = 0.08460646\n",
      "Iteration 70, loss = 0.08461005\n",
      "Iteration 71, loss = 0.09734795\n",
      "Iteration 72, loss = 0.08410276\n",
      "Iteration 73, loss = 0.08276403\n",
      "Iteration 74, loss = 0.08241119\n",
      "Iteration 75, loss = 0.08190592\n",
      "Iteration 76, loss = 0.08147832\n",
      "Iteration 77, loss = 0.08111388\n",
      "Iteration 78, loss = 0.08077964\n",
      "Iteration 79, loss = 0.08041953\n",
      "Iteration 80, loss = 0.07978475\n",
      "Iteration 81, loss = 0.08075170\n",
      "Iteration 82, loss = 0.07883829\n",
      "Iteration 83, loss = 0.07850122\n",
      "Iteration 84, loss = 0.07819886\n",
      "Iteration 85, loss = 0.07782771\n",
      "Iteration 86, loss = 0.07755529\n",
      "Iteration 87, loss = 0.07725134\n",
      "Iteration 88, loss = 0.07698464\n",
      "Iteration 89, loss = 0.07671547\n",
      "Iteration 90, loss = 0.07638675\n",
      "Iteration 91, loss = 0.07617433\n",
      "Iteration 92, loss = 0.07588126\n",
      "Iteration 93, loss = 0.07561804\n",
      "Iteration 94, loss = 0.07535633\n",
      "Iteration 95, loss = 0.07490890\n",
      "Iteration 96, loss = 0.07450906\n",
      "Iteration 97, loss = 0.07419918\n",
      "Iteration 98, loss = 0.07399499\n",
      "Iteration 99, loss = 0.07368757\n",
      "Iteration 100, loss = 0.07349240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.03230242\n",
      "Iteration 2, loss = 1.47934691\n",
      "Iteration 3, loss = 1.07685409\n",
      "Iteration 4, loss = 0.93953912\n",
      "Iteration 5, loss = 0.92524552\n",
      "Iteration 6, loss = 0.74596117\n",
      "Iteration 7, loss = 0.95300834\n",
      "Iteration 8, loss = 0.98231427\n",
      "Iteration 9, loss = 0.81223257\n",
      "Iteration 10, loss = 0.59437749\n",
      "Iteration 11, loss = 0.57054923\n",
      "Iteration 12, loss = 0.68501837\n",
      "Iteration 13, loss = 0.60771824\n",
      "Iteration 14, loss = 0.58815060\n",
      "Iteration 15, loss = 0.52553349\n",
      "Iteration 16, loss = 0.65103759\n",
      "Iteration 17, loss = 0.58468548\n",
      "Iteration 18, loss = 0.49073211\n",
      "Iteration 19, loss = 0.52905065\n",
      "Iteration 20, loss = 0.51596848\n",
      "Iteration 21, loss = 0.47328926\n",
      "Iteration 22, loss = 0.41874396\n",
      "Iteration 23, loss = 0.35282694\n",
      "Iteration 24, loss = 0.43949763\n",
      "Iteration 25, loss = 0.33407635\n",
      "Iteration 26, loss = 0.38451336\n",
      "Iteration 27, loss = 0.38027798\n",
      "Iteration 28, loss = 0.28802505\n",
      "Iteration 29, loss = 0.36659539\n",
      "Iteration 30, loss = 0.33684566\n",
      "Iteration 31, loss = 0.25705528\n",
      "Iteration 32, loss = 0.22855869\n",
      "Iteration 33, loss = 0.28828916\n",
      "Iteration 34, loss = 0.25159003\n",
      "Iteration 35, loss = 0.37270308\n",
      "Iteration 36, loss = 0.32616026\n",
      "Iteration 37, loss = 0.34927846\n",
      "Iteration 38, loss = 0.27754217\n",
      "Iteration 39, loss = 0.35252742\n",
      "Iteration 40, loss = 0.26377339\n",
      "Iteration 41, loss = 0.24692480\n",
      "Iteration 42, loss = 0.26293400\n",
      "Iteration 43, loss = 0.19997867\n",
      "Iteration 44, loss = 0.18182340\n",
      "Iteration 45, loss = 0.21140613\n",
      "Iteration 46, loss = 0.16623018\n",
      "Iteration 47, loss = 0.14622924\n",
      "Iteration 48, loss = 0.19469753\n",
      "Iteration 49, loss = 0.22244233\n",
      "Iteration 50, loss = 0.25197093\n",
      "Iteration 51, loss = 0.18500588\n",
      "Iteration 52, loss = 0.21910116\n",
      "Iteration 53, loss = 0.24445625\n",
      "Iteration 54, loss = 0.16781990\n",
      "Iteration 55, loss = 0.20059169\n",
      "Iteration 56, loss = 0.16082877\n",
      "Iteration 57, loss = 0.22560968\n",
      "Iteration 58, loss = 0.18482091\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 59, loss = 0.22541952\n",
      "Iteration 60, loss = 0.13073422\n",
      "Iteration 61, loss = 0.11806681\n",
      "Iteration 62, loss = 0.11419649\n",
      "Iteration 63, loss = 0.14440005\n",
      "Iteration 64, loss = 0.11714767\n",
      "Iteration 65, loss = 0.10491232\n",
      "Iteration 66, loss = 0.10216023\n",
      "Iteration 67, loss = 0.10111313\n",
      "Iteration 68, loss = 0.09788317\n",
      "Iteration 69, loss = 0.09636409\n",
      "Iteration 70, loss = 0.09552577\n",
      "Iteration 71, loss = 0.09498560\n",
      "Iteration 72, loss = 0.09455708\n",
      "Iteration 73, loss = 0.09405671\n",
      "Iteration 74, loss = 0.09358059\n",
      "Iteration 75, loss = 0.09051851\n",
      "Iteration 76, loss = 0.08948276\n",
      "Iteration 77, loss = 0.10017290\n",
      "Iteration 78, loss = 0.09208845\n",
      "Iteration 79, loss = 0.09326470\n",
      "Iteration 80, loss = 0.08869447\n",
      "Iteration 81, loss = 0.09025247\n",
      "Iteration 82, loss = 0.08458071\n",
      "Iteration 83, loss = 0.08214355\n",
      "Iteration 84, loss = 0.08175186\n",
      "Iteration 85, loss = 0.08129314\n",
      "Iteration 86, loss = 0.08064045\n",
      "Iteration 87, loss = 0.08203808\n",
      "Iteration 88, loss = 0.08997189\n",
      "Iteration 89, loss = 0.08026552\n",
      "Iteration 90, loss = 0.07870803\n",
      "Iteration 91, loss = 0.07839404\n",
      "Iteration 92, loss = 0.07805202\n",
      "Iteration 93, loss = 0.07776388\n",
      "Iteration 94, loss = 0.07745512\n",
      "Iteration 95, loss = 0.07720229\n",
      "Iteration 96, loss = 0.07689734\n",
      "Iteration 97, loss = 0.07663749\n",
      "Iteration 98, loss = 0.07642993\n",
      "Iteration 99, loss = 0.07620926\n",
      "Iteration 100, loss = 0.07596910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.59107096\n",
      "Iteration 2, loss = 0.94509397\n",
      "Iteration 3, loss = 0.73779843\n",
      "Iteration 4, loss = 0.74761856\n",
      "Iteration 5, loss = 0.60848536\n",
      "Iteration 6, loss = 0.58534431\n",
      "Iteration 7, loss = 0.43301902\n",
      "Iteration 8, loss = 0.54996230\n",
      "Iteration 9, loss = 0.53821302\n",
      "Iteration 10, loss = 0.48224390\n",
      "Iteration 11, loss = 0.61426538\n",
      "Iteration 12, loss = 0.47360105\n",
      "Iteration 13, loss = 0.43917857\n",
      "Iteration 14, loss = 0.46075411\n",
      "Iteration 15, loss = 0.44099683\n",
      "Iteration 16, loss = 0.42346211\n",
      "Iteration 17, loss = 0.43806625\n",
      "Iteration 18, loss = 0.46168983\n",
      "Iteration 19, loss = 0.35597461\n",
      "Iteration 20, loss = 0.29610666\n",
      "Iteration 21, loss = 0.44345271\n",
      "Iteration 22, loss = 0.50046810\n",
      "Iteration 23, loss = 0.42670467\n",
      "Iteration 24, loss = 0.37744634\n",
      "Iteration 25, loss = 0.37487435\n",
      "Iteration 26, loss = 0.38420138\n",
      "Iteration 27, loss = 0.30326612\n",
      "Iteration 28, loss = 0.31774784\n",
      "Iteration 29, loss = 0.45468053\n",
      "Iteration 30, loss = 0.37083369\n",
      "Iteration 31, loss = 0.37211041\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 32, loss = 0.30517494\n",
      "Iteration 33, loss = 0.23511977\n",
      "Iteration 34, loss = 0.21211610\n",
      "Iteration 35, loss = 0.21150933\n",
      "Iteration 36, loss = 0.21165012\n",
      "Iteration 37, loss = 0.20562179\n",
      "Iteration 38, loss = 0.20112487\n",
      "Iteration 39, loss = 0.19543177\n",
      "Iteration 40, loss = 0.19404435\n",
      "Iteration 41, loss = 0.18289152\n",
      "Iteration 42, loss = 0.17536564\n",
      "Iteration 43, loss = 0.17494355\n",
      "Iteration 44, loss = 0.17160207\n",
      "Iteration 45, loss = 0.16737059\n",
      "Iteration 46, loss = 0.17377863\n",
      "Iteration 47, loss = 0.16123948\n",
      "Iteration 48, loss = 0.15558572\n",
      "Iteration 49, loss = 0.15491603\n",
      "Iteration 50, loss = 0.14970225\n",
      "Iteration 51, loss = 0.14814571\n",
      "Iteration 52, loss = 0.14812063\n",
      "Iteration 53, loss = 0.14831216\n",
      "Iteration 54, loss = 0.14529040\n",
      "Iteration 55, loss = 0.14352123\n",
      "Iteration 56, loss = 0.14873865\n",
      "Iteration 57, loss = 0.14813233\n",
      "Iteration 58, loss = 0.14185470\n",
      "Iteration 59, loss = 0.14020730\n",
      "Iteration 60, loss = 0.14245728\n",
      "Iteration 61, loss = 0.14459486\n",
      "Iteration 62, loss = 0.13807644\n",
      "Iteration 63, loss = 0.13965569\n",
      "Iteration 64, loss = 0.13657045\n",
      "Iteration 65, loss = 0.13843059\n",
      "Iteration 66, loss = 0.13628375\n",
      "Iteration 67, loss = 0.13836889\n",
      "Iteration 68, loss = 0.14106687\n",
      "Iteration 69, loss = 0.13710069\n",
      "Iteration 70, loss = 0.13534250\n",
      "Iteration 71, loss = 0.13312615\n",
      "Iteration 72, loss = 0.13274124\n",
      "Iteration 73, loss = 0.13208602\n",
      "Iteration 74, loss = 0.13213591\n",
      "Iteration 75, loss = 0.13096026\n",
      "Iteration 76, loss = 0.13171207\n",
      "Iteration 77, loss = 0.13072421\n",
      "Iteration 78, loss = 0.13028021\n",
      "Iteration 79, loss = 0.12966076\n",
      "Iteration 80, loss = 0.12936276\n",
      "Iteration 81, loss = 0.12995330\n",
      "Iteration 82, loss = 0.13154002\n",
      "Iteration 83, loss = 0.13409579\n",
      "Iteration 84, loss = 0.12835430\n",
      "Iteration 85, loss = 0.12833182\n",
      "Iteration 86, loss = 0.12808677\n",
      "Iteration 87, loss = 0.12852755\n",
      "Iteration 88, loss = 0.13595638\n",
      "Iteration 89, loss = 0.12527626\n",
      "Iteration 90, loss = 0.12970280\n",
      "Iteration 91, loss = 0.12625014\n",
      "Iteration 92, loss = 0.12182319\n",
      "Iteration 93, loss = 0.12142307\n",
      "Iteration 94, loss = 0.12120204\n",
      "Iteration 95, loss = 0.12117549\n",
      "Iteration 96, loss = 0.12118180\n",
      "Iteration 97, loss = 0.12009554\n",
      "Iteration 98, loss = 0.11951344\n",
      "Iteration 99, loss = 0.11867043\n",
      "Iteration 100, loss = 0.12024299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.91343270\n",
      "Iteration 2, loss = 1.39906317\n",
      "Iteration 3, loss = 1.16847600\n",
      "Iteration 4, loss = 1.24330856\n",
      "Iteration 5, loss = 0.90250216\n",
      "Iteration 6, loss = 0.80207705\n",
      "Iteration 7, loss = 0.85714681\n",
      "Iteration 8, loss = 0.85856949\n",
      "Iteration 9, loss = 0.64121687\n",
      "Iteration 10, loss = 0.64980633\n",
      "Iteration 11, loss = 0.72926902\n",
      "Iteration 12, loss = 0.60217692\n",
      "Iteration 13, loss = 0.69729622\n",
      "Iteration 14, loss = 0.62789944\n",
      "Iteration 15, loss = 0.47503287\n",
      "Iteration 16, loss = 0.47249419\n",
      "Iteration 17, loss = 0.46759214\n",
      "Iteration 18, loss = 0.42527203\n",
      "Iteration 19, loss = 0.51915538\n",
      "Iteration 20, loss = 0.45879947\n",
      "Iteration 21, loss = 0.49120989\n",
      "Iteration 22, loss = 0.56205093\n",
      "Iteration 23, loss = 0.50110650\n",
      "Iteration 24, loss = 0.49673593\n",
      "Iteration 25, loss = 0.50559122\n",
      "Iteration 26, loss = 0.39593213\n",
      "Iteration 27, loss = 0.43902871\n",
      "Iteration 28, loss = 0.39934044\n",
      "Iteration 29, loss = 0.42109513\n",
      "Iteration 30, loss = 0.53691778\n",
      "Iteration 31, loss = 0.44996930\n",
      "Iteration 32, loss = 0.36175482\n",
      "Iteration 33, loss = 0.36273033\n",
      "Iteration 34, loss = 0.31147549\n",
      "Iteration 35, loss = 0.35554374\n",
      "Iteration 36, loss = 0.30949102\n",
      "Iteration 37, loss = 0.28486085\n",
      "Iteration 38, loss = 0.36337644\n",
      "Iteration 39, loss = 0.30992253\n",
      "Iteration 40, loss = 0.23684041\n",
      "Iteration 41, loss = 0.25153882\n",
      "Iteration 42, loss = 0.23245770\n",
      "Iteration 43, loss = 0.31502866\n",
      "Iteration 44, loss = 0.27902404\n",
      "Iteration 45, loss = 0.51900616\n",
      "Iteration 46, loss = 0.38482452\n",
      "Iteration 47, loss = 0.36105442\n",
      "Iteration 48, loss = 0.37724508\n",
      "Iteration 49, loss = 0.45671213\n",
      "Iteration 50, loss = 0.41296814\n",
      "Iteration 51, loss = 0.36767983\n",
      "Iteration 52, loss = 0.26694395\n",
      "Iteration 53, loss = 0.27436842\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 54, loss = 0.43179013\n",
      "Iteration 55, loss = 0.34225135\n",
      "Iteration 56, loss = 0.29667022\n",
      "Iteration 57, loss = 0.25294065\n",
      "Iteration 58, loss = 0.22426210\n",
      "Iteration 59, loss = 0.22027622\n",
      "Iteration 60, loss = 0.21542443\n",
      "Iteration 61, loss = 0.18931030\n",
      "Iteration 62, loss = 0.18360041\n",
      "Iteration 63, loss = 0.18011347\n",
      "Iteration 64, loss = 0.17637132\n",
      "Iteration 65, loss = 0.17255869\n",
      "Iteration 66, loss = 0.17506480\n",
      "Iteration 67, loss = 0.17057773\n",
      "Iteration 68, loss = 0.16703092\n",
      "Iteration 69, loss = 0.16643311\n",
      "Iteration 70, loss = 0.16803765\n",
      "Iteration 71, loss = 0.16850961\n",
      "Iteration 72, loss = 0.16336269\n",
      "Iteration 73, loss = 0.16256256\n",
      "Iteration 74, loss = 0.15951887\n",
      "Iteration 75, loss = 0.15958418\n",
      "Iteration 76, loss = 0.15692414\n",
      "Iteration 77, loss = 0.15743142\n",
      "Iteration 78, loss = 0.15582106\n",
      "Iteration 79, loss = 0.15345427\n",
      "Iteration 80, loss = 0.15648308\n",
      "Iteration 81, loss = 0.14898272\n",
      "Iteration 82, loss = 0.14902126\n",
      "Iteration 83, loss = 0.14906104\n",
      "Iteration 84, loss = 0.14687664\n",
      "Iteration 85, loss = 0.14770937\n",
      "Iteration 86, loss = 0.14388994\n",
      "Iteration 87, loss = 0.14461623\n",
      "Iteration 88, loss = 0.14630849\n",
      "Iteration 89, loss = 0.14390692\n",
      "Iteration 90, loss = 0.14163969\n",
      "Iteration 91, loss = 0.14074504\n",
      "Iteration 92, loss = 0.14006442\n",
      "Iteration 93, loss = 0.13814382\n",
      "Iteration 94, loss = 0.13722736\n",
      "Iteration 95, loss = 0.13692910\n",
      "Iteration 96, loss = 0.13660414\n",
      "Iteration 97, loss = 0.13597946\n",
      "Iteration 98, loss = 0.13582077\n",
      "Iteration 99, loss = 0.13840597\n",
      "Iteration 100, loss = 0.13673411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.53341036\n",
      "Iteration 2, loss = 0.86092532\n",
      "Iteration 3, loss = 0.73967886\n",
      "Iteration 4, loss = 0.56827371\n",
      "Iteration 5, loss = 0.46337759\n",
      "Iteration 6, loss = 0.41385913\n",
      "Iteration 7, loss = 0.39253292\n",
      "Iteration 8, loss = 0.41314685\n",
      "Iteration 9, loss = 0.38848578\n",
      "Iteration 10, loss = 0.28395527\n",
      "Iteration 11, loss = 0.30553783\n",
      "Iteration 12, loss = 0.33682913\n",
      "Iteration 13, loss = 0.33313514\n",
      "Iteration 14, loss = 0.52132917\n",
      "Iteration 15, loss = 0.36196153\n",
      "Iteration 16, loss = 0.40601379\n",
      "Iteration 17, loss = 0.43834240\n",
      "Iteration 18, loss = 0.45140651\n",
      "Iteration 19, loss = 0.44393565\n",
      "Iteration 20, loss = 0.36297898\n",
      "Iteration 21, loss = 0.30531025\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 22, loss = 0.21942988\n",
      "Iteration 23, loss = 0.17169430\n",
      "Iteration 24, loss = 0.15844495\n",
      "Iteration 25, loss = 0.14934921\n",
      "Iteration 26, loss = 0.14503285\n",
      "Iteration 27, loss = 0.13969758\n",
      "Iteration 28, loss = 0.13045214\n",
      "Iteration 29, loss = 0.12251099\n",
      "Iteration 30, loss = 0.11602837\n",
      "Iteration 31, loss = 0.11414598\n",
      "Iteration 32, loss = 0.11450047\n",
      "Iteration 33, loss = 0.11176547\n",
      "Iteration 34, loss = 0.11006645\n",
      "Iteration 35, loss = 0.10717714\n",
      "Iteration 36, loss = 0.10447341\n",
      "Iteration 37, loss = 0.10287557\n",
      "Iteration 38, loss = 0.10193323\n",
      "Iteration 39, loss = 0.10186408\n",
      "Iteration 40, loss = 0.10032841\n",
      "Iteration 41, loss = 0.09929813\n",
      "Iteration 42, loss = 0.10070315\n",
      "Iteration 43, loss = 0.09751276\n",
      "Iteration 44, loss = 0.10128500\n",
      "Iteration 45, loss = 0.09899444\n",
      "Iteration 46, loss = 0.09473029\n",
      "Iteration 47, loss = 0.09392311\n",
      "Iteration 48, loss = 0.09342614\n",
      "Iteration 49, loss = 0.09088644\n",
      "Iteration 50, loss = 0.09069791\n",
      "Iteration 51, loss = 0.08920150\n",
      "Iteration 52, loss = 0.08843910\n",
      "Iteration 53, loss = 0.08741438\n",
      "Iteration 54, loss = 0.08707132\n",
      "Iteration 55, loss = 0.08629053\n",
      "Iteration 56, loss = 0.08576873\n",
      "Iteration 57, loss = 0.08519513\n",
      "Iteration 58, loss = 0.08460828\n",
      "Iteration 59, loss = 0.08466496\n",
      "Iteration 60, loss = 0.08344391\n",
      "Iteration 61, loss = 0.08304177\n",
      "Iteration 62, loss = 0.08275248\n",
      "Iteration 63, loss = 0.08219016\n",
      "Iteration 64, loss = 0.08145119\n",
      "Iteration 65, loss = 0.08136889\n",
      "Iteration 66, loss = 0.08071760\n",
      "Iteration 67, loss = 0.08037895\n",
      "Iteration 68, loss = 0.08091539\n",
      "Iteration 69, loss = 0.08098272\n",
      "Iteration 70, loss = 0.07955039\n",
      "Iteration 71, loss = 0.07928710\n",
      "Iteration 72, loss = 0.07878377\n",
      "Iteration 73, loss = 0.07835927\n",
      "Iteration 74, loss = 0.07747489\n",
      "Iteration 75, loss = 0.07727338\n",
      "Iteration 76, loss = 0.07696061\n",
      "Iteration 77, loss = 0.07643343\n",
      "Iteration 78, loss = 0.07938550\n",
      "Iteration 79, loss = 0.07872186\n",
      "Iteration 80, loss = 0.07572674\n",
      "Iteration 81, loss = 0.07572619\n",
      "Iteration 82, loss = 0.08061243\n",
      "Iteration 83, loss = 0.07540231\n",
      "Iteration 84, loss = 0.07596586\n",
      "Iteration 85, loss = 0.07442753\n",
      "Iteration 86, loss = 0.07466090\n",
      "Iteration 87, loss = 0.07437648\n",
      "Iteration 88, loss = 0.07353252\n",
      "Iteration 89, loss = 0.07310800\n",
      "Iteration 90, loss = 0.07257759\n",
      "Iteration 91, loss = 0.07126525\n",
      "Iteration 92, loss = 0.07115207\n",
      "Iteration 93, loss = 0.07028406\n",
      "Iteration 94, loss = 0.06958034\n",
      "Iteration 95, loss = 0.06941290\n",
      "Iteration 96, loss = 0.06913059\n",
      "Iteration 97, loss = 0.06877113\n",
      "Iteration 98, loss = 0.06866446\n",
      "Iteration 99, loss = 0.06840837\n",
      "Iteration 100, loss = 0.06840743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.43758377\n",
      "Iteration 2, loss = 0.86190800\n",
      "Iteration 3, loss = 0.86412029\n",
      "Iteration 4, loss = 0.68715999\n",
      "Iteration 5, loss = 0.65410949\n",
      "Iteration 6, loss = 0.57710453\n",
      "Iteration 7, loss = 0.52874175\n",
      "Iteration 8, loss = 0.53233451\n",
      "Iteration 9, loss = 0.54247320\n",
      "Iteration 10, loss = 0.49683303\n",
      "Iteration 11, loss = 0.36095423\n",
      "Iteration 12, loss = 0.36239464\n",
      "Iteration 13, loss = 0.35791085\n",
      "Iteration 14, loss = 0.42763101\n",
      "Iteration 15, loss = 0.38711452\n",
      "Iteration 16, loss = 0.29292647\n",
      "Iteration 17, loss = 0.36708336\n",
      "Iteration 18, loss = 0.32413012\n",
      "Iteration 19, loss = 0.37490923\n",
      "Iteration 20, loss = 0.40429722\n",
      "Iteration 21, loss = 0.35811992\n",
      "Iteration 22, loss = 0.35991455\n",
      "Iteration 23, loss = 0.35135537\n",
      "Iteration 24, loss = 0.28382292\n",
      "Iteration 25, loss = 0.32250379\n",
      "Iteration 26, loss = 0.28563814\n",
      "Iteration 27, loss = 0.42379784\n",
      "Iteration 28, loss = 0.26439515\n",
      "Iteration 29, loss = 0.27601366\n",
      "Iteration 30, loss = 0.25032546\n",
      "Iteration 31, loss = 0.26428944\n",
      "Iteration 32, loss = 0.25289044\n",
      "Iteration 33, loss = 0.23859165\n",
      "Iteration 34, loss = 0.21506858\n",
      "Iteration 35, loss = 0.20229019\n",
      "Iteration 36, loss = 0.18894630\n",
      "Iteration 37, loss = 0.19029232\n",
      "Iteration 38, loss = 0.19173477\n",
      "Iteration 39, loss = 0.26155213\n",
      "Iteration 40, loss = 0.15145027\n",
      "Iteration 41, loss = 0.17617043\n",
      "Iteration 42, loss = 0.22194466\n",
      "Iteration 43, loss = 0.24623158\n",
      "Iteration 44, loss = 0.19579130\n",
      "Iteration 45, loss = 0.21187277\n",
      "Iteration 46, loss = 0.22867038\n",
      "Iteration 47, loss = 0.17926525\n",
      "Iteration 48, loss = 0.17378617\n",
      "Iteration 49, loss = 0.16183300\n",
      "Iteration 50, loss = 0.20649260\n",
      "Iteration 51, loss = 0.15926494\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 52, loss = 0.17826785\n",
      "Iteration 53, loss = 0.15420129\n",
      "Iteration 54, loss = 0.13819170\n",
      "Iteration 55, loss = 0.12439807\n",
      "Iteration 56, loss = 0.12444644\n",
      "Iteration 57, loss = 0.12042607\n",
      "Iteration 58, loss = 0.11079336\n",
      "Iteration 59, loss = 0.10689379\n",
      "Iteration 60, loss = 0.11022207\n",
      "Iteration 61, loss = 0.10796668\n",
      "Iteration 62, loss = 0.10559267\n",
      "Iteration 63, loss = 0.10246488\n",
      "Iteration 64, loss = 0.09664238\n",
      "Iteration 65, loss = 0.09543714\n",
      "Iteration 66, loss = 0.09471650\n",
      "Iteration 67, loss = 0.09309447\n",
      "Iteration 68, loss = 0.09247693\n",
      "Iteration 69, loss = 0.09170436\n",
      "Iteration 70, loss = 0.09102916\n",
      "Iteration 71, loss = 0.09054129\n",
      "Iteration 72, loss = 0.08978645\n",
      "Iteration 73, loss = 0.09188631\n",
      "Iteration 74, loss = 0.08972222\n",
      "Iteration 75, loss = 0.08930853\n",
      "Iteration 76, loss = 0.08897373\n",
      "Iteration 77, loss = 0.08868793\n",
      "Iteration 78, loss = 0.08833339\n",
      "Iteration 79, loss = 0.08794378\n",
      "Iteration 80, loss = 0.08761848\n",
      "Iteration 81, loss = 0.08724016\n",
      "Iteration 82, loss = 0.08696268\n",
      "Iteration 83, loss = 0.08756301\n",
      "Iteration 84, loss = 0.08645715\n",
      "Iteration 85, loss = 0.08562827\n",
      "Iteration 86, loss = 0.08532535\n",
      "Iteration 87, loss = 0.08510226\n",
      "Iteration 88, loss = 0.08492672\n",
      "Iteration 89, loss = 0.08461942\n",
      "Iteration 90, loss = 0.08445987\n",
      "Iteration 91, loss = 0.08416398\n",
      "Iteration 92, loss = 0.08403975\n",
      "Iteration 93, loss = 0.08379711\n",
      "Iteration 94, loss = 0.08353123\n",
      "Iteration 95, loss = 0.08317752\n",
      "Iteration 96, loss = 0.08299700\n",
      "Iteration 97, loss = 0.08275466\n",
      "Iteration 98, loss = 0.08257370\n",
      "Iteration 99, loss = 0.08230732\n",
      "Iteration 100, loss = 0.08207005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.79127788\n",
      "Iteration 2, loss = 1.32725467\n",
      "Iteration 3, loss = 1.29285031\n",
      "Iteration 4, loss = 1.21014961\n",
      "Iteration 5, loss = 1.18474834\n",
      "Iteration 6, loss = 1.02087637\n",
      "Iteration 7, loss = 0.93677163\n",
      "Iteration 8, loss = 0.75531461\n",
      "Iteration 9, loss = 0.91595268\n",
      "Iteration 10, loss = 0.74673369\n",
      "Iteration 11, loss = 0.79544765\n",
      "Iteration 12, loss = 0.70537919\n",
      "Iteration 13, loss = 0.55086596\n",
      "Iteration 14, loss = 0.54509706\n",
      "Iteration 15, loss = 0.56411126\n",
      "Iteration 16, loss = 0.46549412\n",
      "Iteration 17, loss = 0.51617362\n",
      "Iteration 18, loss = 0.50500011\n",
      "Iteration 19, loss = 0.40920031\n",
      "Iteration 20, loss = 0.49977906\n",
      "Iteration 21, loss = 0.45612550\n",
      "Iteration 22, loss = 0.49936906\n",
      "Iteration 23, loss = 0.45175872\n",
      "Iteration 24, loss = 0.46480299\n",
      "Iteration 25, loss = 0.50193896\n",
      "Iteration 26, loss = 0.40595171\n",
      "Iteration 27, loss = 0.44633593\n",
      "Iteration 28, loss = 0.38669975\n",
      "Iteration 29, loss = 0.39721029\n",
      "Iteration 30, loss = 0.47825386\n",
      "Iteration 31, loss = 0.38559973\n",
      "Iteration 32, loss = 0.35574059\n",
      "Iteration 33, loss = 0.35057198\n",
      "Iteration 34, loss = 0.34202821\n",
      "Iteration 35, loss = 0.33598080\n",
      "Iteration 36, loss = 0.45215952\n",
      "Iteration 37, loss = 0.32228206\n",
      "Iteration 38, loss = 0.34581253\n",
      "Iteration 39, loss = 0.35149306\n",
      "Iteration 40, loss = 0.27914038\n",
      "Iteration 41, loss = 0.30862779\n",
      "Iteration 42, loss = 0.34970084\n",
      "Iteration 43, loss = 0.30238479\n",
      "Iteration 44, loss = 0.32883348\n",
      "Iteration 45, loss = 0.33809916\n",
      "Iteration 46, loss = 0.27581043\n",
      "Iteration 47, loss = 0.26935823\n",
      "Iteration 48, loss = 0.33752690\n",
      "Iteration 49, loss = 0.31970025\n",
      "Iteration 50, loss = 0.26687522\n",
      "Iteration 51, loss = 0.31823858\n",
      "Iteration 52, loss = 0.27985022\n",
      "Iteration 53, loss = 0.25391487\n",
      "Iteration 54, loss = 0.37069422\n",
      "Iteration 55, loss = 0.33415866\n",
      "Iteration 56, loss = 0.31456286\n",
      "Iteration 57, loss = 0.25537707\n",
      "Iteration 58, loss = 0.28362753\n",
      "Iteration 59, loss = 0.26839879\n",
      "Iteration 60, loss = 0.26724918\n",
      "Iteration 61, loss = 0.33870571\n",
      "Iteration 62, loss = 0.40264254\n",
      "Iteration 63, loss = 0.37035856\n",
      "Iteration 64, loss = 0.29253847\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 65, loss = 0.24220310\n",
      "Iteration 66, loss = 0.22879942\n",
      "Iteration 67, loss = 0.21810023\n",
      "Iteration 68, loss = 0.20843006\n",
      "Iteration 69, loss = 0.20464786\n",
      "Iteration 70, loss = 0.19952526\n",
      "Iteration 71, loss = 0.19447278\n",
      "Iteration 72, loss = 0.18943506\n",
      "Iteration 73, loss = 0.18785758\n",
      "Iteration 74, loss = 0.18823937\n",
      "Iteration 75, loss = 0.19726806\n",
      "Iteration 76, loss = 0.18673176\n",
      "Iteration 77, loss = 0.18213344\n",
      "Iteration 78, loss = 0.18544417\n",
      "Iteration 79, loss = 0.18293694\n",
      "Iteration 80, loss = 0.19183342\n",
      "Iteration 81, loss = 0.18405859\n",
      "Iteration 82, loss = 0.17636537\n",
      "Iteration 83, loss = 0.17998859\n",
      "Iteration 84, loss = 0.17776251\n",
      "Iteration 85, loss = 0.17508595\n",
      "Iteration 86, loss = 0.17550966\n",
      "Iteration 87, loss = 0.17731747\n",
      "Iteration 88, loss = 0.16954205\n",
      "Iteration 89, loss = 0.16691979\n",
      "Iteration 90, loss = 0.17096554\n",
      "Iteration 91, loss = 0.16347693\n",
      "Iteration 92, loss = 0.16206475\n",
      "Iteration 93, loss = 0.16093352\n",
      "Iteration 94, loss = 0.16061818\n",
      "Iteration 95, loss = 0.16114928\n",
      "Iteration 96, loss = 0.15689724\n",
      "Iteration 97, loss = 0.15723620\n",
      "Iteration 98, loss = 0.15607354\n",
      "Iteration 99, loss = 0.15537524\n",
      "Iteration 100, loss = 0.15387941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.02666554\n",
      "Iteration 2, loss = 1.44724062\n",
      "Iteration 3, loss = 1.09984193\n",
      "Iteration 4, loss = 0.86160832\n",
      "Iteration 5, loss = 0.72551598\n",
      "Iteration 6, loss = 0.67217084\n",
      "Iteration 7, loss = 0.75447915\n",
      "Iteration 8, loss = 0.80702370\n",
      "Iteration 9, loss = 0.70910600\n",
      "Iteration 10, loss = 0.61126139\n",
      "Iteration 11, loss = 0.59930718\n",
      "Iteration 12, loss = 0.57955050\n",
      "Iteration 13, loss = 0.50172160\n",
      "Iteration 14, loss = 0.52089293\n",
      "Iteration 15, loss = 0.47904577\n",
      "Iteration 16, loss = 0.57470247\n",
      "Iteration 17, loss = 0.49157276\n",
      "Iteration 18, loss = 0.48911080\n",
      "Iteration 19, loss = 0.48527509\n",
      "Iteration 20, loss = 0.42869663\n",
      "Iteration 21, loss = 0.45975351\n",
      "Iteration 22, loss = 0.42219201\n",
      "Iteration 23, loss = 0.48151364\n",
      "Iteration 24, loss = 0.55022318\n",
      "Iteration 25, loss = 0.41654077\n",
      "Iteration 26, loss = 0.39662129\n",
      "Iteration 27, loss = 0.46198850\n",
      "Iteration 28, loss = 0.46535142\n",
      "Iteration 29, loss = 0.47827085\n",
      "Iteration 30, loss = 0.34043144\n",
      "Iteration 31, loss = 0.40798197\n",
      "Iteration 32, loss = 0.32324760\n",
      "Iteration 33, loss = 0.29949358\n",
      "Iteration 34, loss = 0.41120420\n",
      "Iteration 35, loss = 0.43859978\n",
      "Iteration 36, loss = 0.28538179\n",
      "Iteration 37, loss = 0.29503620\n",
      "Iteration 38, loss = 0.31074806\n",
      "Iteration 39, loss = 0.24801440\n",
      "Iteration 40, loss = 0.26391631\n",
      "Iteration 41, loss = 0.27846604\n",
      "Iteration 42, loss = 0.29897861\n",
      "Iteration 43, loss = 0.48441657\n",
      "Iteration 44, loss = 0.46577083\n",
      "Iteration 45, loss = 0.47487248\n",
      "Iteration 46, loss = 0.43597112\n",
      "Iteration 47, loss = 0.33584388\n",
      "Iteration 48, loss = 0.30318941\n",
      "Iteration 49, loss = 0.26232285\n",
      "Iteration 50, loss = 0.24707958\n",
      "Iteration 51, loss = 0.21730336\n",
      "Iteration 52, loss = 0.19542340\n",
      "Iteration 53, loss = 0.27826105\n",
      "Iteration 54, loss = 0.27262657\n",
      "Iteration 55, loss = 0.22024705\n",
      "Iteration 56, loss = 0.20629173\n",
      "Iteration 57, loss = 0.23958919\n",
      "Iteration 58, loss = 0.29993132\n",
      "Iteration 59, loss = 0.30878171\n",
      "Iteration 60, loss = 0.21024596\n",
      "Iteration 61, loss = 0.26150923\n",
      "Iteration 62, loss = 0.27361071\n",
      "Iteration 63, loss = 0.28182144\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 64, loss = 0.20207710\n",
      "Iteration 65, loss = 0.17109042\n",
      "Iteration 66, loss = 0.16054945\n",
      "Iteration 67, loss = 0.15335947\n",
      "Iteration 68, loss = 0.14672691\n",
      "Iteration 69, loss = 0.14206825\n",
      "Iteration 70, loss = 0.14092165\n",
      "Iteration 71, loss = 0.13898443\n",
      "Iteration 72, loss = 0.13800540\n",
      "Iteration 73, loss = 0.13609217\n",
      "Iteration 74, loss = 0.13399785\n",
      "Iteration 75, loss = 0.13435528\n",
      "Iteration 76, loss = 0.13282364\n",
      "Iteration 77, loss = 0.13089117\n",
      "Iteration 78, loss = 0.13056381\n",
      "Iteration 79, loss = 0.13017668\n",
      "Iteration 80, loss = 0.12946252\n",
      "Iteration 81, loss = 0.12869208\n",
      "Iteration 82, loss = 0.12809798\n",
      "Iteration 83, loss = 0.12751968\n",
      "Iteration 84, loss = 0.12701572\n",
      "Iteration 85, loss = 0.12681825\n",
      "Iteration 86, loss = 0.12619308\n",
      "Iteration 87, loss = 0.12588893\n",
      "Iteration 88, loss = 0.12566298\n",
      "Iteration 89, loss = 0.12518570\n",
      "Iteration 90, loss = 0.12495483\n",
      "Iteration 91, loss = 0.12409131\n",
      "Iteration 92, loss = 0.12342473\n",
      "Iteration 93, loss = 0.12354148\n",
      "Iteration 94, loss = 0.12259965\n",
      "Iteration 95, loss = 0.12231368\n",
      "Iteration 96, loss = 0.12165295\n",
      "Iteration 97, loss = 0.12134276\n",
      "Iteration 98, loss = 0.12106971\n",
      "Iteration 99, loss = 0.12067606\n",
      "Iteration 100, loss = 0.12027992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.65201778\n",
      "Iteration 2, loss = 0.86505522\n",
      "Iteration 3, loss = 0.65353923\n",
      "Iteration 4, loss = 0.55071128\n",
      "Iteration 5, loss = 0.55339451\n",
      "Iteration 6, loss = 0.63834970\n",
      "Iteration 7, loss = 0.51766710\n",
      "Iteration 8, loss = 0.48931747\n",
      "Iteration 9, loss = 0.43977306\n",
      "Iteration 10, loss = 0.41890808\n",
      "Iteration 11, loss = 0.41729053\n",
      "Iteration 12, loss = 0.43866526\n",
      "Iteration 13, loss = 0.43072857\n",
      "Iteration 14, loss = 0.40636876\n",
      "Iteration 15, loss = 0.35282136\n",
      "Iteration 16, loss = 0.41164026\n",
      "Iteration 17, loss = 0.38397238\n",
      "Iteration 18, loss = 0.33822766\n",
      "Iteration 19, loss = 0.31496563\n",
      "Iteration 20, loss = 0.27259196\n",
      "Iteration 21, loss = 0.34987457\n",
      "Iteration 22, loss = 0.26185582\n",
      "Iteration 23, loss = 0.30860078\n",
      "Iteration 24, loss = 0.26064950\n",
      "Iteration 25, loss = 0.36802669\n",
      "Iteration 26, loss = 0.46213581\n",
      "Iteration 27, loss = 0.43787573\n",
      "Iteration 28, loss = 0.35445565\n",
      "Iteration 29, loss = 0.33418991\n",
      "Iteration 30, loss = 0.36214330\n",
      "Iteration 31, loss = 0.27558873\n",
      "Iteration 32, loss = 0.34590104\n",
      "Iteration 33, loss = 0.33393732\n",
      "Iteration 34, loss = 0.29553670\n",
      "Iteration 35, loss = 0.21207391\n",
      "Iteration 36, loss = 0.26817968\n",
      "Iteration 37, loss = 0.27799335\n",
      "Iteration 38, loss = 0.21041706\n",
      "Iteration 39, loss = 0.22710761\n",
      "Iteration 40, loss = 0.36384091\n",
      "Iteration 41, loss = 0.32512987\n",
      "Iteration 42, loss = 0.22707741\n",
      "Iteration 43, loss = 0.23870372\n",
      "Iteration 44, loss = 0.21115801\n",
      "Iteration 45, loss = 0.20250988\n",
      "Iteration 46, loss = 0.18259189\n",
      "Iteration 47, loss = 0.21624207\n",
      "Iteration 48, loss = 0.15490140\n",
      "Iteration 49, loss = 0.16787763\n",
      "Iteration 50, loss = 0.16869598\n",
      "Iteration 51, loss = 0.18924038\n",
      "Iteration 52, loss = 0.29103318\n",
      "Iteration 53, loss = 0.23895376\n",
      "Iteration 54, loss = 0.20829380\n",
      "Iteration 55, loss = 0.21094583\n",
      "Iteration 56, loss = 0.27728253\n",
      "Iteration 57, loss = 0.24539495\n",
      "Iteration 58, loss = 0.20247915\n",
      "Iteration 59, loss = 0.20918346\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 60, loss = 0.23148029\n",
      "Iteration 61, loss = 0.19223763\n",
      "Iteration 62, loss = 0.16614009\n",
      "Iteration 63, loss = 0.14570100\n",
      "Iteration 64, loss = 0.13937012\n",
      "Iteration 65, loss = 0.13491825\n",
      "Iteration 66, loss = 0.13369745\n",
      "Iteration 67, loss = 0.13156373\n",
      "Iteration 68, loss = 0.12952356\n",
      "Iteration 69, loss = 0.12741247\n",
      "Iteration 70, loss = 0.12653523\n",
      "Iteration 71, loss = 0.12820861\n",
      "Iteration 72, loss = 0.12636642\n",
      "Iteration 73, loss = 0.12722345\n",
      "Iteration 74, loss = 0.12674345\n",
      "Iteration 75, loss = 0.12455639\n",
      "Iteration 76, loss = 0.12772047\n",
      "Iteration 77, loss = 0.11939296\n",
      "Iteration 78, loss = 0.11877082\n",
      "Iteration 79, loss = 0.11814821\n",
      "Iteration 80, loss = 0.11770955\n",
      "Iteration 81, loss = 0.11698031\n",
      "Iteration 82, loss = 0.11633014\n",
      "Iteration 83, loss = 0.11577041\n",
      "Iteration 84, loss = 0.11527939\n",
      "Iteration 85, loss = 0.11448996\n",
      "Iteration 86, loss = 0.11419342\n",
      "Iteration 87, loss = 0.11332711\n",
      "Iteration 88, loss = 0.11292872\n",
      "Iteration 89, loss = 0.11258711\n",
      "Iteration 90, loss = 0.11222651\n",
      "Iteration 91, loss = 0.11191716\n",
      "Iteration 92, loss = 0.11161064\n",
      "Iteration 93, loss = 0.11130204\n",
      "Iteration 94, loss = 0.11097458\n",
      "Iteration 95, loss = 0.10940199\n",
      "Iteration 96, loss = 0.10230568\n",
      "Iteration 97, loss = 0.09952056\n",
      "Iteration 98, loss = 0.09850700\n",
      "Iteration 99, loss = 0.09763478\n",
      "Iteration 100, loss = 0.09506286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.63484748\n",
      "Iteration 2, loss = 1.06478707\n",
      "Iteration 3, loss = 0.80435509\n",
      "Iteration 4, loss = 0.71677024\n",
      "Iteration 5, loss = 0.70682345\n",
      "Iteration 6, loss = 0.55340745\n",
      "Iteration 7, loss = 0.57519984\n",
      "Iteration 8, loss = 0.44758463\n",
      "Iteration 9, loss = 0.44674647\n",
      "Iteration 10, loss = 0.44787258\n",
      "Iteration 11, loss = 0.48599895\n",
      "Iteration 12, loss = 0.46477942\n",
      "Iteration 13, loss = 0.52637195\n",
      "Iteration 14, loss = 0.61437288\n",
      "Iteration 15, loss = 0.46799984\n",
      "Iteration 16, loss = 0.54070769\n",
      "Iteration 17, loss = 0.43158799\n",
      "Iteration 18, loss = 0.44075141\n",
      "Iteration 19, loss = 0.40737509\n",
      "Iteration 20, loss = 0.34821759\n",
      "Iteration 21, loss = 0.52314871\n",
      "Iteration 22, loss = 0.48843160\n",
      "Iteration 23, loss = 0.51261296\n",
      "Iteration 24, loss = 0.46623122\n",
      "Iteration 25, loss = 0.51090085\n",
      "Iteration 26, loss = 0.34634952\n",
      "Iteration 27, loss = 0.30362362\n",
      "Iteration 28, loss = 0.24473741\n",
      "Iteration 29, loss = 0.32728308\n",
      "Iteration 30, loss = 0.35160522\n",
      "Iteration 31, loss = 0.32621974\n",
      "Iteration 32, loss = 0.24680252\n",
      "Iteration 33, loss = 0.36917637\n",
      "Iteration 34, loss = 0.26132103\n",
      "Iteration 35, loss = 0.30649688\n",
      "Iteration 36, loss = 0.29923986\n",
      "Iteration 37, loss = 0.24859951\n",
      "Iteration 38, loss = 0.24428644\n",
      "Iteration 39, loss = 0.18985591\n",
      "Iteration 40, loss = 0.21506297\n",
      "Iteration 41, loss = 0.21549117\n",
      "Iteration 42, loss = 0.20230057\n",
      "Iteration 43, loss = 0.15997984\n",
      "Iteration 44, loss = 0.21979463\n",
      "Iteration 45, loss = 0.26887706\n",
      "Iteration 46, loss = 0.31342819\n",
      "Iteration 47, loss = 0.18560806\n",
      "Iteration 48, loss = 0.30849890\n",
      "Iteration 49, loss = 0.22788536\n",
      "Iteration 50, loss = 0.27965710\n",
      "Iteration 51, loss = 0.27487100\n",
      "Iteration 52, loss = 0.17097398\n",
      "Iteration 53, loss = 0.19918194\n",
      "Iteration 54, loss = 0.26015225\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 55, loss = 0.14078570\n",
      "Iteration 56, loss = 0.12327549\n",
      "Iteration 57, loss = 0.10454207\n",
      "Iteration 58, loss = 0.09904164\n",
      "Iteration 59, loss = 0.10077522\n",
      "Iteration 60, loss = 0.09643057\n",
      "Iteration 61, loss = 0.09342720\n",
      "Iteration 62, loss = 0.09740762\n",
      "Iteration 63, loss = 0.09460631\n",
      "Iteration 64, loss = 0.09316558\n",
      "Iteration 65, loss = 0.09246223\n",
      "Iteration 66, loss = 0.08880166\n",
      "Iteration 67, loss = 0.09167635\n",
      "Iteration 68, loss = 0.08858555\n",
      "Iteration 69, loss = 0.09206338\n",
      "Iteration 70, loss = 0.09040819\n",
      "Iteration 71, loss = 0.08790681\n",
      "Iteration 72, loss = 0.09070778\n",
      "Iteration 73, loss = 0.08837517\n",
      "Iteration 74, loss = 0.08580447\n",
      "Iteration 75, loss = 0.08649154\n",
      "Iteration 76, loss = 0.08453758\n",
      "Iteration 77, loss = 0.08397663\n",
      "Iteration 78, loss = 0.08564133\n",
      "Iteration 79, loss = 0.08342603\n",
      "Iteration 80, loss = 0.08337292\n",
      "Iteration 81, loss = 0.08326869\n",
      "Iteration 82, loss = 0.08306706\n",
      "Iteration 83, loss = 0.08281701\n",
      "Iteration 84, loss = 0.08243626\n",
      "Iteration 85, loss = 0.08223042\n",
      "Iteration 86, loss = 0.08199554\n",
      "Iteration 87, loss = 0.08191465\n",
      "Iteration 88, loss = 0.08185196\n",
      "Iteration 89, loss = 0.08128577\n",
      "Iteration 90, loss = 0.08121292\n",
      "Iteration 91, loss = 0.08106974\n",
      "Iteration 92, loss = 0.08090895\n",
      "Iteration 93, loss = 0.08070824\n",
      "Iteration 94, loss = 0.08036309\n",
      "Iteration 95, loss = 0.08030952\n",
      "Iteration 96, loss = 0.08008628\n",
      "Iteration 97, loss = 0.07985622\n",
      "Iteration 98, loss = 0.08012192\n",
      "Iteration 99, loss = 0.08039085\n",
      "Iteration 100, loss = 0.08011542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.01631576\n",
      "Iteration 2, loss = 1.57833681\n",
      "Iteration 3, loss = 1.15511452\n",
      "Iteration 4, loss = 1.19431140\n",
      "Iteration 5, loss = 1.02330867\n",
      "Iteration 6, loss = 0.90323579\n",
      "Iteration 7, loss = 1.09156022\n",
      "Iteration 8, loss = 0.85889335\n",
      "Iteration 9, loss = 0.71582393\n",
      "Iteration 10, loss = 0.57955283\n",
      "Iteration 11, loss = 0.61085152\n",
      "Iteration 12, loss = 0.64069611\n",
      "Iteration 13, loss = 0.49332158\n",
      "Iteration 14, loss = 0.54783814\n",
      "Iteration 15, loss = 0.57678674\n",
      "Iteration 16, loss = 0.50201320\n",
      "Iteration 17, loss = 0.57527133\n",
      "Iteration 18, loss = 0.50971561\n",
      "Iteration 19, loss = 0.50683124\n",
      "Iteration 20, loss = 0.42353855\n",
      "Iteration 21, loss = 0.62635770\n",
      "Iteration 22, loss = 0.52574410\n",
      "Iteration 23, loss = 0.42710530\n",
      "Iteration 24, loss = 0.53010585\n",
      "Iteration 25, loss = 0.45048035\n",
      "Iteration 26, loss = 0.37043098\n",
      "Iteration 27, loss = 0.51149007\n",
      "Iteration 28, loss = 0.39603267\n",
      "Iteration 29, loss = 0.35810893\n",
      "Iteration 30, loss = 0.40557013\n",
      "Iteration 31, loss = 0.39322697\n",
      "Iteration 32, loss = 0.46087853\n",
      "Iteration 33, loss = 0.40597198\n",
      "Iteration 34, loss = 0.32245519\n",
      "Iteration 35, loss = 0.43848068\n",
      "Iteration 36, loss = 0.28957927\n",
      "Iteration 37, loss = 0.37788206\n",
      "Iteration 38, loss = 0.34339527\n",
      "Iteration 39, loss = 0.33642984\n",
      "Iteration 40, loss = 0.41151062\n",
      "Iteration 41, loss = 0.37469995\n",
      "Iteration 42, loss = 0.31169513\n",
      "Iteration 43, loss = 0.25993891\n",
      "Iteration 44, loss = 0.28431433\n",
      "Iteration 45, loss = 0.26480290\n",
      "Iteration 46, loss = 0.23633346\n",
      "Iteration 47, loss = 0.29295989\n",
      "Iteration 48, loss = 0.33417847\n",
      "Iteration 49, loss = 0.26612891\n",
      "Iteration 50, loss = 0.34876178\n",
      "Iteration 51, loss = 0.28940429\n",
      "Iteration 52, loss = 0.25235791\n",
      "Iteration 53, loss = 0.21449028\n",
      "Iteration 54, loss = 0.20760850\n",
      "Iteration 55, loss = 0.20091771\n",
      "Iteration 56, loss = 0.35391698\n",
      "Iteration 57, loss = 0.26612958\n",
      "Iteration 58, loss = 0.23350835\n",
      "Iteration 59, loss = 0.25477390\n",
      "Iteration 60, loss = 0.20350753\n",
      "Iteration 61, loss = 0.25067697\n",
      "Iteration 62, loss = 0.27266440\n",
      "Iteration 63, loss = 0.27202323\n",
      "Iteration 64, loss = 0.26625586\n",
      "Iteration 65, loss = 0.20619253\n",
      "Iteration 66, loss = 0.22231739\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 67, loss = 0.21024962\n",
      "Iteration 68, loss = 0.17542624\n",
      "Iteration 69, loss = 0.15673966\n",
      "Iteration 70, loss = 0.15329537\n",
      "Iteration 71, loss = 0.14129012\n",
      "Iteration 72, loss = 0.13302655\n",
      "Iteration 73, loss = 0.13433601\n",
      "Iteration 74, loss = 0.13377156\n",
      "Iteration 75, loss = 0.12753974\n",
      "Iteration 76, loss = 0.12430872\n",
      "Iteration 77, loss = 0.12240945\n",
      "Iteration 78, loss = 0.12225936\n",
      "Iteration 79, loss = 0.12069677\n",
      "Iteration 80, loss = 0.12033497\n",
      "Iteration 81, loss = 0.11712676\n",
      "Iteration 82, loss = 0.11627523\n",
      "Iteration 83, loss = 0.11449529\n",
      "Iteration 84, loss = 0.11468646\n",
      "Iteration 85, loss = 0.11375966\n",
      "Iteration 86, loss = 0.11323405\n",
      "Iteration 87, loss = 0.11282953\n",
      "Iteration 88, loss = 0.11238605\n",
      "Iteration 89, loss = 0.11186710\n",
      "Iteration 90, loss = 0.11158614\n",
      "Iteration 91, loss = 0.11114616\n",
      "Iteration 92, loss = 0.10940907\n",
      "Iteration 93, loss = 0.11396920\n",
      "Iteration 94, loss = 0.10922185\n",
      "Iteration 95, loss = 0.10889799\n",
      "Iteration 96, loss = 0.10825600\n",
      "Iteration 97, loss = 0.11041025\n",
      "Iteration 98, loss = 0.10836158\n",
      "Iteration 99, loss = 0.10715137\n",
      "Iteration 100, loss = 0.10685994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.34893049\n",
      "Iteration 2, loss = 0.77075640\n",
      "Iteration 3, loss = 0.67629554\n",
      "Iteration 4, loss = 0.47789969\n",
      "Iteration 5, loss = 0.53759377\n",
      "Iteration 6, loss = 0.41822426\n",
      "Iteration 7, loss = 0.37353208\n",
      "Iteration 8, loss = 0.45908317\n",
      "Iteration 9, loss = 0.38189813\n",
      "Iteration 10, loss = 0.40075243\n",
      "Iteration 11, loss = 0.37348714\n",
      "Iteration 12, loss = 0.28095010\n",
      "Iteration 13, loss = 0.26604957\n",
      "Iteration 14, loss = 0.43254029\n",
      "Iteration 15, loss = 0.42213439\n",
      "Iteration 16, loss = 0.33751853\n",
      "Iteration 17, loss = 0.41697705\n",
      "Iteration 18, loss = 0.38096064\n",
      "Iteration 19, loss = 0.37238522\n",
      "Iteration 20, loss = 0.27593152\n",
      "Iteration 21, loss = 0.26653322\n",
      "Iteration 22, loss = 0.33657382\n",
      "Iteration 23, loss = 0.31584857\n",
      "Iteration 24, loss = 0.34003015\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 25, loss = 0.26596923\n",
      "Iteration 26, loss = 0.20889771\n",
      "Iteration 27, loss = 0.20750522\n",
      "Iteration 28, loss = 0.18623584\n",
      "Iteration 29, loss = 0.17226015\n",
      "Iteration 30, loss = 0.17497450\n",
      "Iteration 31, loss = 0.16266246\n",
      "Iteration 32, loss = 0.15665695\n",
      "Iteration 33, loss = 0.16500238\n",
      "Iteration 34, loss = 0.15770857\n",
      "Iteration 35, loss = 0.14334771\n",
      "Iteration 36, loss = 0.13649462\n",
      "Iteration 37, loss = 0.14107656\n",
      "Iteration 38, loss = 0.13413339\n",
      "Iteration 39, loss = 0.12848062\n",
      "Iteration 40, loss = 0.11555575\n",
      "Iteration 41, loss = 0.11155538\n",
      "Iteration 42, loss = 0.10890107\n",
      "Iteration 43, loss = 0.10621596\n",
      "Iteration 44, loss = 0.10915409\n",
      "Iteration 45, loss = 0.10393648\n",
      "Iteration 46, loss = 0.10219358\n",
      "Iteration 47, loss = 0.09879023\n",
      "Iteration 48, loss = 0.10159410\n",
      "Iteration 49, loss = 0.09778658\n",
      "Iteration 50, loss = 0.10094527\n",
      "Iteration 51, loss = 0.09859078\n",
      "Iteration 52, loss = 0.09407359\n",
      "Iteration 53, loss = 0.09205768\n",
      "Iteration 54, loss = 0.10042998\n",
      "Iteration 55, loss = 0.09004217\n",
      "Iteration 56, loss = 0.08509383\n",
      "Iteration 57, loss = 0.09245936\n",
      "Iteration 58, loss = 0.08391828\n",
      "Iteration 59, loss = 0.08203714\n",
      "Iteration 60, loss = 0.08438566\n",
      "Iteration 61, loss = 0.08151013\n",
      "Iteration 62, loss = 0.08013633\n",
      "Iteration 63, loss = 0.07818599\n",
      "Iteration 64, loss = 0.07747002\n",
      "Iteration 65, loss = 0.07733843\n",
      "Iteration 66, loss = 0.07625461\n",
      "Iteration 67, loss = 0.07586885\n",
      "Iteration 68, loss = 0.07512034\n",
      "Iteration 69, loss = 0.07493399\n",
      "Iteration 70, loss = 0.07448016\n",
      "Iteration 71, loss = 0.07409734\n",
      "Iteration 72, loss = 0.07369771\n",
      "Iteration 73, loss = 0.07339242\n",
      "Iteration 74, loss = 0.07810850\n",
      "Iteration 75, loss = 0.07787423\n",
      "Iteration 76, loss = 0.07363631\n",
      "Iteration 77, loss = 0.07214610\n",
      "Iteration 78, loss = 0.07168574\n",
      "Iteration 79, loss = 0.07136458\n",
      "Iteration 80, loss = 0.07101141\n",
      "Iteration 81, loss = 0.07066123\n",
      "Iteration 82, loss = 0.07190987\n",
      "Iteration 83, loss = 0.07023893\n",
      "Iteration 84, loss = 0.06984029\n",
      "Iteration 85, loss = 0.06952476\n",
      "Iteration 86, loss = 0.06939912\n",
      "Iteration 87, loss = 0.06913325\n",
      "Iteration 88, loss = 0.06859789\n",
      "Iteration 89, loss = 0.06829953\n",
      "Iteration 90, loss = 0.06815952\n",
      "Iteration 91, loss = 0.06801070\n",
      "Iteration 92, loss = 0.07112152\n",
      "Iteration 93, loss = 0.07598012\n",
      "Iteration 94, loss = 0.06936934\n",
      "Iteration 95, loss = 0.06737022\n",
      "Iteration 96, loss = 0.06690491\n",
      "Iteration 97, loss = 0.06611473\n",
      "Iteration 98, loss = 0.06622828\n",
      "Iteration 99, loss = 0.06593006\n",
      "Iteration 100, loss = 0.06550730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.56434061\n",
      "Iteration 2, loss = 0.94809424\n",
      "Iteration 3, loss = 0.72927897\n",
      "Iteration 4, loss = 0.59234607\n",
      "Iteration 5, loss = 0.63462100\n",
      "Iteration 6, loss = 0.66750475\n",
      "Iteration 7, loss = 0.57040404\n",
      "Iteration 8, loss = 0.57059715\n",
      "Iteration 9, loss = 0.51410270\n",
      "Iteration 10, loss = 0.60419635\n",
      "Iteration 11, loss = 0.56029354\n",
      "Iteration 12, loss = 0.55337289\n",
      "Iteration 13, loss = 0.51569480\n",
      "Iteration 14, loss = 0.49749022\n",
      "Iteration 15, loss = 0.39037394\n",
      "Iteration 16, loss = 0.50555898\n",
      "Iteration 17, loss = 0.39885478\n",
      "Iteration 18, loss = 0.50074899\n",
      "Iteration 19, loss = 0.49726919\n",
      "Iteration 20, loss = 0.31754477\n",
      "Iteration 21, loss = 0.33038851\n",
      "Iteration 22, loss = 0.29210882\n",
      "Iteration 23, loss = 0.24875315\n",
      "Iteration 24, loss = 0.33129243\n",
      "Iteration 25, loss = 0.33170773\n",
      "Iteration 26, loss = 0.28719440\n",
      "Iteration 27, loss = 0.25779754\n",
      "Iteration 28, loss = 0.25174404\n",
      "Iteration 29, loss = 0.30109241\n",
      "Iteration 30, loss = 0.25094624\n",
      "Iteration 31, loss = 0.28900766\n",
      "Iteration 32, loss = 0.22455487\n",
      "Iteration 33, loss = 0.20204973\n",
      "Iteration 34, loss = 0.26439196\n",
      "Iteration 35, loss = 0.25081056\n",
      "Iteration 36, loss = 0.20333990\n",
      "Iteration 37, loss = 0.19260242\n",
      "Iteration 38, loss = 0.21292821\n",
      "Iteration 39, loss = 0.22557018\n",
      "Iteration 40, loss = 0.24054817\n",
      "Iteration 41, loss = 0.29796859\n",
      "Iteration 42, loss = 0.34516957\n",
      "Iteration 43, loss = 0.31725290\n",
      "Iteration 44, loss = 0.24907392\n",
      "Iteration 45, loss = 0.22878486\n",
      "Iteration 46, loss = 0.27478468\n",
      "Iteration 47, loss = 0.24642301\n",
      "Iteration 48, loss = 0.26039681\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 49, loss = 0.23393052\n",
      "Iteration 50, loss = 0.22280091\n",
      "Iteration 51, loss = 0.20145267\n",
      "Iteration 52, loss = 0.18592587\n",
      "Iteration 53, loss = 0.16835110\n",
      "Iteration 54, loss = 0.15694587\n",
      "Iteration 55, loss = 0.13908449\n",
      "Iteration 56, loss = 0.13519350\n",
      "Iteration 57, loss = 0.13192281\n",
      "Iteration 58, loss = 0.13446142\n",
      "Iteration 59, loss = 0.12711954\n",
      "Iteration 60, loss = 0.12309668\n",
      "Iteration 61, loss = 0.12209952\n",
      "Iteration 62, loss = 0.12134077\n",
      "Iteration 63, loss = 0.12062541\n",
      "Iteration 64, loss = 0.12024438\n",
      "Iteration 65, loss = 0.11898931\n",
      "Iteration 66, loss = 0.11828236\n",
      "Iteration 67, loss = 0.11688141\n",
      "Iteration 68, loss = 0.11614510\n",
      "Iteration 69, loss = 0.11562590\n",
      "Iteration 70, loss = 0.11444685\n",
      "Iteration 71, loss = 0.11572535\n",
      "Iteration 72, loss = 0.11347674\n",
      "Iteration 73, loss = 0.11271719\n",
      "Iteration 74, loss = 0.11205173\n",
      "Iteration 75, loss = 0.11111475\n",
      "Iteration 76, loss = 0.11078431\n",
      "Iteration 77, loss = 0.10679739\n",
      "Iteration 78, loss = 0.10686308\n",
      "Iteration 79, loss = 0.10459983\n",
      "Iteration 80, loss = 0.10349691\n",
      "Iteration 81, loss = 0.10037458\n",
      "Iteration 82, loss = 0.09960865\n",
      "Iteration 83, loss = 0.10518920\n",
      "Iteration 84, loss = 0.10241747\n",
      "Iteration 85, loss = 0.09500260\n",
      "Iteration 86, loss = 0.09465919\n",
      "Iteration 87, loss = 0.09384139\n",
      "Iteration 88, loss = 0.09857239\n",
      "Iteration 89, loss = 0.09325461\n",
      "Iteration 90, loss = 0.09334200\n",
      "Iteration 91, loss = 0.09235119\n",
      "Iteration 92, loss = 0.09087519\n",
      "Iteration 93, loss = 0.09057217\n",
      "Iteration 94, loss = 0.08972909\n",
      "Iteration 95, loss = 0.08947962\n",
      "Iteration 96, loss = 0.08864294\n",
      "Iteration 97, loss = 0.08873005\n",
      "Iteration 98, loss = 0.08786533\n",
      "Iteration 99, loss = 0.08741568\n",
      "Iteration 100, loss = 0.08708972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.52025326\n",
      "Iteration 2, loss = 0.91274306\n",
      "Iteration 3, loss = 0.77236017\n",
      "Iteration 4, loss = 0.67407496\n",
      "Iteration 5, loss = 0.56152762\n",
      "Iteration 6, loss = 0.42779421\n",
      "Iteration 7, loss = 0.44287739\n",
      "Iteration 8, loss = 0.48411773\n",
      "Iteration 9, loss = 0.47717534\n",
      "Iteration 10, loss = 0.43802093\n",
      "Iteration 11, loss = 0.41589645\n",
      "Iteration 12, loss = 0.29475343\n",
      "Iteration 13, loss = 0.44169370\n",
      "Iteration 14, loss = 0.35760742\n",
      "Iteration 15, loss = 0.45858949\n",
      "Iteration 16, loss = 0.33185076\n",
      "Iteration 17, loss = 0.27705622\n",
      "Iteration 18, loss = 0.29827122\n",
      "Iteration 19, loss = 0.25563393\n",
      "Iteration 20, loss = 0.30004322\n",
      "Iteration 21, loss = 0.36032525\n",
      "Iteration 22, loss = 0.26724608\n",
      "Iteration 23, loss = 0.20714891\n",
      "Iteration 24, loss = 0.23124700\n",
      "Iteration 25, loss = 0.24835559\n",
      "Iteration 26, loss = 0.32355168\n",
      "Iteration 27, loss = 0.24268173\n",
      "Iteration 28, loss = 0.20754880\n",
      "Iteration 29, loss = 0.31896954\n",
      "Iteration 30, loss = 0.23253737\n",
      "Iteration 31, loss = 0.23966605\n",
      "Iteration 32, loss = 0.25333728\n",
      "Iteration 33, loss = 0.30592557\n",
      "Iteration 34, loss = 0.23005006\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 35, loss = 0.16835014\n",
      "Iteration 36, loss = 0.14136959\n",
      "Iteration 37, loss = 0.13160614\n",
      "Iteration 38, loss = 0.11705990\n",
      "Iteration 39, loss = 0.11509671\n",
      "Iteration 40, loss = 0.11189003\n",
      "Iteration 41, loss = 0.11030714\n",
      "Iteration 42, loss = 0.10312295\n",
      "Iteration 43, loss = 0.10381260\n",
      "Iteration 44, loss = 0.10851455\n",
      "Iteration 45, loss = 0.10459103\n",
      "Iteration 46, loss = 0.10287095\n",
      "Iteration 47, loss = 0.10029927\n",
      "Iteration 48, loss = 0.10002703\n",
      "Iteration 49, loss = 0.09951223\n",
      "Iteration 50, loss = 0.10562578\n",
      "Iteration 51, loss = 0.10464483\n",
      "Iteration 52, loss = 0.10141957\n",
      "Iteration 53, loss = 0.09764503\n",
      "Iteration 54, loss = 0.09252848\n",
      "Iteration 55, loss = 0.09474795\n",
      "Iteration 56, loss = 0.09057630\n",
      "Iteration 57, loss = 0.08937587\n",
      "Iteration 58, loss = 0.08909714\n",
      "Iteration 59, loss = 0.08809817\n",
      "Iteration 60, loss = 0.08615453\n",
      "Iteration 61, loss = 0.08559853\n",
      "Iteration 62, loss = 0.08476688\n",
      "Iteration 63, loss = 0.08389546\n",
      "Iteration 64, loss = 0.08334242\n",
      "Iteration 65, loss = 0.08224089\n",
      "Iteration 66, loss = 0.08229206\n",
      "Iteration 67, loss = 0.08100152\n",
      "Iteration 68, loss = 0.08063582\n",
      "Iteration 69, loss = 0.08016530\n",
      "Iteration 70, loss = 0.07969251\n",
      "Iteration 71, loss = 0.07919657\n",
      "Iteration 72, loss = 0.07879014\n",
      "Iteration 73, loss = 0.07847620\n",
      "Iteration 74, loss = 0.07813798\n",
      "Iteration 75, loss = 0.07783038\n",
      "Iteration 76, loss = 0.07754605\n",
      "Iteration 77, loss = 0.07733209\n",
      "Iteration 78, loss = 0.07690483\n",
      "Iteration 79, loss = 0.07645158\n",
      "Iteration 80, loss = 0.07619237\n",
      "Iteration 81, loss = 0.07598074\n",
      "Iteration 82, loss = 0.07562340\n",
      "Iteration 83, loss = 0.07539320\n",
      "Iteration 84, loss = 0.07511275\n",
      "Iteration 85, loss = 0.07491768\n",
      "Iteration 86, loss = 0.07466878\n",
      "Iteration 87, loss = 0.07444405\n",
      "Iteration 88, loss = 0.07422722\n",
      "Iteration 89, loss = 0.07403901\n",
      "Iteration 90, loss = 0.07381414\n",
      "Iteration 91, loss = 0.07362825\n",
      "Iteration 92, loss = 0.07347341\n",
      "Iteration 93, loss = 0.07358872\n",
      "Iteration 94, loss = 0.07496356\n",
      "Iteration 95, loss = 0.07461676\n",
      "Iteration 96, loss = 0.07234611\n",
      "Iteration 97, loss = 0.07151506\n",
      "Iteration 98, loss = 0.07122011\n",
      "Iteration 99, loss = 0.07113780\n",
      "Iteration 100, loss = 0.07075311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.72555935\n",
      "Iteration 2, loss = 1.04418838\n",
      "Iteration 3, loss = 0.67616107\n",
      "Iteration 4, loss = 0.69431485\n",
      "Iteration 5, loss = 0.56525477\n",
      "Iteration 6, loss = 0.74548470\n",
      "Iteration 7, loss = 0.52757281\n",
      "Iteration 8, loss = 0.51255794\n",
      "Iteration 9, loss = 0.44138633\n",
      "Iteration 10, loss = 0.54092162\n",
      "Iteration 11, loss = 0.39715402\n",
      "Iteration 12, loss = 0.45819434\n",
      "Iteration 13, loss = 0.57727082\n",
      "Iteration 14, loss = 0.42627471\n",
      "Iteration 15, loss = 0.44672857\n",
      "Iteration 16, loss = 0.49271898\n",
      "Iteration 17, loss = 0.50327093\n",
      "Iteration 18, loss = 0.50082367\n",
      "Iteration 19, loss = 0.44922354\n",
      "Iteration 20, loss = 0.43421030\n",
      "Iteration 21, loss = 0.36982188\n",
      "Iteration 22, loss = 0.52183779\n",
      "Iteration 23, loss = 0.55842213\n",
      "Iteration 24, loss = 0.50579348\n",
      "Iteration 25, loss = 0.38671335\n",
      "Iteration 26, loss = 0.36935585\n",
      "Iteration 27, loss = 0.35203633\n",
      "Iteration 28, loss = 0.30556375\n",
      "Iteration 29, loss = 0.35145922\n",
      "Iteration 30, loss = 0.33064253\n",
      "Iteration 31, loss = 0.27800992\n",
      "Iteration 32, loss = 0.29414239\n",
      "Iteration 33, loss = 0.30852092\n",
      "Iteration 34, loss = 0.33761270\n",
      "Iteration 35, loss = 0.30237984\n",
      "Iteration 36, loss = 0.24615784\n",
      "Iteration 37, loss = 0.24725719\n",
      "Iteration 38, loss = 0.53507798\n",
      "Iteration 39, loss = 0.45297363\n",
      "Iteration 40, loss = 0.48040384\n",
      "Iteration 41, loss = 0.41408004\n",
      "Iteration 42, loss = 0.41688569\n",
      "Iteration 43, loss = 0.35176071\n",
      "Iteration 44, loss = 0.43236274\n",
      "Iteration 45, loss = 0.34178618\n",
      "Iteration 46, loss = 0.32966994\n",
      "Iteration 47, loss = 0.30471786\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 48, loss = 0.31504006\n",
      "Iteration 49, loss = 0.25669143\n",
      "Iteration 50, loss = 0.22489243\n",
      "Iteration 51, loss = 0.21338195\n",
      "Iteration 52, loss = 0.20891630\n",
      "Iteration 53, loss = 0.19886851\n",
      "Iteration 54, loss = 0.19156303\n",
      "Iteration 55, loss = 0.18750419\n",
      "Iteration 56, loss = 0.17807352\n",
      "Iteration 57, loss = 0.16904768\n",
      "Iteration 58, loss = 0.16534568\n",
      "Iteration 59, loss = 0.16413291\n",
      "Iteration 60, loss = 0.16718592\n",
      "Iteration 61, loss = 0.15962877\n",
      "Iteration 62, loss = 0.16255090\n",
      "Iteration 63, loss = 0.15647127\n",
      "Iteration 64, loss = 0.15393278\n",
      "Iteration 65, loss = 0.15222132\n",
      "Iteration 66, loss = 0.15131381\n",
      "Iteration 67, loss = 0.14816735\n",
      "Iteration 68, loss = 0.14856617\n",
      "Iteration 69, loss = 0.14544138\n",
      "Iteration 70, loss = 0.14424694\n",
      "Iteration 71, loss = 0.14356828\n",
      "Iteration 72, loss = 0.14320591\n",
      "Iteration 73, loss = 0.14139149\n",
      "Iteration 74, loss = 0.14161241\n",
      "Iteration 75, loss = 0.13957131\n",
      "Iteration 76, loss = 0.13879483\n",
      "Iteration 77, loss = 0.13869980\n",
      "Iteration 78, loss = 0.13761820\n",
      "Iteration 79, loss = 0.13754178\n",
      "Iteration 80, loss = 0.13749026\n",
      "Iteration 81, loss = 0.13376245\n",
      "Iteration 82, loss = 0.13236494\n",
      "Iteration 83, loss = 0.13243028\n",
      "Iteration 84, loss = 0.13112867\n",
      "Iteration 85, loss = 0.13112860\n",
      "Iteration 86, loss = 0.12968219\n",
      "Iteration 87, loss = 0.12889784\n",
      "Iteration 88, loss = 0.12797955\n",
      "Iteration 89, loss = 0.12824305\n",
      "Iteration 90, loss = 0.12661851\n",
      "Iteration 91, loss = 0.12619874\n",
      "Iteration 92, loss = 0.12593408\n",
      "Iteration 93, loss = 0.12532757\n",
      "Iteration 94, loss = 0.12423942\n",
      "Iteration 95, loss = 0.12352341\n",
      "Iteration 96, loss = 0.12270668\n",
      "Iteration 97, loss = 0.12267334\n",
      "Iteration 98, loss = 0.12200853\n",
      "Iteration 99, loss = 0.12135272\n",
      "Iteration 100, loss = 0.12128614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.36906433\n",
      "Iteration 2, loss = 0.82725015\n",
      "Iteration 3, loss = 0.62516168\n",
      "Iteration 4, loss = 0.63755077\n",
      "Iteration 5, loss = 0.63104702\n",
      "Iteration 6, loss = 0.47097708\n",
      "Iteration 7, loss = 0.47460720\n",
      "Iteration 8, loss = 0.38720105\n",
      "Iteration 9, loss = 0.35184242\n",
      "Iteration 10, loss = 0.42050394\n",
      "Iteration 11, loss = 0.50541002\n",
      "Iteration 12, loss = 0.44741694\n",
      "Iteration 13, loss = 0.49140974\n",
      "Iteration 14, loss = 0.37717281\n",
      "Iteration 15, loss = 0.43524559\n",
      "Iteration 16, loss = 0.35724694\n",
      "Iteration 17, loss = 0.40237387\n",
      "Iteration 18, loss = 0.38371317\n",
      "Iteration 19, loss = 0.33096284\n",
      "Iteration 20, loss = 0.35316045\n",
      "Iteration 21, loss = 0.31215848\n",
      "Iteration 22, loss = 0.28897882\n",
      "Iteration 23, loss = 0.37566336\n",
      "Iteration 24, loss = 0.35106170\n",
      "Iteration 25, loss = 0.31355063\n",
      "Iteration 26, loss = 0.38179175\n",
      "Iteration 27, loss = 0.35430172\n",
      "Iteration 28, loss = 0.39277971\n",
      "Iteration 29, loss = 0.27520266\n",
      "Iteration 30, loss = 0.27412482\n",
      "Iteration 31, loss = 0.22892085\n",
      "Iteration 32, loss = 0.20760243\n",
      "Iteration 33, loss = 0.21441397\n",
      "Iteration 34, loss = 0.26256279\n",
      "Iteration 35, loss = 0.18533601\n",
      "Iteration 36, loss = 0.23177791\n",
      "Iteration 37, loss = 0.20383978\n",
      "Iteration 38, loss = 0.23370261\n",
      "Iteration 39, loss = 0.24242676\n",
      "Iteration 40, loss = 0.22639034\n",
      "Iteration 41, loss = 0.17281184\n",
      "Iteration 42, loss = 0.24599454\n",
      "Iteration 43, loss = 0.19138604\n",
      "Iteration 44, loss = 0.14111756\n",
      "Iteration 45, loss = 0.17318618\n",
      "Iteration 46, loss = 0.14055759\n",
      "Iteration 47, loss = 0.21497247\n",
      "Iteration 48, loss = 0.16497125\n",
      "Iteration 49, loss = 0.10361802\n",
      "Iteration 50, loss = 0.13264196\n",
      "Iteration 51, loss = 0.11559919\n",
      "Iteration 52, loss = 0.12087218\n",
      "Iteration 53, loss = 0.12150474\n",
      "Iteration 54, loss = 0.16007191\n",
      "Iteration 55, loss = 0.13777331\n",
      "Iteration 56, loss = 0.09107976\n",
      "Iteration 57, loss = 0.08546569\n",
      "Iteration 58, loss = 0.08651623\n",
      "Iteration 59, loss = 0.09942972\n",
      "Iteration 60, loss = 0.08328153\n",
      "Iteration 61, loss = 0.07981873\n",
      "Iteration 62, loss = 0.09528215\n",
      "Iteration 63, loss = 0.14068974\n",
      "Iteration 64, loss = 0.16001108\n",
      "Iteration 65, loss = 0.12101373\n",
      "Iteration 66, loss = 0.20266265\n",
      "Iteration 67, loss = 0.12744220\n",
      "Iteration 68, loss = 0.17475611\n",
      "Iteration 69, loss = 0.16132688\n",
      "Iteration 70, loss = 0.17759207\n",
      "Iteration 71, loss = 0.14291089\n",
      "Iteration 72, loss = 0.11535595\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 73, loss = 0.12590809\n",
      "Iteration 74, loss = 0.11455665\n",
      "Iteration 75, loss = 0.10500484\n",
      "Iteration 76, loss = 0.09838147\n",
      "Iteration 77, loss = 0.09290533\n",
      "Iteration 78, loss = 0.08975159\n",
      "Iteration 79, loss = 0.08676196\n",
      "Iteration 80, loss = 0.08554600\n",
      "Iteration 81, loss = 0.08449453\n",
      "Iteration 82, loss = 0.08399791\n",
      "Iteration 83, loss = 0.08404330\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 84, loss = 0.08201133\n",
      "Iteration 85, loss = 0.08175287\n",
      "Iteration 86, loss = 0.08154219\n",
      "Iteration 87, loss = 0.08131535\n",
      "Iteration 88, loss = 0.08099857\n",
      "Iteration 89, loss = 0.08064602\n",
      "Iteration 90, loss = 0.08044230\n",
      "Iteration 91, loss = 0.08028164\n",
      "Iteration 92, loss = 0.08014774\n",
      "Iteration 93, loss = 0.07998825\n",
      "Iteration 94, loss = 0.07984784\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 95, loss = 0.07955098\n",
      "Iteration 96, loss = 0.07950281\n",
      "Iteration 97, loss = 0.07946039\n",
      "Iteration 98, loss = 0.07941222\n",
      "Iteration 99, loss = 0.07935935\n",
      "Iteration 100, loss = 0.07930536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.63167699\n",
      "Iteration 2, loss = 0.99167055\n",
      "Iteration 3, loss = 0.76474492\n",
      "Iteration 4, loss = 0.72999518\n",
      "Iteration 5, loss = 0.57387239\n",
      "Iteration 6, loss = 0.57346567\n",
      "Iteration 7, loss = 0.49934575\n",
      "Iteration 8, loss = 0.39717967\n",
      "Iteration 9, loss = 0.49411931\n",
      "Iteration 10, loss = 0.51564130\n",
      "Iteration 11, loss = 0.41369601\n",
      "Iteration 12, loss = 0.35752941\n",
      "Iteration 13, loss = 0.31286417\n",
      "Iteration 14, loss = 0.41352815\n",
      "Iteration 15, loss = 0.38425377\n",
      "Iteration 16, loss = 0.47531702\n",
      "Iteration 17, loss = 0.34421895\n",
      "Iteration 18, loss = 0.34712004\n",
      "Iteration 19, loss = 0.35059574\n",
      "Iteration 20, loss = 0.32977923\n",
      "Iteration 21, loss = 0.26778245\n",
      "Iteration 22, loss = 0.23677821\n",
      "Iteration 23, loss = 0.22099629\n",
      "Iteration 24, loss = 0.32837670\n",
      "Iteration 25, loss = 0.24211923\n",
      "Iteration 26, loss = 0.33810157\n",
      "Iteration 27, loss = 0.33404702\n",
      "Iteration 28, loss = 0.29407568\n",
      "Iteration 29, loss = 0.23119228\n",
      "Iteration 30, loss = 0.31892690\n",
      "Iteration 31, loss = 0.23799792\n",
      "Iteration 32, loss = 0.37566022\n",
      "Iteration 33, loss = 0.23013841\n",
      "Iteration 34, loss = 0.20437066\n",
      "Iteration 35, loss = 0.18813355\n",
      "Iteration 36, loss = 0.39071644\n",
      "Iteration 37, loss = 0.25349048\n",
      "Iteration 38, loss = 0.30630625\n",
      "Iteration 39, loss = 0.29642883\n",
      "Iteration 40, loss = 0.25977523\n",
      "Iteration 41, loss = 0.40855945\n",
      "Iteration 42, loss = 0.24055107\n",
      "Iteration 43, loss = 0.16988845\n",
      "Iteration 44, loss = 0.23758342\n",
      "Iteration 45, loss = 0.30362929\n",
      "Iteration 46, loss = 0.29650050\n",
      "Iteration 47, loss = 0.26335328\n",
      "Iteration 48, loss = 0.29652358\n",
      "Iteration 49, loss = 0.40838954\n",
      "Iteration 50, loss = 0.25284076\n",
      "Iteration 51, loss = 0.23210804\n",
      "Iteration 52, loss = 0.21301917\n",
      "Iteration 53, loss = 0.18004465\n",
      "Iteration 54, loss = 0.18773267\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 55, loss = 0.17868453\n",
      "Iteration 56, loss = 0.15729824\n",
      "Iteration 57, loss = 0.14413233\n",
      "Iteration 58, loss = 0.14268533\n",
      "Iteration 59, loss = 0.13212138\n",
      "Iteration 60, loss = 0.13075725\n",
      "Iteration 61, loss = 0.12635017\n",
      "Iteration 62, loss = 0.12231528\n",
      "Iteration 63, loss = 0.11931026\n",
      "Iteration 64, loss = 0.11844266\n",
      "Iteration 65, loss = 0.11715700\n",
      "Iteration 66, loss = 0.11583203\n",
      "Iteration 67, loss = 0.10994590\n",
      "Iteration 68, loss = 0.10732047\n",
      "Iteration 69, loss = 0.10404930\n",
      "Iteration 70, loss = 0.10089568\n",
      "Iteration 71, loss = 0.10038361\n",
      "Iteration 72, loss = 0.09707355\n",
      "Iteration 73, loss = 0.09696149\n",
      "Iteration 74, loss = 0.09645148\n",
      "Iteration 75, loss = 0.09596969\n",
      "Iteration 76, loss = 0.09563687\n",
      "Iteration 77, loss = 0.09536958\n",
      "Iteration 78, loss = 0.09530124\n",
      "Iteration 79, loss = 0.09472430\n",
      "Iteration 80, loss = 0.09430623\n",
      "Iteration 81, loss = 0.09377471\n",
      "Iteration 82, loss = 0.09362264\n",
      "Iteration 83, loss = 0.09329986\n",
      "Iteration 84, loss = 0.09316415\n",
      "Iteration 85, loss = 0.09286702\n",
      "Iteration 86, loss = 0.09263706\n",
      "Iteration 87, loss = 0.09267770\n",
      "Iteration 88, loss = 0.09230677\n",
      "Iteration 89, loss = 0.09195175\n",
      "Iteration 90, loss = 0.09175918\n",
      "Iteration 91, loss = 0.09158034\n",
      "Iteration 92, loss = 0.09127986\n",
      "Iteration 93, loss = 0.09092341\n",
      "Iteration 94, loss = 0.09087452\n",
      "Iteration 95, loss = 0.09068897\n",
      "Iteration 96, loss = 0.09074871\n",
      "Iteration 97, loss = 0.09044380\n",
      "Iteration 98, loss = 0.09039416\n",
      "Iteration 99, loss = 0.09012655\n",
      "Iteration 100, loss = 0.08984508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.49043948\n",
      "Iteration 2, loss = 0.76522604\n",
      "Iteration 3, loss = 0.59358603\n",
      "Iteration 4, loss = 0.51059861\n",
      "Iteration 5, loss = 0.45638238\n",
      "Iteration 6, loss = 0.37158630\n",
      "Iteration 7, loss = 0.42713274\n",
      "Iteration 8, loss = 0.36696989\n",
      "Iteration 9, loss = 0.39987496\n",
      "Iteration 10, loss = 0.35691395\n",
      "Iteration 11, loss = 0.39942385\n",
      "Iteration 12, loss = 0.35394435\n",
      "Iteration 13, loss = 0.28567357\n",
      "Iteration 14, loss = 0.25888286\n",
      "Iteration 15, loss = 0.29461978\n",
      "Iteration 16, loss = 0.26588835\n",
      "Iteration 17, loss = 0.29344333\n",
      "Iteration 18, loss = 0.29313747\n",
      "Iteration 19, loss = 0.23886344\n",
      "Iteration 20, loss = 0.23511008\n",
      "Iteration 21, loss = 0.27567432\n",
      "Iteration 22, loss = 0.24947601\n",
      "Iteration 23, loss = 0.25635665\n",
      "Iteration 24, loss = 0.27478871\n",
      "Iteration 25, loss = 0.17873892\n",
      "Iteration 26, loss = 0.20964897\n",
      "Iteration 27, loss = 0.20602366\n",
      "Iteration 28, loss = 0.16272296\n",
      "Iteration 29, loss = 0.17997126\n",
      "Iteration 30, loss = 0.24734634\n",
      "Iteration 31, loss = 0.22117647\n",
      "Iteration 32, loss = 0.16080528\n",
      "Iteration 33, loss = 0.14818778\n",
      "Iteration 34, loss = 0.19980238\n",
      "Iteration 35, loss = 0.18797394\n",
      "Iteration 36, loss = 0.12577897\n",
      "Iteration 37, loss = 0.23158415\n",
      "Iteration 38, loss = 0.22728981\n",
      "Iteration 39, loss = 0.15774318\n",
      "Iteration 40, loss = 0.16947413\n",
      "Iteration 41, loss = 0.23263752\n",
      "Iteration 42, loss = 0.13443427\n",
      "Iteration 43, loss = 0.13328394\n",
      "Iteration 44, loss = 0.15652885\n",
      "Iteration 45, loss = 0.11955825\n",
      "Iteration 46, loss = 0.11329076\n",
      "Iteration 47, loss = 0.11393845\n",
      "Iteration 48, loss = 0.11011712\n",
      "Iteration 49, loss = 0.06612571\n",
      "Iteration 50, loss = 0.06289745\n",
      "Iteration 51, loss = 0.08120967\n",
      "Iteration 52, loss = 0.07257068\n",
      "Iteration 53, loss = 0.06078342\n",
      "Iteration 54, loss = 0.08251092\n",
      "Iteration 55, loss = 0.12404296\n",
      "Iteration 56, loss = 0.13536278\n",
      "Iteration 57, loss = 0.13581186\n",
      "Iteration 58, loss = 0.07481799\n",
      "Iteration 59, loss = 0.07790594\n",
      "Iteration 60, loss = 0.11270326\n",
      "Iteration 61, loss = 0.15354051\n",
      "Iteration 62, loss = 0.11955903\n",
      "Iteration 63, loss = 0.07344414\n",
      "Iteration 64, loss = 0.15680916\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 65, loss = 0.08257852\n",
      "Iteration 66, loss = 0.05509731\n",
      "Iteration 67, loss = 0.05376207\n",
      "Iteration 68, loss = 0.05044838\n",
      "Iteration 69, loss = 0.04873304\n",
      "Iteration 70, loss = 0.04768333\n",
      "Iteration 71, loss = 0.04695622\n",
      "Iteration 72, loss = 0.04605270\n",
      "Iteration 73, loss = 0.04496758\n",
      "Iteration 74, loss = 0.04406546\n",
      "Iteration 75, loss = 0.04343783\n",
      "Iteration 76, loss = 0.04312743\n",
      "Iteration 77, loss = 0.04210988\n",
      "Iteration 78, loss = 0.04180701\n",
      "Iteration 79, loss = 0.04145595\n",
      "Iteration 80, loss = 0.04123777\n",
      "Iteration 81, loss = 0.04077086\n",
      "Iteration 82, loss = 0.04043124\n",
      "Iteration 83, loss = 0.04002977\n",
      "Iteration 84, loss = 0.03961026\n",
      "Iteration 85, loss = 0.03920243\n",
      "Iteration 86, loss = 0.03885542\n",
      "Iteration 87, loss = 0.03854108\n",
      "Iteration 88, loss = 0.03845297\n",
      "Iteration 89, loss = 0.03584923\n",
      "Iteration 90, loss = 0.03555785\n",
      "Iteration 91, loss = 0.03529031\n",
      "Iteration 92, loss = 0.03497451\n",
      "Iteration 93, loss = 0.03438008\n",
      "Iteration 94, loss = 0.03402822\n",
      "Iteration 95, loss = 0.03387432\n",
      "Iteration 96, loss = 0.03361355\n",
      "Iteration 97, loss = 0.03306320\n",
      "Iteration 98, loss = 0.03304660\n",
      "Iteration 99, loss = 0.03195078\n",
      "Iteration 100, loss = 0.03223936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.52320645\n",
      "Iteration 2, loss = 0.81077535\n",
      "Iteration 3, loss = 0.66198278\n",
      "Iteration 4, loss = 0.56902493\n",
      "Iteration 5, loss = 0.49692643\n",
      "Iteration 6, loss = 0.37590096\n",
      "Iteration 7, loss = 0.37778586\n",
      "Iteration 8, loss = 0.37237382\n",
      "Iteration 9, loss = 0.31836826\n",
      "Iteration 10, loss = 0.45095018\n",
      "Iteration 11, loss = 0.40167384\n",
      "Iteration 12, loss = 0.35997235\n",
      "Iteration 13, loss = 0.32934255\n",
      "Iteration 14, loss = 0.31777670\n",
      "Iteration 15, loss = 0.35619430\n",
      "Iteration 16, loss = 0.39037991\n",
      "Iteration 17, loss = 0.43958261\n",
      "Iteration 18, loss = 0.41359377\n",
      "Iteration 19, loss = 0.32847074\n",
      "Iteration 20, loss = 0.33531864\n",
      "Iteration 21, loss = 0.29176224\n",
      "Iteration 22, loss = 0.34083522\n",
      "Iteration 23, loss = 0.44861095\n",
      "Iteration 24, loss = 0.32732237\n",
      "Iteration 25, loss = 0.31316158\n",
      "Iteration 26, loss = 0.30674757\n",
      "Iteration 27, loss = 0.29283449\n",
      "Iteration 28, loss = 0.28782275\n",
      "Iteration 29, loss = 0.31708058\n",
      "Iteration 30, loss = 0.34093833\n",
      "Iteration 31, loss = 0.27578986\n",
      "Iteration 32, loss = 0.30850975\n",
      "Iteration 33, loss = 0.24521780\n",
      "Iteration 34, loss = 0.19102360\n",
      "Iteration 35, loss = 0.28262520\n",
      "Iteration 36, loss = 0.33388444\n",
      "Iteration 37, loss = 0.31701509\n",
      "Iteration 38, loss = 0.36756924\n",
      "Iteration 39, loss = 0.29532885\n",
      "Iteration 40, loss = 0.24796484\n",
      "Iteration 41, loss = 0.23414610\n",
      "Iteration 42, loss = 0.26715645\n",
      "Iteration 43, loss = 0.28866106\n",
      "Iteration 44, loss = 0.15204122\n",
      "Iteration 45, loss = 0.15124507\n",
      "Iteration 46, loss = 0.21185922\n",
      "Iteration 47, loss = 0.15058846\n",
      "Iteration 48, loss = 0.15065992\n",
      "Iteration 49, loss = 0.12586444\n",
      "Iteration 50, loss = 0.12550556\n",
      "Iteration 51, loss = 0.11765924\n",
      "Iteration 52, loss = 0.16065406\n",
      "Iteration 53, loss = 0.19659217\n",
      "Iteration 54, loss = 0.12988118\n",
      "Iteration 55, loss = 0.12621166\n",
      "Iteration 56, loss = 0.11800242\n",
      "Iteration 57, loss = 0.08974678\n",
      "Iteration 58, loss = 0.16450951\n",
      "Iteration 59, loss = 0.18335117\n",
      "Iteration 60, loss = 0.13134275\n",
      "Iteration 61, loss = 0.12827852\n",
      "Iteration 62, loss = 0.23876639\n",
      "Iteration 63, loss = 0.16741174\n",
      "Iteration 64, loss = 0.20675554\n",
      "Iteration 65, loss = 0.37332252\n",
      "Iteration 66, loss = 0.20309632\n",
      "Iteration 67, loss = 0.19022734\n",
      "Iteration 68, loss = 0.22322699\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 69, loss = 0.21014447\n",
      "Iteration 70, loss = 0.17065079\n",
      "Iteration 71, loss = 0.14295815\n",
      "Iteration 72, loss = 0.11768953\n",
      "Iteration 73, loss = 0.11000515\n",
      "Iteration 74, loss = 0.10774128\n",
      "Iteration 75, loss = 0.10772109\n",
      "Iteration 76, loss = 0.10422068\n",
      "Iteration 77, loss = 0.11073071\n",
      "Iteration 78, loss = 0.10063680\n",
      "Iteration 79, loss = 0.09150423\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 80, loss = 0.08923288\n",
      "Iteration 81, loss = 0.08656298\n",
      "Iteration 82, loss = 0.08589362\n",
      "Iteration 83, loss = 0.08542218\n",
      "Iteration 84, loss = 0.08362522\n",
      "Iteration 85, loss = 0.08288453\n",
      "Iteration 86, loss = 0.08157936\n",
      "Iteration 87, loss = 0.08125191\n",
      "Iteration 88, loss = 0.08085778\n",
      "Iteration 89, loss = 0.08048987\n",
      "Iteration 90, loss = 0.08017811\n",
      "Iteration 91, loss = 0.07974596\n",
      "Iteration 92, loss = 0.07928126\n",
      "Iteration 93, loss = 0.07901064\n",
      "Iteration 94, loss = 0.07858969\n",
      "Iteration 95, loss = 0.07839434\n",
      "Iteration 96, loss = 0.07825108\n",
      "Iteration 97, loss = 0.07804827\n",
      "Iteration 98, loss = 0.07794169\n",
      "Iteration 99, loss = 0.07776216\n",
      "Iteration 100, loss = 0.07766776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.59715329\n",
      "Iteration 2, loss = 0.86734024\n",
      "Iteration 3, loss = 0.59155178\n",
      "Iteration 4, loss = 0.54345455\n",
      "Iteration 5, loss = 0.39445775\n",
      "Iteration 6, loss = 0.37904903\n",
      "Iteration 7, loss = 0.39609960\n",
      "Iteration 8, loss = 0.34798304\n",
      "Iteration 9, loss = 0.36089728\n",
      "Iteration 10, loss = 0.33195656\n",
      "Iteration 11, loss = 0.35501931\n",
      "Iteration 12, loss = 0.30632841\n",
      "Iteration 13, loss = 0.30869563\n",
      "Iteration 14, loss = 0.28246911\n",
      "Iteration 15, loss = 0.37510222\n",
      "Iteration 16, loss = 0.31812002\n",
      "Iteration 17, loss = 0.26097505\n",
      "Iteration 18, loss = 0.29796305\n",
      "Iteration 19, loss = 0.23476344\n",
      "Iteration 20, loss = 0.19750467\n",
      "Iteration 21, loss = 0.33068489\n",
      "Iteration 22, loss = 0.27064711\n",
      "Iteration 23, loss = 0.21001358\n",
      "Iteration 24, loss = 0.22498603\n",
      "Iteration 25, loss = 0.28120858\n",
      "Iteration 26, loss = 0.43951476\n",
      "Iteration 27, loss = 0.34185877\n",
      "Iteration 28, loss = 0.34817522\n",
      "Iteration 29, loss = 0.25295527\n",
      "Iteration 30, loss = 0.26799784\n",
      "Iteration 31, loss = 0.28096699\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 32, loss = 0.22101345\n",
      "Iteration 33, loss = 0.17112384\n",
      "Iteration 34, loss = 0.16334989\n",
      "Iteration 35, loss = 0.15340512\n",
      "Iteration 36, loss = 0.14533458\n",
      "Iteration 37, loss = 0.13226166\n",
      "Iteration 38, loss = 0.12462738\n",
      "Iteration 39, loss = 0.12930601\n",
      "Iteration 40, loss = 0.11567804\n",
      "Iteration 41, loss = 0.11340028\n",
      "Iteration 42, loss = 0.11230235\n",
      "Iteration 43, loss = 0.10525670\n",
      "Iteration 44, loss = 0.10156248\n",
      "Iteration 45, loss = 0.10760581\n",
      "Iteration 46, loss = 0.09946071\n",
      "Iteration 47, loss = 0.09469173\n",
      "Iteration 48, loss = 0.09847220\n",
      "Iteration 49, loss = 0.09255389\n",
      "Iteration 50, loss = 0.09241571\n",
      "Iteration 51, loss = 0.09500542\n",
      "Iteration 52, loss = 0.08633552\n",
      "Iteration 53, loss = 0.08445758\n",
      "Iteration 54, loss = 0.08384394\n",
      "Iteration 55, loss = 0.08428305\n",
      "Iteration 56, loss = 0.08282611\n",
      "Iteration 57, loss = 0.08197438\n",
      "Iteration 58, loss = 0.08047736\n",
      "Iteration 59, loss = 0.08805271\n",
      "Iteration 60, loss = 0.08100576\n",
      "Iteration 61, loss = 0.08031789\n",
      "Iteration 62, loss = 0.07716797\n",
      "Iteration 63, loss = 0.07501107\n",
      "Iteration 64, loss = 0.07531393\n",
      "Iteration 65, loss = 0.07385386\n",
      "Iteration 66, loss = 0.07265651\n",
      "Iteration 67, loss = 0.06983099\n",
      "Iteration 68, loss = 0.07041319\n",
      "Iteration 69, loss = 0.06940050\n",
      "Iteration 70, loss = 0.06887316\n",
      "Iteration 71, loss = 0.06844694\n",
      "Iteration 72, loss = 0.06783124\n",
      "Iteration 73, loss = 0.06788778\n",
      "Iteration 74, loss = 0.06745087\n",
      "Iteration 75, loss = 0.06638879\n",
      "Iteration 76, loss = 0.06619078\n",
      "Iteration 77, loss = 0.06514409\n",
      "Iteration 78, loss = 0.06475364\n",
      "Iteration 79, loss = 0.06489730\n",
      "Iteration 80, loss = 0.06446319\n",
      "Iteration 81, loss = 0.06421273\n",
      "Iteration 82, loss = 0.06362323\n",
      "Iteration 83, loss = 0.06333127\n",
      "Iteration 84, loss = 0.06330890\n",
      "Iteration 85, loss = 0.06299486\n",
      "Iteration 86, loss = 0.06268155\n",
      "Iteration 87, loss = 0.06238623\n",
      "Iteration 88, loss = 0.06168802\n",
      "Iteration 89, loss = 0.06154343\n",
      "Iteration 90, loss = 0.06159287\n",
      "Iteration 91, loss = 0.06105204\n",
      "Iteration 92, loss = 0.06102883\n",
      "Iteration 93, loss = 0.06074488\n",
      "Iteration 94, loss = 0.06028121\n",
      "Iteration 95, loss = 0.06039761\n",
      "Iteration 96, loss = 0.06000035\n",
      "Iteration 97, loss = 0.05996036\n",
      "Iteration 98, loss = 0.05945483\n",
      "Iteration 99, loss = 0.05921359\n",
      "Iteration 100, loss = 0.05907628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.33016618\n",
      "Iteration 2, loss = 0.69679037\n",
      "Iteration 3, loss = 0.49633896\n",
      "Iteration 4, loss = 0.41723940\n",
      "Iteration 5, loss = 0.44235965\n",
      "Iteration 6, loss = 0.45068754\n",
      "Iteration 7, loss = 0.45627415\n",
      "Iteration 8, loss = 0.40648306\n",
      "Iteration 9, loss = 0.50087678\n",
      "Iteration 10, loss = 0.50083498\n",
      "Iteration 11, loss = 0.34137626\n",
      "Iteration 12, loss = 0.47777164\n",
      "Iteration 13, loss = 0.53860755\n",
      "Iteration 14, loss = 0.43301352\n",
      "Iteration 15, loss = 0.44107217\n",
      "Iteration 16, loss = 0.41393777\n",
      "Iteration 17, loss = 0.34235068\n",
      "Iteration 18, loss = 0.29563223\n",
      "Iteration 19, loss = 0.26825071\n",
      "Iteration 20, loss = 0.27079293\n",
      "Iteration 21, loss = 0.28426988\n",
      "Iteration 22, loss = 0.30506790\n",
      "Iteration 23, loss = 0.29738067\n",
      "Iteration 24, loss = 0.28241791\n",
      "Iteration 25, loss = 0.28584803\n",
      "Iteration 26, loss = 0.26063346\n",
      "Iteration 27, loss = 0.24713879\n",
      "Iteration 28, loss = 0.22303500\n",
      "Iteration 29, loss = 0.24570922\n",
      "Iteration 30, loss = 0.27267692\n",
      "Iteration 31, loss = 0.25450865\n",
      "Iteration 32, loss = 0.20732811\n",
      "Iteration 33, loss = 0.24662314\n",
      "Iteration 34, loss = 0.31991589\n",
      "Iteration 35, loss = 0.43351669\n",
      "Iteration 36, loss = 0.28586224\n",
      "Iteration 37, loss = 0.29190675\n",
      "Iteration 38, loss = 0.38091286\n",
      "Iteration 39, loss = 0.39832981\n",
      "Iteration 40, loss = 0.38889469\n",
      "Iteration 41, loss = 0.34015330\n",
      "Iteration 42, loss = 0.27600317\n",
      "Iteration 43, loss = 0.24759681\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 44, loss = 0.17888174\n",
      "Iteration 45, loss = 0.13957043\n",
      "Iteration 46, loss = 0.15244464\n",
      "Iteration 47, loss = 0.12558270\n",
      "Iteration 48, loss = 0.11991925\n",
      "Iteration 49, loss = 0.11103230\n",
      "Iteration 50, loss = 0.10913226\n",
      "Iteration 51, loss = 0.10592066\n",
      "Iteration 52, loss = 0.10271702\n",
      "Iteration 53, loss = 0.09942990\n",
      "Iteration 54, loss = 0.10533313\n",
      "Iteration 55, loss = 0.10234195\n",
      "Iteration 56, loss = 0.10138540\n",
      "Iteration 57, loss = 0.09814852\n",
      "Iteration 58, loss = 0.09669185\n",
      "Iteration 59, loss = 0.09526012\n",
      "Iteration 60, loss = 0.09457396\n",
      "Iteration 61, loss = 0.09399157\n",
      "Iteration 62, loss = 0.09314053\n",
      "Iteration 63, loss = 0.09114883\n",
      "Iteration 64, loss = 0.09414191\n",
      "Iteration 65, loss = 0.08511584\n",
      "Iteration 66, loss = 0.08575413\n",
      "Iteration 67, loss = 0.08464010\n",
      "Iteration 68, loss = 0.08415611\n",
      "Iteration 69, loss = 0.08204786\n",
      "Iteration 70, loss = 0.08288596\n",
      "Iteration 71, loss = 0.08071512\n",
      "Iteration 72, loss = 0.08115621\n",
      "Iteration 73, loss = 0.08093112\n",
      "Iteration 74, loss = 0.07952304\n",
      "Iteration 75, loss = 0.07799741\n",
      "Iteration 76, loss = 0.08061569\n",
      "Iteration 77, loss = 0.07657858\n",
      "Iteration 78, loss = 0.07894861\n",
      "Iteration 79, loss = 0.07739319\n",
      "Iteration 80, loss = 0.07988128\n",
      "Iteration 81, loss = 0.07893693\n",
      "Iteration 82, loss = 0.07701870\n",
      "Iteration 83, loss = 0.07474561\n",
      "Iteration 84, loss = 0.07125196\n",
      "Iteration 85, loss = 0.07140186\n",
      "Iteration 86, loss = 0.07026993\n",
      "Iteration 87, loss = 0.06957814\n",
      "Iteration 88, loss = 0.07013761\n",
      "Iteration 89, loss = 0.06856950\n",
      "Iteration 90, loss = 0.06892073\n",
      "Iteration 91, loss = 0.06825828\n",
      "Iteration 92, loss = 0.06768749\n",
      "Iteration 93, loss = 0.06792143\n",
      "Iteration 94, loss = 0.07141159\n",
      "Iteration 95, loss = 0.06722981\n",
      "Iteration 96, loss = 0.06865308\n",
      "Iteration 97, loss = 0.06668885\n",
      "Iteration 98, loss = 0.06563369\n",
      "Iteration 99, loss = 0.06449134\n",
      "Iteration 100, loss = 0.06434983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.44905807\n",
      "Iteration 2, loss = 0.73704021\n",
      "Iteration 3, loss = 0.56800947\n",
      "Iteration 4, loss = 0.45179824\n",
      "Iteration 5, loss = 0.49385649\n",
      "Iteration 6, loss = 0.46107486\n",
      "Iteration 7, loss = 0.46647195\n",
      "Iteration 8, loss = 0.37323307\n",
      "Iteration 9, loss = 0.37767901\n",
      "Iteration 10, loss = 0.38568702\n",
      "Iteration 11, loss = 0.36926883\n",
      "Iteration 12, loss = 0.37768987\n",
      "Iteration 13, loss = 0.31853574\n",
      "Iteration 14, loss = 0.45087554\n",
      "Iteration 15, loss = 0.36653898\n",
      "Iteration 16, loss = 0.37983864\n",
      "Iteration 17, loss = 0.33293181\n",
      "Iteration 18, loss = 0.42972338\n",
      "Iteration 19, loss = 0.29509788\n",
      "Iteration 20, loss = 0.23749900\n",
      "Iteration 21, loss = 0.25376270\n",
      "Iteration 22, loss = 0.26717644\n",
      "Iteration 23, loss = 0.33669763\n",
      "Iteration 24, loss = 0.34131967\n",
      "Iteration 25, loss = 0.22047137\n",
      "Iteration 26, loss = 0.18917088\n",
      "Iteration 27, loss = 0.21419946\n",
      "Iteration 28, loss = 0.17373872\n",
      "Iteration 29, loss = 0.19561270\n",
      "Iteration 30, loss = 0.22933894\n",
      "Iteration 31, loss = 0.23806378\n",
      "Iteration 32, loss = 0.23864982\n",
      "Iteration 33, loss = 0.28246888\n",
      "Iteration 34, loss = 0.23938974\n",
      "Iteration 35, loss = 0.20564582\n",
      "Iteration 36, loss = 0.24094793\n",
      "Iteration 37, loss = 0.20852021\n",
      "Iteration 38, loss = 0.22673640\n",
      "Iteration 39, loss = 0.21827374\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 40, loss = 0.12914595\n",
      "Iteration 41, loss = 0.09099034\n",
      "Iteration 42, loss = 0.08496645\n",
      "Iteration 43, loss = 0.08248319\n",
      "Iteration 44, loss = 0.08073777\n",
      "Iteration 45, loss = 0.07695350\n",
      "Iteration 46, loss = 0.07423964\n",
      "Iteration 47, loss = 0.06851512\n",
      "Iteration 48, loss = 0.06898417\n",
      "Iteration 49, loss = 0.06681476\n",
      "Iteration 50, loss = 0.06494206\n",
      "Iteration 51, loss = 0.06349691\n",
      "Iteration 52, loss = 0.06113581\n",
      "Iteration 53, loss = 0.05937654\n",
      "Iteration 54, loss = 0.05876568\n",
      "Iteration 55, loss = 0.05831342\n",
      "Iteration 56, loss = 0.05764855\n",
      "Iteration 57, loss = 0.05744947\n",
      "Iteration 58, loss = 0.05681878\n",
      "Iteration 59, loss = 0.05622874\n",
      "Iteration 60, loss = 0.05542362\n",
      "Iteration 61, loss = 0.05447412\n",
      "Iteration 62, loss = 0.05354039\n",
      "Iteration 63, loss = 0.05308706\n",
      "Iteration 64, loss = 0.05269946\n",
      "Iteration 65, loss = 0.05209005\n",
      "Iteration 66, loss = 0.05175317\n",
      "Iteration 67, loss = 0.05199482\n",
      "Iteration 68, loss = 0.05111657\n",
      "Iteration 69, loss = 0.04980604\n",
      "Iteration 70, loss = 0.04899918\n",
      "Iteration 71, loss = 0.04755711\n",
      "Iteration 72, loss = 0.04790240\n",
      "Iteration 73, loss = 0.04773479\n",
      "Iteration 74, loss = 0.04689951\n",
      "Iteration 75, loss = 0.04620955\n",
      "Iteration 76, loss = 0.04575156\n",
      "Iteration 77, loss = 0.04515851\n",
      "Iteration 78, loss = 0.04520776\n",
      "Iteration 79, loss = 0.04473331\n",
      "Iteration 80, loss = 0.04464689\n",
      "Iteration 81, loss = 0.04427776\n",
      "Iteration 82, loss = 0.04362095\n",
      "Iteration 83, loss = 0.04381178\n",
      "Iteration 84, loss = 0.04333882\n",
      "Iteration 85, loss = 0.04313066\n",
      "Iteration 86, loss = 0.04246841\n",
      "Iteration 87, loss = 0.04360452\n",
      "Iteration 88, loss = 0.04229270\n",
      "Iteration 89, loss = 0.04172476\n",
      "Iteration 90, loss = 0.04097214\n",
      "Iteration 91, loss = 0.04201577\n",
      "Iteration 92, loss = 0.04191928\n",
      "Iteration 93, loss = 0.03951819\n",
      "Iteration 94, loss = 0.03980417\n",
      "Iteration 95, loss = 0.03923258\n",
      "Iteration 96, loss = 0.03831394\n",
      "Iteration 97, loss = 0.03820346\n",
      "Iteration 98, loss = 0.03798066\n",
      "Iteration 99, loss = 0.03776265\n",
      "Iteration 100, loss = 0.03779743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.56497265\n",
      "Iteration 2, loss = 0.81520179\n",
      "Iteration 3, loss = 0.67788613\n",
      "Iteration 4, loss = 0.53908039\n",
      "Iteration 5, loss = 0.42686867\n",
      "Iteration 6, loss = 0.47031600\n",
      "Iteration 7, loss = 0.34587064\n",
      "Iteration 8, loss = 0.44662441\n",
      "Iteration 9, loss = 0.51280296\n",
      "Iteration 10, loss = 0.42504957\n",
      "Iteration 11, loss = 0.41592785\n",
      "Iteration 12, loss = 0.46070530\n",
      "Iteration 13, loss = 0.35852301\n",
      "Iteration 14, loss = 0.34969991\n",
      "Iteration 15, loss = 0.33846793\n",
      "Iteration 16, loss = 0.35299387\n",
      "Iteration 17, loss = 0.45343106\n",
      "Iteration 18, loss = 0.40854283\n",
      "Iteration 19, loss = 0.36176626\n",
      "Iteration 20, loss = 0.31708347\n",
      "Iteration 21, loss = 0.26555853\n",
      "Iteration 22, loss = 0.34156885\n",
      "Iteration 23, loss = 0.35798024\n",
      "Iteration 24, loss = 0.26375466\n",
      "Iteration 25, loss = 0.26964848\n",
      "Iteration 26, loss = 0.23982813\n",
      "Iteration 27, loss = 0.34403308\n",
      "Iteration 28, loss = 0.27857364\n",
      "Iteration 29, loss = 0.29218195\n",
      "Iteration 30, loss = 0.28856075\n",
      "Iteration 31, loss = 0.23022401\n",
      "Iteration 32, loss = 0.21078769\n",
      "Iteration 33, loss = 0.20084951\n",
      "Iteration 34, loss = 0.27430633\n",
      "Iteration 35, loss = 0.27081970\n",
      "Iteration 36, loss = 0.27582033\n",
      "Iteration 37, loss = 0.18954652\n",
      "Iteration 38, loss = 0.19870401\n",
      "Iteration 39, loss = 0.17284722\n",
      "Iteration 40, loss = 0.24784934\n",
      "Iteration 41, loss = 0.29490443\n",
      "Iteration 42, loss = 0.23456301\n",
      "Iteration 43, loss = 0.24664349\n",
      "Iteration 44, loss = 0.26855081\n",
      "Iteration 45, loss = 0.28841986\n",
      "Iteration 46, loss = 0.27555743\n",
      "Iteration 47, loss = 0.31994681\n",
      "Iteration 48, loss = 0.30982926\n",
      "Iteration 49, loss = 0.41319912\n",
      "Iteration 50, loss = 0.35827505\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 51, loss = 0.32861499\n",
      "Iteration 52, loss = 0.24333372\n",
      "Iteration 53, loss = 0.23083669\n",
      "Iteration 54, loss = 0.20494424\n",
      "Iteration 55, loss = 0.19324246\n",
      "Iteration 56, loss = 0.18255574\n",
      "Iteration 57, loss = 0.17534817\n",
      "Iteration 58, loss = 0.17187056\n",
      "Iteration 59, loss = 0.16334690\n",
      "Iteration 60, loss = 0.15858545\n",
      "Iteration 61, loss = 0.15754780\n",
      "Iteration 62, loss = 0.15242657\n",
      "Iteration 63, loss = 0.15182902\n",
      "Iteration 64, loss = 0.14717105\n",
      "Iteration 65, loss = 0.14630522\n",
      "Iteration 66, loss = 0.14343466\n",
      "Iteration 67, loss = 0.14240983\n",
      "Iteration 68, loss = 0.14324502\n",
      "Iteration 69, loss = 0.14013679\n",
      "Iteration 70, loss = 0.13896514\n",
      "Iteration 71, loss = 0.13798269\n",
      "Iteration 72, loss = 0.13611055\n",
      "Iteration 73, loss = 0.13420513\n",
      "Iteration 74, loss = 0.13349418\n",
      "Iteration 75, loss = 0.13234881\n",
      "Iteration 76, loss = 0.13113930\n",
      "Iteration 77, loss = 0.12988178\n",
      "Iteration 78, loss = 0.12896088\n",
      "Iteration 79, loss = 0.12665781\n",
      "Iteration 80, loss = 0.12592958\n",
      "Iteration 81, loss = 0.12511259\n",
      "Iteration 82, loss = 0.12383843\n",
      "Iteration 83, loss = 0.12587572\n",
      "Iteration 84, loss = 0.12612553\n",
      "Iteration 85, loss = 0.12488458\n",
      "Iteration 86, loss = 0.12369691\n",
      "Iteration 87, loss = 0.12065212\n",
      "Iteration 88, loss = 0.11833236\n",
      "Iteration 89, loss = 0.11933877\n",
      "Iteration 90, loss = 0.11781769\n",
      "Iteration 91, loss = 0.11592265\n",
      "Iteration 92, loss = 0.11550887\n",
      "Iteration 93, loss = 0.11536151\n",
      "Iteration 94, loss = 0.11453995\n",
      "Iteration 95, loss = 0.12451103\n",
      "Iteration 96, loss = 0.11916020\n",
      "Iteration 97, loss = 0.11928808\n",
      "Iteration 98, loss = 0.12091850\n",
      "Iteration 99, loss = 0.11347516\n",
      "Iteration 100, loss = 0.11214091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.52093781\n",
      "Iteration 2, loss = 0.92796452\n",
      "Iteration 3, loss = 0.69068087\n",
      "Iteration 4, loss = 0.60446597\n",
      "Iteration 5, loss = 0.55516827\n",
      "Iteration 6, loss = 0.56339456\n",
      "Iteration 7, loss = 0.46139826\n",
      "Iteration 8, loss = 0.37153787\n",
      "Iteration 9, loss = 0.43898739\n",
      "Iteration 10, loss = 0.51869318\n",
      "Iteration 11, loss = 0.45211836\n",
      "Iteration 12, loss = 0.39046423\n",
      "Iteration 13, loss = 0.42539559\n",
      "Iteration 14, loss = 0.39274875\n",
      "Iteration 15, loss = 0.41455589\n",
      "Iteration 16, loss = 0.41309152\n",
      "Iteration 17, loss = 0.31255950\n",
      "Iteration 18, loss = 0.34098576\n",
      "Iteration 19, loss = 0.37404879\n",
      "Iteration 20, loss = 0.31527417\n",
      "Iteration 21, loss = 0.40541898\n",
      "Iteration 22, loss = 0.41402395\n",
      "Iteration 23, loss = 0.37032219\n",
      "Iteration 24, loss = 0.36688946\n",
      "Iteration 25, loss = 0.49343738\n",
      "Iteration 26, loss = 0.32564195\n",
      "Iteration 27, loss = 0.27473795\n",
      "Iteration 28, loss = 0.30107421\n",
      "Iteration 29, loss = 0.24029959\n",
      "Iteration 30, loss = 0.19962507\n",
      "Iteration 31, loss = 0.37241136\n",
      "Iteration 32, loss = 0.30627467\n",
      "Iteration 33, loss = 0.23384375\n",
      "Iteration 34, loss = 0.28957303\n",
      "Iteration 35, loss = 0.35091074\n",
      "Iteration 36, loss = 0.31043260\n",
      "Iteration 37, loss = 0.24650107\n",
      "Iteration 38, loss = 0.25763790\n",
      "Iteration 39, loss = 0.23857876\n",
      "Iteration 40, loss = 0.27975566\n",
      "Iteration 41, loss = 0.28349676\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 42, loss = 0.24768081\n",
      "Iteration 43, loss = 0.20077195\n",
      "Iteration 44, loss = 0.17990163\n",
      "Iteration 45, loss = 0.16470516\n",
      "Iteration 46, loss = 0.16489611\n",
      "Iteration 47, loss = 0.14846273\n",
      "Iteration 48, loss = 0.14452558\n",
      "Iteration 49, loss = 0.14293141\n",
      "Iteration 50, loss = 0.14672744\n",
      "Iteration 51, loss = 0.14430853\n",
      "Iteration 52, loss = 0.13671236\n",
      "Iteration 53, loss = 0.13140220\n",
      "Iteration 54, loss = 0.12568069\n",
      "Iteration 55, loss = 0.12166944\n",
      "Iteration 56, loss = 0.12000760\n",
      "Iteration 57, loss = 0.11763855\n",
      "Iteration 58, loss = 0.11381692\n",
      "Iteration 59, loss = 0.11387998\n",
      "Iteration 60, loss = 0.11265196\n",
      "Iteration 61, loss = 0.10906919\n",
      "Iteration 62, loss = 0.11007099\n",
      "Iteration 63, loss = 0.11011763\n",
      "Iteration 64, loss = 0.10947452\n",
      "Iteration 65, loss = 0.10818893\n",
      "Iteration 66, loss = 0.10787978\n",
      "Iteration 67, loss = 0.10628119\n",
      "Iteration 68, loss = 0.10584451\n",
      "Iteration 69, loss = 0.10541702\n",
      "Iteration 70, loss = 0.10176123\n",
      "Iteration 71, loss = 0.10031240\n",
      "Iteration 72, loss = 0.10161103\n",
      "Iteration 73, loss = 0.09663853\n",
      "Iteration 74, loss = 0.11071463\n",
      "Iteration 75, loss = 0.10049904\n",
      "Iteration 76, loss = 0.09183634\n",
      "Iteration 77, loss = 0.09574566\n",
      "Iteration 78, loss = 0.09447950\n",
      "Iteration 79, loss = 0.10013325\n",
      "Iteration 80, loss = 0.09315947\n",
      "Iteration 81, loss = 0.09009238\n",
      "Iteration 82, loss = 0.08951012\n",
      "Iteration 83, loss = 0.09033024\n",
      "Iteration 84, loss = 0.08929342\n",
      "Iteration 85, loss = 0.08872418\n",
      "Iteration 86, loss = 0.08830936\n",
      "Iteration 87, loss = 0.08739983\n",
      "Iteration 88, loss = 0.08699296\n",
      "Iteration 89, loss = 0.08706250\n",
      "Iteration 90, loss = 0.08625649\n",
      "Iteration 91, loss = 0.08584759\n",
      "Iteration 92, loss = 0.08530686\n",
      "Iteration 93, loss = 0.08602351\n",
      "Iteration 94, loss = 0.08514674\n",
      "Iteration 95, loss = 0.08375381\n",
      "Iteration 96, loss = 0.08370800\n",
      "Iteration 97, loss = 0.08394477\n",
      "Iteration 98, loss = 0.08308488\n",
      "Iteration 99, loss = 0.08373454\n",
      "Iteration 100, loss = 0.08685138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.57126269\n",
      "Iteration 2, loss = 0.95618842\n",
      "Iteration 3, loss = 0.71315157\n",
      "Iteration 4, loss = 0.69614446\n",
      "Iteration 5, loss = 0.72151717\n",
      "Iteration 6, loss = 0.71976699\n",
      "Iteration 7, loss = 0.65515403\n",
      "Iteration 8, loss = 0.65986729\n",
      "Iteration 9, loss = 0.80370230\n",
      "Iteration 10, loss = 0.66887536\n",
      "Iteration 11, loss = 0.51385432\n",
      "Iteration 12, loss = 0.49853560\n",
      "Iteration 13, loss = 0.49877124\n",
      "Iteration 14, loss = 0.54658332\n",
      "Iteration 15, loss = 0.48672943\n",
      "Iteration 16, loss = 0.43001766\n",
      "Iteration 17, loss = 0.44859289\n",
      "Iteration 18, loss = 0.42465784\n",
      "Iteration 19, loss = 0.40057782\n",
      "Iteration 20, loss = 0.46726290\n",
      "Iteration 21, loss = 0.63662081\n",
      "Iteration 22, loss = 0.63300857\n",
      "Iteration 23, loss = 0.36213791\n",
      "Iteration 24, loss = 0.46310295\n",
      "Iteration 25, loss = 0.53338181\n",
      "Iteration 26, loss = 0.55355712\n",
      "Iteration 27, loss = 0.38249352\n",
      "Iteration 28, loss = 0.31258308\n",
      "Iteration 29, loss = 0.33456111\n",
      "Iteration 30, loss = 0.30481095\n",
      "Iteration 31, loss = 0.33268620\n",
      "Iteration 32, loss = 0.35868955\n",
      "Iteration 33, loss = 0.29962696\n",
      "Iteration 34, loss = 0.33232223\n",
      "Iteration 35, loss = 0.29966143\n",
      "Iteration 36, loss = 0.29180272\n",
      "Iteration 37, loss = 0.41684192\n",
      "Iteration 38, loss = 0.35973103\n",
      "Iteration 39, loss = 0.30966218\n",
      "Iteration 40, loss = 0.26800381\n",
      "Iteration 41, loss = 0.25236385\n",
      "Iteration 42, loss = 0.33248834\n",
      "Iteration 43, loss = 0.32248069\n",
      "Iteration 44, loss = 0.24877784\n",
      "Iteration 45, loss = 0.31429716\n",
      "Iteration 46, loss = 0.40690961\n",
      "Iteration 47, loss = 0.35728194\n",
      "Iteration 48, loss = 0.30261788\n",
      "Iteration 49, loss = 0.33759216\n",
      "Iteration 50, loss = 0.30962694\n",
      "Iteration 51, loss = 0.40143742\n",
      "Iteration 52, loss = 0.53855943\n",
      "Iteration 53, loss = 0.51916745\n",
      "Iteration 54, loss = 0.43830304\n",
      "Iteration 55, loss = 0.41621235\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 56, loss = 0.30708448\n",
      "Iteration 57, loss = 0.23032436\n",
      "Iteration 58, loss = 0.22723604\n",
      "Iteration 59, loss = 0.20927959\n",
      "Iteration 60, loss = 0.20326649\n",
      "Iteration 61, loss = 0.20118714\n",
      "Iteration 62, loss = 0.18803429\n",
      "Iteration 63, loss = 0.18201343\n",
      "Iteration 64, loss = 0.17803864\n",
      "Iteration 65, loss = 0.17869556\n",
      "Iteration 66, loss = 0.17607701\n",
      "Iteration 67, loss = 0.17036910\n",
      "Iteration 68, loss = 0.16921988\n",
      "Iteration 69, loss = 0.15682373\n",
      "Iteration 70, loss = 0.16223392\n",
      "Iteration 71, loss = 0.14719582\n",
      "Iteration 72, loss = 0.15373657\n",
      "Iteration 73, loss = 0.15320666\n",
      "Iteration 74, loss = 0.14914619\n",
      "Iteration 75, loss = 0.14364979\n",
      "Iteration 76, loss = 0.14999727\n",
      "Iteration 77, loss = 0.14555852\n",
      "Iteration 78, loss = 0.14225622\n",
      "Iteration 79, loss = 0.14216892\n",
      "Iteration 80, loss = 0.13786805\n",
      "Iteration 81, loss = 0.14145822\n",
      "Iteration 82, loss = 0.13533365\n",
      "Iteration 83, loss = 0.13929019\n",
      "Iteration 84, loss = 0.13071150\n",
      "Iteration 85, loss = 0.13049590\n",
      "Iteration 86, loss = 0.13099330\n",
      "Iteration 87, loss = 0.13080616\n",
      "Iteration 88, loss = 0.13290848\n",
      "Iteration 89, loss = 0.13047867\n",
      "Iteration 90, loss = 0.12668163\n",
      "Iteration 91, loss = 0.12499884\n",
      "Iteration 92, loss = 0.12428035\n",
      "Iteration 93, loss = 0.12315682\n",
      "Iteration 94, loss = 0.12150808\n",
      "Iteration 95, loss = 0.12023317\n",
      "Iteration 96, loss = 0.11931962\n",
      "Iteration 97, loss = 0.12045676\n",
      "Iteration 98, loss = 0.12076907\n",
      "Iteration 99, loss = 0.11649314\n",
      "Iteration 100, loss = 0.11892655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.64452073\n",
      "Iteration 2, loss = 0.94557288\n",
      "Iteration 3, loss = 0.73960651\n",
      "Iteration 4, loss = 0.72208922\n",
      "Iteration 5, loss = 0.56453420\n",
      "Iteration 6, loss = 0.53269947\n",
      "Iteration 7, loss = 0.52871107\n",
      "Iteration 8, loss = 0.59519024\n",
      "Iteration 9, loss = 0.40418906\n",
      "Iteration 10, loss = 0.46884538\n",
      "Iteration 11, loss = 0.40882189\n",
      "Iteration 12, loss = 0.37854546\n",
      "Iteration 13, loss = 0.58731421\n",
      "Iteration 14, loss = 0.50333452\n",
      "Iteration 15, loss = 0.30358594\n",
      "Iteration 16, loss = 0.30633081\n",
      "Iteration 17, loss = 0.39614175\n",
      "Iteration 18, loss = 0.34962529\n",
      "Iteration 19, loss = 0.39213714\n",
      "Iteration 20, loss = 0.53211819\n",
      "Iteration 21, loss = 0.53133663\n",
      "Iteration 22, loss = 0.38273301\n",
      "Iteration 23, loss = 0.30921951\n",
      "Iteration 24, loss = 0.33313838\n",
      "Iteration 25, loss = 0.39816748\n",
      "Iteration 26, loss = 0.33097009\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 27, loss = 0.28898523\n",
      "Iteration 28, loss = 0.25132024\n",
      "Iteration 29, loss = 0.25115629\n",
      "Iteration 30, loss = 0.25245007\n",
      "Iteration 31, loss = 0.21373551\n",
      "Iteration 32, loss = 0.22135063\n",
      "Iteration 33, loss = 0.19575367\n",
      "Iteration 34, loss = 0.18131357\n",
      "Iteration 35, loss = 0.18297339\n",
      "Iteration 36, loss = 0.16366626\n",
      "Iteration 37, loss = 0.16323261\n",
      "Iteration 38, loss = 0.15433976\n",
      "Iteration 39, loss = 0.15262737\n",
      "Iteration 40, loss = 0.14227537\n",
      "Iteration 41, loss = 0.12567792\n",
      "Iteration 42, loss = 0.11419559\n",
      "Iteration 43, loss = 0.10992286\n",
      "Iteration 44, loss = 0.11408097\n",
      "Iteration 45, loss = 0.10586498\n",
      "Iteration 46, loss = 0.10268474\n",
      "Iteration 47, loss = 0.10472366\n",
      "Iteration 48, loss = 0.10032021\n",
      "Iteration 49, loss = 0.09928044\n",
      "Iteration 50, loss = 0.09840354\n",
      "Iteration 51, loss = 0.09784228\n",
      "Iteration 52, loss = 0.09721223\n",
      "Iteration 53, loss = 0.09701918\n",
      "Iteration 54, loss = 0.09628100\n",
      "Iteration 55, loss = 0.09458060\n",
      "Iteration 56, loss = 0.09934264\n",
      "Iteration 57, loss = 0.09608063\n",
      "Iteration 58, loss = 0.09422196\n",
      "Iteration 59, loss = 0.09225546\n",
      "Iteration 60, loss = 0.09106426\n",
      "Iteration 61, loss = 0.09093563\n",
      "Iteration 62, loss = 0.09049387\n",
      "Iteration 63, loss = 0.08968633\n",
      "Iteration 64, loss = 0.08933838\n",
      "Iteration 65, loss = 0.08883529\n",
      "Iteration 66, loss = 0.08828386\n",
      "Iteration 67, loss = 0.08844335\n",
      "Iteration 68, loss = 0.08763545\n",
      "Iteration 69, loss = 0.08822874\n",
      "Iteration 70, loss = 0.08644121\n",
      "Iteration 71, loss = 0.09026201\n",
      "Iteration 72, loss = 0.08783901\n",
      "Iteration 73, loss = 0.08673854\n",
      "Iteration 74, loss = 0.08610838\n",
      "Iteration 75, loss = 0.08552410\n",
      "Iteration 76, loss = 0.08760184\n",
      "Iteration 77, loss = 0.09097909\n",
      "Iteration 78, loss = 0.09520302\n",
      "Iteration 79, loss = 0.09061714\n",
      "Iteration 80, loss = 0.08358315\n",
      "Iteration 81, loss = 0.08325736\n",
      "Iteration 82, loss = 0.08273178\n",
      "Iteration 83, loss = 0.08256831\n",
      "Iteration 84, loss = 0.08254371\n",
      "Iteration 85, loss = 0.08232851\n",
      "Iteration 86, loss = 0.08342954\n",
      "Iteration 87, loss = 0.08208770\n",
      "Iteration 88, loss = 0.08181298\n",
      "Iteration 89, loss = 0.08222901\n",
      "Iteration 90, loss = 0.08147945\n",
      "Iteration 91, loss = 0.08125754\n",
      "Iteration 92, loss = 0.08095344\n",
      "Iteration 93, loss = 0.08088634\n",
      "Iteration 94, loss = 0.08069448\n",
      "Iteration 95, loss = 0.08191469\n",
      "Iteration 96, loss = 0.08118072\n",
      "Iteration 97, loss = 0.07913044\n",
      "Iteration 98, loss = 0.07897064\n",
      "Iteration 99, loss = 0.07917728\n",
      "Iteration 100, loss = 0.07835274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.57631215\n",
      "Iteration 2, loss = 0.92553756\n",
      "Iteration 3, loss = 0.78675499\n",
      "Iteration 4, loss = 0.84464323\n",
      "Iteration 5, loss = 0.72162120\n",
      "Iteration 6, loss = 0.63560612\n",
      "Iteration 7, loss = 0.63149272\n",
      "Iteration 8, loss = 0.49511415\n",
      "Iteration 9, loss = 0.39961590\n",
      "Iteration 10, loss = 0.45114968\n",
      "Iteration 11, loss = 0.43107630\n",
      "Iteration 12, loss = 0.52772683\n",
      "Iteration 13, loss = 0.44189518\n",
      "Iteration 14, loss = 0.35334002\n",
      "Iteration 15, loss = 0.34282945\n",
      "Iteration 16, loss = 0.43071757\n",
      "Iteration 17, loss = 0.39162304\n",
      "Iteration 18, loss = 0.34582021\n",
      "Iteration 19, loss = 0.27250920\n",
      "Iteration 20, loss = 0.30059754\n",
      "Iteration 21, loss = 0.34470608\n",
      "Iteration 22, loss = 0.27149275\n",
      "Iteration 23, loss = 0.29662445\n",
      "Iteration 24, loss = 0.32406734\n",
      "Iteration 25, loss = 0.31226904\n",
      "Iteration 26, loss = 0.32277142\n",
      "Iteration 27, loss = 0.29422295\n",
      "Iteration 28, loss = 0.27932276\n",
      "Iteration 29, loss = 0.26388367\n",
      "Iteration 30, loss = 0.31672632\n",
      "Iteration 31, loss = 0.25103167\n",
      "Iteration 32, loss = 0.30332942\n",
      "Iteration 33, loss = 0.29706017\n",
      "Iteration 34, loss = 0.34071704\n",
      "Iteration 35, loss = 0.26842499\n",
      "Iteration 36, loss = 0.31726240\n",
      "Iteration 37, loss = 0.23508035\n",
      "Iteration 38, loss = 0.26354478\n",
      "Iteration 39, loss = 0.18521639\n",
      "Iteration 40, loss = 0.15556519\n",
      "Iteration 41, loss = 0.16536104\n",
      "Iteration 42, loss = 0.18654561\n",
      "Iteration 43, loss = 0.20628728\n",
      "Iteration 44, loss = 0.18248314\n",
      "Iteration 45, loss = 0.21921926\n",
      "Iteration 46, loss = 0.23905135\n",
      "Iteration 47, loss = 0.20399147\n",
      "Iteration 48, loss = 0.21659051\n",
      "Iteration 49, loss = 0.25509626\n",
      "Iteration 50, loss = 0.29205873\n",
      "Iteration 51, loss = 0.24621181\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 52, loss = 0.23646250\n",
      "Iteration 53, loss = 0.19691720\n",
      "Iteration 54, loss = 0.18039310\n",
      "Iteration 55, loss = 0.16796396\n",
      "Iteration 56, loss = 0.15488659\n",
      "Iteration 57, loss = 0.14133784\n",
      "Iteration 58, loss = 0.13138061\n",
      "Iteration 59, loss = 0.11663282\n",
      "Iteration 60, loss = 0.10475006\n",
      "Iteration 61, loss = 0.09728640\n",
      "Iteration 62, loss = 0.09318172\n",
      "Iteration 63, loss = 0.08711858\n",
      "Iteration 64, loss = 0.08413292\n",
      "Iteration 65, loss = 0.08360404\n",
      "Iteration 66, loss = 0.08352621\n",
      "Iteration 67, loss = 0.08102501\n",
      "Iteration 68, loss = 0.08013539\n",
      "Iteration 69, loss = 0.07981677\n",
      "Iteration 70, loss = 0.07887216\n",
      "Iteration 71, loss = 0.07866141\n",
      "Iteration 72, loss = 0.07765565\n",
      "Iteration 73, loss = 0.07794260\n",
      "Iteration 74, loss = 0.07700407\n",
      "Iteration 75, loss = 0.07621514\n",
      "Iteration 76, loss = 0.07563831\n",
      "Iteration 77, loss = 0.07551370\n",
      "Iteration 78, loss = 0.07507114\n",
      "Iteration 79, loss = 0.07515834\n",
      "Iteration 80, loss = 0.07418098\n",
      "Iteration 81, loss = 0.07542859\n",
      "Iteration 82, loss = 0.07415315\n",
      "Iteration 83, loss = 0.07377984\n",
      "Iteration 84, loss = 0.07337899\n",
      "Iteration 85, loss = 0.07463573\n",
      "Iteration 86, loss = 0.07139179\n",
      "Iteration 87, loss = 0.07062631\n",
      "Iteration 88, loss = 0.07023981\n",
      "Iteration 89, loss = 0.06987134\n",
      "Iteration 90, loss = 0.06968070\n",
      "Iteration 91, loss = 0.06948843\n",
      "Iteration 92, loss = 0.06904649\n",
      "Iteration 93, loss = 0.06887540\n",
      "Iteration 94, loss = 0.06864843\n",
      "Iteration 95, loss = 0.06835925\n",
      "Iteration 96, loss = 0.06827065\n",
      "Iteration 97, loss = 0.06802247\n",
      "Iteration 98, loss = 0.06776541\n",
      "Iteration 99, loss = 0.06759607\n",
      "Iteration 100, loss = 0.06739273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.37109069\n",
      "Iteration 2, loss = 0.77643928\n",
      "Iteration 3, loss = 0.60040125\n",
      "Iteration 4, loss = 0.50382548\n",
      "Iteration 5, loss = 0.56953958\n",
      "Iteration 6, loss = 0.39960900\n",
      "Iteration 7, loss = 0.47667303\n",
      "Iteration 8, loss = 0.40915015\n",
      "Iteration 9, loss = 0.42459999\n",
      "Iteration 10, loss = 0.42005693\n",
      "Iteration 11, loss = 0.37685261\n",
      "Iteration 12, loss = 0.41116318\n",
      "Iteration 13, loss = 0.40268276\n",
      "Iteration 14, loss = 0.42612283\n",
      "Iteration 15, loss = 0.44552559\n",
      "Iteration 16, loss = 0.40194966\n",
      "Iteration 17, loss = 0.27513891\n",
      "Iteration 18, loss = 0.29584408\n",
      "Iteration 19, loss = 0.33906294\n",
      "Iteration 20, loss = 0.36364700\n",
      "Iteration 21, loss = 0.34394672\n",
      "Iteration 22, loss = 0.27283245\n",
      "Iteration 23, loss = 0.24136761\n",
      "Iteration 24, loss = 0.23939375\n",
      "Iteration 25, loss = 0.26177976\n",
      "Iteration 26, loss = 0.30797866\n",
      "Iteration 27, loss = 0.27339539\n",
      "Iteration 28, loss = 0.34722452\n",
      "Iteration 29, loss = 0.30016055\n",
      "Iteration 30, loss = 0.22483710\n",
      "Iteration 31, loss = 0.31450604\n",
      "Iteration 32, loss = 0.34087509\n",
      "Iteration 33, loss = 0.29384964\n",
      "Iteration 34, loss = 0.25487824\n",
      "Iteration 35, loss = 0.27496690\n",
      "Iteration 36, loss = 0.20632940\n",
      "Iteration 37, loss = 0.19773671\n",
      "Iteration 38, loss = 0.24730690\n",
      "Iteration 39, loss = 0.18033338\n",
      "Iteration 40, loss = 0.24528510\n",
      "Iteration 41, loss = 0.18267590\n",
      "Iteration 42, loss = 0.22343690\n",
      "Iteration 43, loss = 0.22835375\n",
      "Iteration 44, loss = 0.19379816\n",
      "Iteration 45, loss = 0.20845145\n",
      "Iteration 46, loss = 0.25848361\n",
      "Iteration 47, loss = 0.18069781\n",
      "Iteration 48, loss = 0.13960930\n",
      "Iteration 49, loss = 0.18148276\n",
      "Iteration 50, loss = 0.23407029\n",
      "Iteration 51, loss = 0.28245220\n",
      "Iteration 52, loss = 0.19063417\n",
      "Iteration 53, loss = 0.19149458\n",
      "Iteration 54, loss = 0.20765887\n",
      "Iteration 55, loss = 0.21461203\n",
      "Iteration 56, loss = 0.24368643\n",
      "Iteration 57, loss = 0.18594645\n",
      "Iteration 58, loss = 0.13585518\n",
      "Iteration 59, loss = 0.11067234\n",
      "Iteration 60, loss = 0.14960038\n",
      "Iteration 61, loss = 0.18176043\n",
      "Iteration 62, loss = 0.18744840\n",
      "Iteration 63, loss = 0.15368510\n",
      "Iteration 64, loss = 0.15042186\n",
      "Iteration 65, loss = 0.10371314\n",
      "Iteration 66, loss = 0.12749391\n",
      "Iteration 67, loss = 0.16065289\n",
      "Iteration 68, loss = 0.16725851\n",
      "Iteration 69, loss = 0.14327338\n",
      "Iteration 70, loss = 0.15731963\n",
      "Iteration 71, loss = 0.11881646\n",
      "Iteration 72, loss = 0.12790706\n",
      "Iteration 73, loss = 0.17515952\n",
      "Iteration 74, loss = 0.14286514\n",
      "Iteration 75, loss = 0.16614599\n",
      "Iteration 76, loss = 0.23947106\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 77, loss = 0.18207530\n",
      "Iteration 78, loss = 0.15749729\n",
      "Iteration 79, loss = 0.14520896\n",
      "Iteration 80, loss = 0.12441257\n",
      "Iteration 81, loss = 0.12761077\n",
      "Iteration 82, loss = 0.11334316\n",
      "Iteration 83, loss = 0.10029310\n",
      "Iteration 84, loss = 0.08778950\n",
      "Iteration 85, loss = 0.08638429\n",
      "Iteration 86, loss = 0.08530828\n",
      "Iteration 87, loss = 0.08302582\n",
      "Iteration 88, loss = 0.08160346\n",
      "Iteration 89, loss = 0.07997801\n",
      "Iteration 90, loss = 0.07841271\n",
      "Iteration 91, loss = 0.07870577\n",
      "Iteration 92, loss = 0.07594867\n",
      "Iteration 93, loss = 0.07549051\n",
      "Iteration 94, loss = 0.07501297\n",
      "Iteration 95, loss = 0.07445863\n",
      "Iteration 96, loss = 0.07398796\n",
      "Iteration 97, loss = 0.07278936\n",
      "Iteration 98, loss = 0.07224027\n",
      "Iteration 99, loss = 0.06971013\n",
      "Iteration 100, loss = 0.06869503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.50159789\n",
      "Iteration 2, loss = 0.83876277\n",
      "Iteration 3, loss = 0.72895329\n",
      "Iteration 4, loss = 0.50830384\n",
      "Iteration 5, loss = 0.56245352\n",
      "Iteration 6, loss = 0.42586528\n",
      "Iteration 7, loss = 0.41772400\n",
      "Iteration 8, loss = 0.46047019\n",
      "Iteration 9, loss = 0.35601454\n",
      "Iteration 10, loss = 0.31551115\n",
      "Iteration 11, loss = 0.32185600\n",
      "Iteration 12, loss = 0.36458543\n",
      "Iteration 13, loss = 0.41754758\n",
      "Iteration 14, loss = 0.30722667\n",
      "Iteration 15, loss = 0.33365456\n",
      "Iteration 16, loss = 0.40827570\n",
      "Iteration 17, loss = 0.37337625\n",
      "Iteration 18, loss = 0.30654627\n",
      "Iteration 19, loss = 0.26014493\n",
      "Iteration 20, loss = 0.29226126\n",
      "Iteration 21, loss = 0.26833758\n",
      "Iteration 22, loss = 0.28452003\n",
      "Iteration 23, loss = 0.26916421\n",
      "Iteration 24, loss = 0.36313005\n",
      "Iteration 25, loss = 0.43357031\n",
      "Iteration 26, loss = 0.35496482\n",
      "Iteration 27, loss = 0.31714353\n",
      "Iteration 28, loss = 0.19438037\n",
      "Iteration 29, loss = 0.41344966\n",
      "Iteration 30, loss = 0.32078267\n",
      "Iteration 31, loss = 0.28057044\n",
      "Iteration 32, loss = 0.23346509\n",
      "Iteration 33, loss = 0.21383403\n",
      "Iteration 34, loss = 0.19253756\n",
      "Iteration 35, loss = 0.21764530\n",
      "Iteration 36, loss = 0.25748220\n",
      "Iteration 37, loss = 0.26857928\n",
      "Iteration 38, loss = 0.23360205\n",
      "Iteration 39, loss = 0.22181849\n",
      "Iteration 40, loss = 0.23076527\n",
      "Iteration 41, loss = 0.22664644\n",
      "Iteration 42, loss = 0.17991603\n",
      "Iteration 43, loss = 0.26411456\n",
      "Iteration 44, loss = 0.23985654\n",
      "Iteration 45, loss = 0.24728983\n",
      "Iteration 46, loss = 0.21161115\n",
      "Iteration 47, loss = 0.17063736\n",
      "Iteration 48, loss = 0.16275140\n",
      "Iteration 49, loss = 0.14407887\n",
      "Iteration 50, loss = 0.12196401\n",
      "Iteration 51, loss = 0.17514729\n",
      "Iteration 52, loss = 0.13087934\n",
      "Iteration 53, loss = 0.15066893\n",
      "Iteration 54, loss = 0.17383859\n",
      "Iteration 55, loss = 0.15610700\n",
      "Iteration 56, loss = 0.25636090\n",
      "Iteration 57, loss = 0.24524295\n",
      "Iteration 58, loss = 0.21567331\n",
      "Iteration 59, loss = 0.16885262\n",
      "Iteration 60, loss = 0.22505381\n",
      "Iteration 61, loss = 0.19997274\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 62, loss = 0.12497680\n",
      "Iteration 63, loss = 0.10564487\n",
      "Iteration 64, loss = 0.09907438\n",
      "Iteration 65, loss = 0.10654789\n",
      "Iteration 66, loss = 0.09651911\n",
      "Iteration 67, loss = 0.09343770\n",
      "Iteration 68, loss = 0.08547949\n",
      "Iteration 69, loss = 0.08476155\n",
      "Iteration 70, loss = 0.08032178\n",
      "Iteration 71, loss = 0.08082170\n",
      "Iteration 72, loss = 0.07623024\n",
      "Iteration 73, loss = 0.07566054\n",
      "Iteration 74, loss = 0.07559846\n",
      "Iteration 75, loss = 0.07396428\n",
      "Iteration 76, loss = 0.07361048\n",
      "Iteration 77, loss = 0.07011841\n",
      "Iteration 78, loss = 0.06909705\n",
      "Iteration 79, loss = 0.07070583\n",
      "Iteration 80, loss = 0.06861033\n",
      "Iteration 81, loss = 0.06789759\n",
      "Iteration 82, loss = 0.06856354\n",
      "Iteration 83, loss = 0.06746157\n",
      "Iteration 84, loss = 0.06677921\n",
      "Iteration 85, loss = 0.06442136\n",
      "Iteration 86, loss = 0.06576152\n",
      "Iteration 87, loss = 0.06238874\n",
      "Iteration 88, loss = 0.06187091\n",
      "Iteration 89, loss = 0.06166273\n",
      "Iteration 90, loss = 0.06125643\n",
      "Iteration 91, loss = 0.06076301\n",
      "Iteration 92, loss = 0.06030316\n",
      "Iteration 93, loss = 0.05989935\n",
      "Iteration 94, loss = 0.05941280\n",
      "Iteration 95, loss = 0.05918585\n",
      "Iteration 96, loss = 0.05885385\n",
      "Iteration 97, loss = 0.05860167\n",
      "Iteration 98, loss = 0.05739176\n",
      "Iteration 99, loss = 0.05819048\n",
      "Iteration 100, loss = 0.05614233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.32007333\n",
      "Iteration 2, loss = 0.70943535\n",
      "Iteration 3, loss = 0.57987638\n",
      "Iteration 4, loss = 0.49752611\n",
      "Iteration 5, loss = 0.37991117\n",
      "Iteration 6, loss = 0.47734070\n",
      "Iteration 7, loss = 0.46608802\n",
      "Iteration 8, loss = 0.40150095\n",
      "Iteration 9, loss = 0.35616788\n",
      "Iteration 10, loss = 0.32970689\n",
      "Iteration 11, loss = 0.36546828\n",
      "Iteration 12, loss = 0.34652813\n",
      "Iteration 13, loss = 0.28832215\n",
      "Iteration 14, loss = 0.34330485\n",
      "Iteration 15, loss = 0.33542296\n",
      "Iteration 16, loss = 0.33405708\n",
      "Iteration 17, loss = 0.37064937\n",
      "Iteration 18, loss = 0.26134739\n",
      "Iteration 19, loss = 0.25232544\n",
      "Iteration 20, loss = 0.19897936\n",
      "Iteration 21, loss = 0.22165584\n",
      "Iteration 22, loss = 0.31650489\n",
      "Iteration 23, loss = 0.26790321\n",
      "Iteration 24, loss = 0.29131144\n",
      "Iteration 25, loss = 0.27840832\n",
      "Iteration 26, loss = 0.29610405\n",
      "Iteration 27, loss = 0.25018349\n",
      "Iteration 28, loss = 0.25499068\n",
      "Iteration 29, loss = 0.26627069\n",
      "Iteration 30, loss = 0.23397907\n",
      "Iteration 31, loss = 0.22169344\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 32, loss = 0.18718468\n",
      "Iteration 33, loss = 0.16655907\n",
      "Iteration 34, loss = 0.14865560\n",
      "Iteration 35, loss = 0.13807100\n",
      "Iteration 36, loss = 0.12359550\n",
      "Iteration 37, loss = 0.11084795\n",
      "Iteration 38, loss = 0.10298412\n",
      "Iteration 39, loss = 0.09830189\n",
      "Iteration 40, loss = 0.09571543\n",
      "Iteration 41, loss = 0.09330379\n",
      "Iteration 42, loss = 0.09156625\n",
      "Iteration 43, loss = 0.09058546\n",
      "Iteration 44, loss = 0.08900173\n",
      "Iteration 45, loss = 0.08786904\n",
      "Iteration 46, loss = 0.08721525\n",
      "Iteration 47, loss = 0.08621241\n",
      "Iteration 48, loss = 0.08503636\n",
      "Iteration 49, loss = 0.08418319\n",
      "Iteration 50, loss = 0.08309439\n",
      "Iteration 51, loss = 0.08196485\n",
      "Iteration 52, loss = 0.08098052\n",
      "Iteration 53, loss = 0.08035959\n",
      "Iteration 54, loss = 0.07973097\n",
      "Iteration 55, loss = 0.07909584\n",
      "Iteration 56, loss = 0.07755915\n",
      "Iteration 57, loss = 0.07868890\n",
      "Iteration 58, loss = 0.07506463\n",
      "Iteration 59, loss = 0.07394505\n",
      "Iteration 60, loss = 0.07211987\n",
      "Iteration 61, loss = 0.07389618\n",
      "Iteration 62, loss = 0.07184778\n",
      "Iteration 63, loss = 0.07094661\n",
      "Iteration 64, loss = 0.07031456\n",
      "Iteration 65, loss = 0.06970304\n",
      "Iteration 66, loss = 0.06939248\n",
      "Iteration 67, loss = 0.06847448\n",
      "Iteration 68, loss = 0.06859864\n",
      "Iteration 69, loss = 0.06779404\n",
      "Iteration 70, loss = 0.06736873\n",
      "Iteration 71, loss = 0.06690237\n",
      "Iteration 72, loss = 0.06655828\n",
      "Iteration 73, loss = 0.06631909\n",
      "Iteration 74, loss = 0.06590798\n",
      "Iteration 75, loss = 0.06566594\n",
      "Iteration 76, loss = 0.06531579\n",
      "Iteration 77, loss = 0.06520133\n",
      "Iteration 78, loss = 0.06477307\n",
      "Iteration 79, loss = 0.06456447\n",
      "Iteration 80, loss = 0.06436792\n",
      "Iteration 81, loss = 0.06412902\n",
      "Iteration 82, loss = 0.06383967\n",
      "Iteration 83, loss = 0.06353673\n",
      "Iteration 84, loss = 0.06334299\n",
      "Iteration 85, loss = 0.06305873\n",
      "Iteration 86, loss = 0.06280862\n",
      "Iteration 87, loss = 0.06250190\n",
      "Iteration 88, loss = 0.06227512\n",
      "Iteration 89, loss = 0.06205342\n",
      "Iteration 90, loss = 0.06190302\n",
      "Iteration 91, loss = 0.06168565\n",
      "Iteration 92, loss = 0.06142079\n",
      "Iteration 93, loss = 0.06119543\n",
      "Iteration 94, loss = 0.06096147\n",
      "Iteration 95, loss = 0.06074661\n",
      "Iteration 96, loss = 0.06051631\n",
      "Iteration 97, loss = 0.06026494\n",
      "Iteration 98, loss = 0.06007824\n",
      "Iteration 99, loss = 0.05988896\n",
      "Iteration 100, loss = 0.05963303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.39047706\n",
      "Iteration 2, loss = 0.77314886\n",
      "Iteration 3, loss = 0.52609806\n",
      "Iteration 4, loss = 0.42227803\n",
      "Iteration 5, loss = 0.38274924\n",
      "Iteration 6, loss = 0.42462820\n",
      "Iteration 7, loss = 0.42011264\n",
      "Iteration 8, loss = 0.28931181\n",
      "Iteration 9, loss = 0.34163842\n",
      "Iteration 10, loss = 0.34894013\n",
      "Iteration 11, loss = 0.33287019\n",
      "Iteration 12, loss = 0.25979735\n",
      "Iteration 13, loss = 0.32010180\n",
      "Iteration 14, loss = 0.28574972\n",
      "Iteration 15, loss = 0.24882094\n",
      "Iteration 16, loss = 0.25888815\n",
      "Iteration 17, loss = 0.26350423\n",
      "Iteration 18, loss = 0.30327500\n",
      "Iteration 19, loss = 0.28388351\n",
      "Iteration 20, loss = 0.28244820\n",
      "Iteration 21, loss = 0.27741303\n",
      "Iteration 22, loss = 0.25992323\n",
      "Iteration 23, loss = 0.26244151\n",
      "Iteration 24, loss = 0.22961590\n",
      "Iteration 25, loss = 0.21785294\n",
      "Iteration 26, loss = 0.31060993\n",
      "Iteration 27, loss = 0.27110569\n",
      "Iteration 28, loss = 0.29798031\n",
      "Iteration 29, loss = 0.17217631\n",
      "Iteration 30, loss = 0.21224199\n",
      "Iteration 31, loss = 0.21137007\n",
      "Iteration 32, loss = 0.16396203\n",
      "Iteration 33, loss = 0.16321050\n",
      "Iteration 34, loss = 0.16931607\n",
      "Iteration 35, loss = 0.17678326\n",
      "Iteration 36, loss = 0.18800404\n",
      "Iteration 37, loss = 0.14159197\n",
      "Iteration 38, loss = 0.13692727\n",
      "Iteration 39, loss = 0.11924466\n",
      "Iteration 40, loss = 0.12271736\n",
      "Iteration 41, loss = 0.13179242\n",
      "Iteration 42, loss = 0.14718906\n",
      "Iteration 43, loss = 0.13121442\n",
      "Iteration 44, loss = 0.18550452\n",
      "Iteration 45, loss = 0.12757438\n",
      "Iteration 46, loss = 0.13555403\n",
      "Iteration 47, loss = 0.14247735\n",
      "Iteration 48, loss = 0.19528684\n",
      "Iteration 49, loss = 0.15868307\n",
      "Iteration 50, loss = 0.14545466\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 51, loss = 0.11657022\n",
      "Iteration 52, loss = 0.08120439\n",
      "Iteration 53, loss = 0.07599614\n",
      "Iteration 54, loss = 0.07163478\n",
      "Iteration 55, loss = 0.06622008\n",
      "Iteration 56, loss = 0.06317719\n",
      "Iteration 57, loss = 0.06159181\n",
      "Iteration 58, loss = 0.06119727\n",
      "Iteration 59, loss = 0.05900238\n",
      "Iteration 60, loss = 0.05684124\n",
      "Iteration 61, loss = 0.05548472\n",
      "Iteration 62, loss = 0.05283762\n",
      "Iteration 63, loss = 0.05202319\n",
      "Iteration 64, loss = 0.05119453\n",
      "Iteration 65, loss = 0.05056861\n",
      "Iteration 66, loss = 0.05126731\n",
      "Iteration 67, loss = 0.04765711\n",
      "Iteration 68, loss = 0.04647020\n",
      "Iteration 69, loss = 0.04535577\n",
      "Iteration 70, loss = 0.04472947\n",
      "Iteration 71, loss = 0.04593302\n",
      "Iteration 72, loss = 0.04390705\n",
      "Iteration 73, loss = 0.04323990\n",
      "Iteration 74, loss = 0.04271888\n",
      "Iteration 75, loss = 0.04243419\n",
      "Iteration 76, loss = 0.04204579\n",
      "Iteration 77, loss = 0.04164969\n",
      "Iteration 78, loss = 0.04137884\n",
      "Iteration 79, loss = 0.04108986\n",
      "Iteration 80, loss = 0.04076858\n",
      "Iteration 81, loss = 0.04050539\n",
      "Iteration 82, loss = 0.04022823\n",
      "Iteration 83, loss = 0.04006017\n",
      "Iteration 84, loss = 0.03977790\n",
      "Iteration 85, loss = 0.03952649\n",
      "Iteration 86, loss = 0.03929982\n",
      "Iteration 87, loss = 0.03906271\n",
      "Iteration 88, loss = 0.03885983\n",
      "Iteration 89, loss = 0.03862828\n",
      "Iteration 90, loss = 0.03840495\n",
      "Iteration 91, loss = 0.03826663\n",
      "Iteration 92, loss = 0.03803092\n",
      "Iteration 93, loss = 0.03782786\n",
      "Iteration 94, loss = 0.03764757\n",
      "Iteration 95, loss = 0.03747603\n",
      "Iteration 96, loss = 0.03736784\n",
      "Iteration 97, loss = 0.03712960\n",
      "Iteration 98, loss = 0.03692265\n",
      "Iteration 99, loss = 0.03668383\n",
      "Iteration 100, loss = 0.04111278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.37813680\n",
      "Iteration 2, loss = 0.70637947\n",
      "Iteration 3, loss = 0.59924852\n",
      "Iteration 4, loss = 0.41118380\n",
      "Iteration 5, loss = 0.55535756\n",
      "Iteration 6, loss = 0.49415333\n",
      "Iteration 7, loss = 0.41303532\n",
      "Iteration 8, loss = 0.35485102\n",
      "Iteration 9, loss = 0.32056615\n",
      "Iteration 10, loss = 0.26630021\n",
      "Iteration 11, loss = 0.27340326\n",
      "Iteration 12, loss = 0.30263834\n",
      "Iteration 13, loss = 0.21357101\n",
      "Iteration 14, loss = 0.19098362\n",
      "Iteration 15, loss = 0.30973375\n",
      "Iteration 16, loss = 0.29208119\n",
      "Iteration 17, loss = 0.29907932\n",
      "Iteration 18, loss = 0.30991959\n",
      "Iteration 19, loss = 0.18140119\n",
      "Iteration 20, loss = 0.20929592\n",
      "Iteration 21, loss = 0.18621632\n",
      "Iteration 22, loss = 0.21203079\n",
      "Iteration 23, loss = 0.16529175\n",
      "Iteration 24, loss = 0.21191841\n",
      "Iteration 25, loss = 0.37428988\n",
      "Iteration 26, loss = 0.32037706\n",
      "Iteration 27, loss = 0.21462060\n",
      "Iteration 28, loss = 0.37033293\n",
      "Iteration 29, loss = 0.41752333\n",
      "Iteration 30, loss = 0.27484924\n",
      "Iteration 31, loss = 0.35422056\n",
      "Iteration 32, loss = 0.34786840\n",
      "Iteration 33, loss = 0.25664467\n",
      "Iteration 34, loss = 0.23418962\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 35, loss = 0.15270692\n",
      "Iteration 36, loss = 0.12596725\n",
      "Iteration 37, loss = 0.11586084\n",
      "Iteration 38, loss = 0.11388105\n",
      "Iteration 39, loss = 0.10627255\n",
      "Iteration 40, loss = 0.10554972\n",
      "Iteration 41, loss = 0.10013585\n",
      "Iteration 42, loss = 0.10225810\n",
      "Iteration 43, loss = 0.09921943\n",
      "Iteration 44, loss = 0.09349477\n",
      "Iteration 45, loss = 0.09350132\n",
      "Iteration 46, loss = 0.09019961\n",
      "Iteration 47, loss = 0.08791161\n",
      "Iteration 48, loss = 0.08585535\n",
      "Iteration 49, loss = 0.08468750\n",
      "Iteration 50, loss = 0.08363748\n",
      "Iteration 51, loss = 0.08300373\n",
      "Iteration 52, loss = 0.08205243\n",
      "Iteration 53, loss = 0.08141839\n",
      "Iteration 54, loss = 0.08145333\n",
      "Iteration 55, loss = 0.07996058\n",
      "Iteration 56, loss = 0.07892836\n",
      "Iteration 57, loss = 0.07787044\n",
      "Iteration 58, loss = 0.07737027\n",
      "Iteration 59, loss = 0.07671466\n",
      "Iteration 60, loss = 0.07610216\n",
      "Iteration 61, loss = 0.07539958\n",
      "Iteration 62, loss = 0.07480351\n",
      "Iteration 63, loss = 0.07426422\n",
      "Iteration 64, loss = 0.07370791\n",
      "Iteration 65, loss = 0.07305188\n",
      "Iteration 66, loss = 0.07268059\n",
      "Iteration 67, loss = 0.07207883\n",
      "Iteration 68, loss = 0.07155504\n",
      "Iteration 69, loss = 0.07106101\n",
      "Iteration 70, loss = 0.07074431\n",
      "Iteration 71, loss = 0.07024636\n",
      "Iteration 72, loss = 0.06991695\n",
      "Iteration 73, loss = 0.06952817\n",
      "Iteration 74, loss = 0.06917693\n",
      "Iteration 75, loss = 0.06887815\n",
      "Iteration 76, loss = 0.06847733\n",
      "Iteration 77, loss = 0.06798555\n",
      "Iteration 78, loss = 0.06759776\n",
      "Iteration 79, loss = 0.06740507\n",
      "Iteration 80, loss = 0.06702654\n",
      "Iteration 81, loss = 0.06679953\n",
      "Iteration 82, loss = 0.06651751\n",
      "Iteration 83, loss = 0.06617445\n",
      "Iteration 84, loss = 0.06591049\n",
      "Iteration 85, loss = 0.06543519\n",
      "Iteration 86, loss = 0.06504049\n",
      "Iteration 87, loss = 0.06488837\n",
      "Iteration 88, loss = 0.06456627\n",
      "Iteration 89, loss = 0.06421143\n",
      "Iteration 90, loss = 0.06382042\n",
      "Iteration 91, loss = 0.06341734\n",
      "Iteration 92, loss = 0.06314626\n",
      "Iteration 93, loss = 0.06276068\n",
      "Iteration 94, loss = 0.06239675\n",
      "Iteration 95, loss = 0.06213477\n",
      "Iteration 96, loss = 0.06192628\n",
      "Iteration 97, loss = 0.06163745\n",
      "Iteration 98, loss = 0.06148691\n",
      "Iteration 99, loss = 0.06124320\n",
      "Iteration 100, loss = 0.06104313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.33516602\n",
      "Iteration 2, loss = 0.74404201\n",
      "Iteration 3, loss = 0.61892025\n",
      "Iteration 4, loss = 0.50074860\n",
      "Iteration 5, loss = 0.53608930\n",
      "Iteration 6, loss = 0.39799727\n",
      "Iteration 7, loss = 0.39711269\n",
      "Iteration 8, loss = 0.33248705\n",
      "Iteration 9, loss = 0.32211581\n",
      "Iteration 10, loss = 0.32211067\n",
      "Iteration 11, loss = 0.39409017\n",
      "Iteration 12, loss = 0.28465033\n",
      "Iteration 13, loss = 0.25673647\n",
      "Iteration 14, loss = 0.27173306\n",
      "Iteration 15, loss = 0.25285409\n",
      "Iteration 16, loss = 0.30243502\n",
      "Iteration 17, loss = 0.27265970\n",
      "Iteration 18, loss = 0.32524923\n",
      "Iteration 19, loss = 0.23512822\n",
      "Iteration 20, loss = 0.22437978\n",
      "Iteration 21, loss = 0.18154755\n",
      "Iteration 22, loss = 0.22207043\n",
      "Iteration 23, loss = 0.25278881\n",
      "Iteration 24, loss = 0.21416457\n",
      "Iteration 25, loss = 0.24333888\n",
      "Iteration 26, loss = 0.20554969\n",
      "Iteration 27, loss = 0.19696417\n",
      "Iteration 28, loss = 0.23242203\n",
      "Iteration 29, loss = 0.17951273\n",
      "Iteration 30, loss = 0.22553750\n",
      "Iteration 31, loss = 0.26246138\n",
      "Iteration 32, loss = 0.23671838\n",
      "Iteration 33, loss = 0.23073343\n",
      "Iteration 34, loss = 0.21634381\n",
      "Iteration 35, loss = 0.24637354\n",
      "Iteration 36, loss = 0.23688514\n",
      "Iteration 37, loss = 0.22458776\n",
      "Iteration 38, loss = 0.21058667\n",
      "Iteration 39, loss = 0.25875918\n",
      "Iteration 40, loss = 0.19874918\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 41, loss = 0.13533588\n",
      "Iteration 42, loss = 0.11965396\n",
      "Iteration 43, loss = 0.09666221\n",
      "Iteration 44, loss = 0.10063443\n",
      "Iteration 45, loss = 0.08789717\n",
      "Iteration 46, loss = 0.08542495\n",
      "Iteration 47, loss = 0.08691887\n",
      "Iteration 48, loss = 0.08125232\n",
      "Iteration 49, loss = 0.08024866\n",
      "Iteration 50, loss = 0.07536919\n",
      "Iteration 51, loss = 0.07038847\n",
      "Iteration 52, loss = 0.07107173\n",
      "Iteration 53, loss = 0.06599604\n",
      "Iteration 54, loss = 0.06555883\n",
      "Iteration 55, loss = 0.06501232\n",
      "Iteration 56, loss = 0.06403530\n",
      "Iteration 57, loss = 0.06172690\n",
      "Iteration 58, loss = 0.06211673\n",
      "Iteration 59, loss = 0.05992184\n",
      "Iteration 60, loss = 0.05983266\n",
      "Iteration 61, loss = 0.05738211\n",
      "Iteration 62, loss = 0.05628229\n",
      "Iteration 63, loss = 0.05560732\n",
      "Iteration 64, loss = 0.05478991\n",
      "Iteration 65, loss = 0.05406722\n",
      "Iteration 66, loss = 0.05372782\n",
      "Iteration 67, loss = 0.05338033\n",
      "Iteration 68, loss = 0.05304423\n",
      "Iteration 69, loss = 0.05271104\n",
      "Iteration 70, loss = 0.05242448\n",
      "Iteration 71, loss = 0.05208812\n",
      "Iteration 72, loss = 0.05181176\n",
      "Iteration 73, loss = 0.05152842\n",
      "Iteration 74, loss = 0.05120049\n",
      "Iteration 75, loss = 0.05094204\n",
      "Iteration 76, loss = 0.05067669\n",
      "Iteration 77, loss = 0.05045071\n",
      "Iteration 78, loss = 0.05019206\n",
      "Iteration 79, loss = 0.04996010\n",
      "Iteration 80, loss = 0.04967760\n",
      "Iteration 81, loss = 0.04947927\n",
      "Iteration 82, loss = 0.04925759\n",
      "Iteration 83, loss = 0.04902319\n",
      "Iteration 84, loss = 0.04880714\n",
      "Iteration 85, loss = 0.04860469\n",
      "Iteration 86, loss = 0.04834263\n",
      "Iteration 87, loss = 0.04813864\n",
      "Iteration 88, loss = 0.04786669\n",
      "Iteration 89, loss = 0.04762350\n",
      "Iteration 90, loss = 0.04742800\n",
      "Iteration 91, loss = 0.04721977\n",
      "Iteration 92, loss = 0.04703348\n",
      "Iteration 93, loss = 0.04684614\n",
      "Iteration 94, loss = 0.04669196\n",
      "Iteration 95, loss = 0.04652119\n",
      "Iteration 96, loss = 0.04630995\n",
      "Iteration 97, loss = 0.04612424\n",
      "Iteration 98, loss = 0.04597603\n",
      "Iteration 99, loss = 0.04577485\n",
      "Iteration 100, loss = 0.04563712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.31334354\n",
      "Iteration 2, loss = 0.65897086\n",
      "Iteration 3, loss = 0.52287019\n",
      "Iteration 4, loss = 0.46605611\n",
      "Iteration 5, loss = 0.42601349\n",
      "Iteration 6, loss = 0.40894751\n",
      "Iteration 7, loss = 0.40482406\n",
      "Iteration 8, loss = 0.39167640\n",
      "Iteration 9, loss = 0.36220541\n",
      "Iteration 10, loss = 0.38000666\n",
      "Iteration 11, loss = 0.44104445\n",
      "Iteration 12, loss = 0.41307970\n",
      "Iteration 13, loss = 0.35130745\n",
      "Iteration 14, loss = 0.35979367\n",
      "Iteration 15, loss = 0.25509942\n",
      "Iteration 16, loss = 0.26178865\n",
      "Iteration 17, loss = 0.26397857\n",
      "Iteration 18, loss = 0.37443501\n",
      "Iteration 19, loss = 0.43787732\n",
      "Iteration 20, loss = 0.42159018\n",
      "Iteration 21, loss = 0.32238759\n",
      "Iteration 22, loss = 0.37752730\n",
      "Iteration 23, loss = 0.25040347\n",
      "Iteration 24, loss = 0.23464884\n",
      "Iteration 25, loss = 0.18156910\n",
      "Iteration 26, loss = 0.20090486\n",
      "Iteration 27, loss = 0.21812950\n",
      "Iteration 28, loss = 0.28736966\n",
      "Iteration 29, loss = 0.22520818\n",
      "Iteration 30, loss = 0.20543152\n",
      "Iteration 31, loss = 0.20634777\n",
      "Iteration 32, loss = 0.26147456\n",
      "Iteration 33, loss = 0.20229514\n",
      "Iteration 34, loss = 0.22036825\n",
      "Iteration 35, loss = 0.14868684\n",
      "Iteration 36, loss = 0.14623535\n",
      "Iteration 37, loss = 0.13550848\n",
      "Iteration 38, loss = 0.13809035\n",
      "Iteration 39, loss = 0.14482777\n",
      "Iteration 40, loss = 0.18042852\n",
      "Iteration 41, loss = 0.18443179\n",
      "Iteration 42, loss = 0.16927528\n",
      "Iteration 43, loss = 0.12422921\n",
      "Iteration 44, loss = 0.25727045\n",
      "Iteration 45, loss = 0.21820127\n",
      "Iteration 46, loss = 0.18635123\n",
      "Iteration 47, loss = 0.15191995\n",
      "Iteration 48, loss = 0.14595200\n",
      "Iteration 49, loss = 0.11221660\n",
      "Iteration 50, loss = 0.19888664\n",
      "Iteration 51, loss = 0.17308733\n",
      "Iteration 52, loss = 0.18595444\n",
      "Iteration 53, loss = 0.13974223\n",
      "Iteration 54, loss = 0.15097720\n",
      "Iteration 55, loss = 0.20283419\n",
      "Iteration 56, loss = 0.23873396\n",
      "Iteration 57, loss = 0.31709228\n",
      "Iteration 58, loss = 0.23935889\n",
      "Iteration 59, loss = 0.27425458\n",
      "Iteration 60, loss = 0.26954985\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 61, loss = 0.23599765\n",
      "Iteration 62, loss = 0.18245203\n",
      "Iteration 63, loss = 0.14144728\n",
      "Iteration 64, loss = 0.12372740\n",
      "Iteration 65, loss = 0.12142800\n",
      "Iteration 66, loss = 0.11222137\n",
      "Iteration 67, loss = 0.09488805\n",
      "Iteration 68, loss = 0.09093605\n",
      "Iteration 69, loss = 0.08688627\n",
      "Iteration 70, loss = 0.08314642\n",
      "Iteration 71, loss = 0.08039598\n",
      "Iteration 72, loss = 0.08153521\n",
      "Iteration 73, loss = 0.07888091\n",
      "Iteration 74, loss = 0.07863044\n",
      "Iteration 75, loss = 0.07281473\n",
      "Iteration 76, loss = 0.07035081\n",
      "Iteration 77, loss = 0.06744924\n",
      "Iteration 78, loss = 0.06553918\n",
      "Iteration 79, loss = 0.06335021\n",
      "Iteration 80, loss = 0.06273797\n",
      "Iteration 81, loss = 0.06194221\n",
      "Iteration 82, loss = 0.06114275\n",
      "Iteration 83, loss = 0.06047195\n",
      "Iteration 84, loss = 0.05998720\n",
      "Iteration 85, loss = 0.05955454\n",
      "Iteration 86, loss = 0.05936528\n",
      "Iteration 87, loss = 0.05888172\n",
      "Iteration 88, loss = 0.05842841\n",
      "Iteration 89, loss = 0.05791653\n",
      "Iteration 90, loss = 0.05755748\n",
      "Iteration 91, loss = 0.05702580\n",
      "Iteration 92, loss = 0.05683479\n",
      "Iteration 93, loss = 0.05638886\n",
      "Iteration 94, loss = 0.05589115\n",
      "Iteration 95, loss = 0.05413389\n",
      "Iteration 96, loss = 0.05405823\n",
      "Iteration 97, loss = 0.05328272\n",
      "Iteration 98, loss = 0.05279727\n",
      "Iteration 99, loss = 0.05227561\n",
      "Iteration 100, loss = 0.05233155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.53933697\n",
      "Iteration 2, loss = 0.66615130\n",
      "Iteration 3, loss = 0.53330404\n",
      "Iteration 4, loss = 0.47817265\n",
      "Iteration 5, loss = 0.49765557\n",
      "Iteration 6, loss = 0.45903649\n",
      "Iteration 7, loss = 0.42389920\n",
      "Iteration 8, loss = 0.32741828\n",
      "Iteration 9, loss = 0.34025394\n",
      "Iteration 10, loss = 0.37217766\n",
      "Iteration 11, loss = 0.32545418\n",
      "Iteration 12, loss = 0.33477618\n",
      "Iteration 13, loss = 0.33373213\n",
      "Iteration 14, loss = 0.29614444\n",
      "Iteration 15, loss = 0.33131469\n",
      "Iteration 16, loss = 0.39522769\n",
      "Iteration 17, loss = 0.29569781\n",
      "Iteration 18, loss = 0.34500780\n",
      "Iteration 19, loss = 0.36186934\n",
      "Iteration 20, loss = 0.25971974\n",
      "Iteration 21, loss = 0.27275327\n",
      "Iteration 22, loss = 0.31738951\n",
      "Iteration 23, loss = 0.28900553\n",
      "Iteration 24, loss = 0.25958143\n",
      "Iteration 25, loss = 0.21447792\n",
      "Iteration 26, loss = 0.27094859\n",
      "Iteration 27, loss = 0.33943630\n",
      "Iteration 28, loss = 0.23038180\n",
      "Iteration 29, loss = 0.24756026\n",
      "Iteration 30, loss = 0.18139222\n",
      "Iteration 31, loss = 0.22844838\n",
      "Iteration 32, loss = 0.19800800\n",
      "Iteration 33, loss = 0.21148129\n",
      "Iteration 34, loss = 0.28366903\n",
      "Iteration 35, loss = 0.28251637\n",
      "Iteration 36, loss = 0.30594186\n",
      "Iteration 37, loss = 0.28547023\n",
      "Iteration 38, loss = 0.23967798\n",
      "Iteration 39, loss = 0.18823045\n",
      "Iteration 40, loss = 0.29161802\n",
      "Iteration 41, loss = 0.35139898\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 42, loss = 0.25179179\n",
      "Iteration 43, loss = 0.18406872\n",
      "Iteration 44, loss = 0.14291544\n",
      "Iteration 45, loss = 0.13525413\n",
      "Iteration 46, loss = 0.12259495\n",
      "Iteration 47, loss = 0.11760052\n",
      "Iteration 48, loss = 0.11455406\n",
      "Iteration 49, loss = 0.10491111\n",
      "Iteration 50, loss = 0.10245067\n",
      "Iteration 51, loss = 0.09763832\n",
      "Iteration 52, loss = 0.09972989\n",
      "Iteration 53, loss = 0.09539799\n",
      "Iteration 54, loss = 0.09202000\n",
      "Iteration 55, loss = 0.09871063\n",
      "Iteration 56, loss = 0.09653552\n",
      "Iteration 57, loss = 0.08920796\n",
      "Iteration 58, loss = 0.08854241\n",
      "Iteration 59, loss = 0.08356188\n",
      "Iteration 60, loss = 0.08265122\n",
      "Iteration 61, loss = 0.08136858\n",
      "Iteration 62, loss = 0.08059508\n",
      "Iteration 63, loss = 0.07946026\n",
      "Iteration 64, loss = 0.07828461\n",
      "Iteration 65, loss = 0.07731328\n",
      "Iteration 66, loss = 0.07650491\n",
      "Iteration 67, loss = 0.07608471\n",
      "Iteration 68, loss = 0.07520662\n",
      "Iteration 69, loss = 0.07571249\n",
      "Iteration 70, loss = 0.07420052\n",
      "Iteration 71, loss = 0.07341388\n",
      "Iteration 72, loss = 0.07276110\n",
      "Iteration 73, loss = 0.07109480\n",
      "Iteration 74, loss = 0.07031352\n",
      "Iteration 75, loss = 0.06732608\n",
      "Iteration 76, loss = 0.06663937\n",
      "Iteration 77, loss = 0.06610040\n",
      "Iteration 78, loss = 0.06563429\n",
      "Iteration 79, loss = 0.06514400\n",
      "Iteration 80, loss = 0.06468118\n",
      "Iteration 81, loss = 0.06441326\n",
      "Iteration 82, loss = 0.06399963\n",
      "Iteration 83, loss = 0.06335573\n",
      "Iteration 84, loss = 0.06289271\n",
      "Iteration 85, loss = 0.06542700\n",
      "Iteration 86, loss = 0.06354598\n",
      "Iteration 87, loss = 0.06271899\n",
      "Iteration 88, loss = 0.06147502\n",
      "Iteration 89, loss = 0.06090308\n",
      "Iteration 90, loss = 0.05995098\n",
      "Iteration 91, loss = 0.05891550\n",
      "Iteration 92, loss = 0.05838513\n",
      "Iteration 93, loss = 0.05796334\n",
      "Iteration 94, loss = 0.05756651\n",
      "Iteration 95, loss = 0.05736336\n",
      "Iteration 96, loss = 0.05761172\n",
      "Iteration 97, loss = 0.05686641\n",
      "Iteration 98, loss = 0.05652839\n",
      "Iteration 99, loss = 0.05622277\n",
      "Iteration 100, loss = 0.05585156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.25549641\n",
      "Iteration 2, loss = 0.63618893\n",
      "Iteration 3, loss = 0.48332592\n",
      "Iteration 4, loss = 0.60289630\n",
      "Iteration 5, loss = 0.53882897\n",
      "Iteration 6, loss = 0.42321914\n",
      "Iteration 7, loss = 0.39335811\n",
      "Iteration 8, loss = 0.36564628\n",
      "Iteration 9, loss = 0.39412702\n",
      "Iteration 10, loss = 0.39555713\n",
      "Iteration 11, loss = 0.36582648\n",
      "Iteration 12, loss = 0.30448946\n",
      "Iteration 13, loss = 0.29878311\n",
      "Iteration 14, loss = 0.22867585\n",
      "Iteration 15, loss = 0.30316003\n",
      "Iteration 16, loss = 0.21767492\n",
      "Iteration 17, loss = 0.24352148\n",
      "Iteration 18, loss = 0.28542929\n",
      "Iteration 19, loss = 0.24764638\n",
      "Iteration 20, loss = 0.27968686\n",
      "Iteration 21, loss = 0.29997848\n",
      "Iteration 22, loss = 0.24884859\n",
      "Iteration 23, loss = 0.24273625\n",
      "Iteration 24, loss = 0.41205178\n",
      "Iteration 25, loss = 0.31097624\n",
      "Iteration 26, loss = 0.31077700\n",
      "Iteration 27, loss = 0.27213712\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 28, loss = 0.24171884\n",
      "Iteration 29, loss = 0.18987970\n",
      "Iteration 30, loss = 0.16987309\n",
      "Iteration 31, loss = 0.15374260\n",
      "Iteration 32, loss = 0.13729196\n",
      "Iteration 33, loss = 0.13079222\n",
      "Iteration 34, loss = 0.12331377\n",
      "Iteration 35, loss = 0.11860392\n",
      "Iteration 36, loss = 0.11492530\n",
      "Iteration 37, loss = 0.10767973\n",
      "Iteration 38, loss = 0.10352732\n",
      "Iteration 39, loss = 0.10049030\n",
      "Iteration 40, loss = 0.09699368\n",
      "Iteration 41, loss = 0.09479333\n",
      "Iteration 42, loss = 0.09263546\n",
      "Iteration 43, loss = 0.09258376\n",
      "Iteration 44, loss = 0.08910541\n",
      "Iteration 45, loss = 0.08688728\n",
      "Iteration 46, loss = 0.08547495\n",
      "Iteration 47, loss = 0.08469532\n",
      "Iteration 48, loss = 0.08340243\n",
      "Iteration 49, loss = 0.08362902\n",
      "Iteration 50, loss = 0.08170311\n",
      "Iteration 51, loss = 0.08018161\n",
      "Iteration 52, loss = 0.07896444\n",
      "Iteration 53, loss = 0.07834042\n",
      "Iteration 54, loss = 0.07759803\n",
      "Iteration 55, loss = 0.07668263\n",
      "Iteration 56, loss = 0.07629899\n",
      "Iteration 57, loss = 0.07592919\n",
      "Iteration 58, loss = 0.07452521\n",
      "Iteration 59, loss = 0.07659580\n",
      "Iteration 60, loss = 0.07249266\n",
      "Iteration 61, loss = 0.07126422\n",
      "Iteration 62, loss = 0.07161440\n",
      "Iteration 63, loss = 0.07115641\n",
      "Iteration 64, loss = 0.06965685\n",
      "Iteration 65, loss = 0.06919353\n",
      "Iteration 66, loss = 0.06860455\n",
      "Iteration 67, loss = 0.06764537\n",
      "Iteration 68, loss = 0.06782688\n",
      "Iteration 69, loss = 0.06719277\n",
      "Iteration 70, loss = 0.06712092\n",
      "Iteration 71, loss = 0.06609669\n",
      "Iteration 72, loss = 0.06635829\n",
      "Iteration 73, loss = 0.06505103\n",
      "Iteration 74, loss = 0.06458858\n",
      "Iteration 75, loss = 0.06406014\n",
      "Iteration 76, loss = 0.06394076\n",
      "Iteration 77, loss = 0.06363082\n",
      "Iteration 78, loss = 0.06288757\n",
      "Iteration 79, loss = 0.06281448\n",
      "Iteration 80, loss = 0.06252632\n",
      "Iteration 81, loss = 0.06210732\n",
      "Iteration 82, loss = 0.06156505\n",
      "Iteration 83, loss = 0.06210459\n",
      "Iteration 84, loss = 0.06054679\n",
      "Iteration 85, loss = 0.06025647\n",
      "Iteration 86, loss = 0.05921896\n",
      "Iteration 87, loss = 0.05812023\n",
      "Iteration 88, loss = 0.05833259\n",
      "Iteration 89, loss = 0.06019529\n",
      "Iteration 90, loss = 0.05664638\n",
      "Iteration 91, loss = 0.05757755\n",
      "Iteration 92, loss = 0.05868107\n",
      "Iteration 93, loss = 0.05679487\n",
      "Iteration 94, loss = 0.05493715\n",
      "Iteration 95, loss = 0.05463962\n",
      "Iteration 96, loss = 0.05464089\n",
      "Iteration 97, loss = 0.05403433\n",
      "Iteration 98, loss = 0.05371845\n",
      "Iteration 99, loss = 0.05334500\n",
      "Iteration 100, loss = 0.05299515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.19834746\n",
      "Iteration 2, loss = 0.68106429\n",
      "Iteration 3, loss = 0.58701986\n",
      "Iteration 4, loss = 0.50471141\n",
      "Iteration 5, loss = 0.50341496\n",
      "Iteration 6, loss = 0.43528115\n",
      "Iteration 7, loss = 0.45795064\n",
      "Iteration 8, loss = 0.34930374\n",
      "Iteration 9, loss = 0.36003115\n",
      "Iteration 10, loss = 0.38044348\n",
      "Iteration 11, loss = 0.49917153\n",
      "Iteration 12, loss = 0.45584026\n",
      "Iteration 13, loss = 0.46526732\n",
      "Iteration 14, loss = 0.36382535\n",
      "Iteration 15, loss = 0.43268472\n",
      "Iteration 16, loss = 0.39996145\n",
      "Iteration 17, loss = 0.35120335\n",
      "Iteration 18, loss = 0.28669984\n",
      "Iteration 19, loss = 0.38352689\n",
      "Iteration 20, loss = 0.30634252\n",
      "Iteration 21, loss = 0.37963051\n",
      "Iteration 22, loss = 0.37169978\n",
      "Iteration 23, loss = 0.38688424\n",
      "Iteration 24, loss = 0.36675905\n",
      "Iteration 25, loss = 0.25791862\n",
      "Iteration 26, loss = 0.40406871\n",
      "Iteration 27, loss = 0.27749972\n",
      "Iteration 28, loss = 0.30500667\n",
      "Iteration 29, loss = 0.31371591\n",
      "Iteration 30, loss = 0.36494514\n",
      "Iteration 31, loss = 0.29646150\n",
      "Iteration 32, loss = 0.29411174\n",
      "Iteration 33, loss = 0.22442521\n",
      "Iteration 34, loss = 0.27377253\n",
      "Iteration 35, loss = 0.25262641\n",
      "Iteration 36, loss = 0.24952692\n",
      "Iteration 37, loss = 0.25926684\n",
      "Iteration 38, loss = 0.27164388\n",
      "Iteration 39, loss = 0.31556136\n",
      "Iteration 40, loss = 0.33920245\n",
      "Iteration 41, loss = 0.30982495\n",
      "Iteration 42, loss = 0.25620503\n",
      "Iteration 43, loss = 0.23860748\n",
      "Iteration 44, loss = 0.21517766\n",
      "Iteration 45, loss = 0.19761649\n",
      "Iteration 46, loss = 0.19451297\n",
      "Iteration 47, loss = 0.18569945\n",
      "Iteration 48, loss = 0.21750456\n",
      "Iteration 49, loss = 0.19892171\n",
      "Iteration 50, loss = 0.21661587\n",
      "Iteration 51, loss = 0.27731318\n",
      "Iteration 52, loss = 0.25891144\n",
      "Iteration 53, loss = 0.34828471\n",
      "Iteration 54, loss = 0.25566089\n",
      "Iteration 55, loss = 0.25620092\n",
      "Iteration 56, loss = 0.22275647\n",
      "Iteration 57, loss = 0.40756851\n",
      "Iteration 58, loss = 0.32380659\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 59, loss = 0.19159775\n",
      "Iteration 60, loss = 0.15086708\n",
      "Iteration 61, loss = 0.12456830\n",
      "Iteration 62, loss = 0.12910469\n",
      "Iteration 63, loss = 0.12572459\n",
      "Iteration 64, loss = 0.12025576\n",
      "Iteration 65, loss = 0.11208179\n",
      "Iteration 66, loss = 0.11925769\n",
      "Iteration 67, loss = 0.11303590\n",
      "Iteration 68, loss = 0.10977089\n",
      "Iteration 69, loss = 0.10883607\n",
      "Iteration 70, loss = 0.10760693\n",
      "Iteration 71, loss = 0.10342383\n",
      "Iteration 72, loss = 0.10089804\n",
      "Iteration 73, loss = 0.09991426\n",
      "Iteration 74, loss = 0.09998526\n",
      "Iteration 75, loss = 0.09792040\n",
      "Iteration 76, loss = 0.09671803\n",
      "Iteration 77, loss = 0.09615320\n",
      "Iteration 78, loss = 0.09556461\n",
      "Iteration 79, loss = 0.09488960\n",
      "Iteration 80, loss = 0.09413810\n",
      "Iteration 81, loss = 0.09387299\n",
      "Iteration 82, loss = 0.09298322\n",
      "Iteration 83, loss = 0.09253167\n",
      "Iteration 84, loss = 0.09218068\n",
      "Iteration 85, loss = 0.09170572\n",
      "Iteration 86, loss = 0.09116770\n",
      "Iteration 87, loss = 0.09080025\n",
      "Iteration 88, loss = 0.09028295\n",
      "Iteration 89, loss = 0.08987589\n",
      "Iteration 90, loss = 0.08939594\n",
      "Iteration 91, loss = 0.08890816\n",
      "Iteration 92, loss = 0.08845560\n",
      "Iteration 93, loss = 0.08875370\n",
      "Iteration 94, loss = 0.08791298\n",
      "Iteration 95, loss = 0.08774436\n",
      "Iteration 96, loss = 0.09110769\n",
      "Iteration 97, loss = 0.08563555\n",
      "Iteration 98, loss = 0.07723648\n",
      "Iteration 99, loss = 0.07537305\n",
      "Iteration 100, loss = 0.07456325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.42118161\n",
      "Iteration 2, loss = 0.74092434\n",
      "Iteration 3, loss = 0.68018583\n",
      "Iteration 4, loss = 0.56587992\n",
      "Iteration 5, loss = 0.51423106\n",
      "Iteration 6, loss = 0.44308703\n",
      "Iteration 7, loss = 0.38847801\n",
      "Iteration 8, loss = 0.32964022\n",
      "Iteration 9, loss = 0.33827000\n",
      "Iteration 10, loss = 0.38131920\n",
      "Iteration 11, loss = 0.31775423\n",
      "Iteration 12, loss = 0.29835585\n",
      "Iteration 13, loss = 0.26095957\n",
      "Iteration 14, loss = 0.29761600\n",
      "Iteration 15, loss = 0.24536701\n",
      "Iteration 16, loss = 0.27951990\n",
      "Iteration 17, loss = 0.33505319\n",
      "Iteration 18, loss = 0.36027361\n",
      "Iteration 19, loss = 0.26467533\n",
      "Iteration 20, loss = 0.24219469\n",
      "Iteration 21, loss = 0.21226497\n",
      "Iteration 22, loss = 0.26730207\n",
      "Iteration 23, loss = 0.29443206\n",
      "Iteration 24, loss = 0.19744444\n",
      "Iteration 25, loss = 0.20292588\n",
      "Iteration 26, loss = 0.28872497\n",
      "Iteration 27, loss = 0.30586397\n",
      "Iteration 28, loss = 0.25091484\n",
      "Iteration 29, loss = 0.31837924\n",
      "Iteration 30, loss = 0.28106212\n",
      "Iteration 31, loss = 0.31960140\n",
      "Iteration 32, loss = 0.48712277\n",
      "Iteration 33, loss = 0.40742119\n",
      "Iteration 34, loss = 0.29960995\n",
      "Iteration 35, loss = 0.21857719\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 36, loss = 0.12184198\n",
      "Iteration 37, loss = 0.10179651\n",
      "Iteration 38, loss = 0.09845981\n",
      "Iteration 39, loss = 0.09027838\n",
      "Iteration 40, loss = 0.09062064\n",
      "Iteration 41, loss = 0.08738682\n",
      "Iteration 42, loss = 0.08042401\n",
      "Iteration 43, loss = 0.08458551\n",
      "Iteration 44, loss = 0.07893926\n",
      "Iteration 45, loss = 0.07582246\n",
      "Iteration 46, loss = 0.07120024\n",
      "Iteration 47, loss = 0.06956898\n",
      "Iteration 48, loss = 0.06818987\n",
      "Iteration 49, loss = 0.06792542\n",
      "Iteration 50, loss = 0.06713489\n",
      "Iteration 51, loss = 0.06649452\n",
      "Iteration 52, loss = 0.06618506\n",
      "Iteration 53, loss = 0.06560709\n",
      "Iteration 54, loss = 0.06456431\n",
      "Iteration 55, loss = 0.06393870\n",
      "Iteration 56, loss = 0.06225626\n",
      "Iteration 57, loss = 0.06192574\n",
      "Iteration 58, loss = 0.06084925\n",
      "Iteration 59, loss = 0.06072706\n",
      "Iteration 60, loss = 0.06002824\n",
      "Iteration 61, loss = 0.05943439\n",
      "Iteration 62, loss = 0.06068880\n",
      "Iteration 63, loss = 0.05970551\n",
      "Iteration 64, loss = 0.05783069\n",
      "Iteration 65, loss = 0.05881871\n",
      "Iteration 66, loss = 0.06606063\n",
      "Iteration 67, loss = 0.05854885\n",
      "Iteration 68, loss = 0.05991586\n",
      "Iteration 69, loss = 0.05533964\n",
      "Iteration 70, loss = 0.05512727\n",
      "Iteration 71, loss = 0.05208086\n",
      "Iteration 72, loss = 0.05332843\n",
      "Iteration 73, loss = 0.05196832\n",
      "Iteration 74, loss = 0.05020102\n",
      "Iteration 75, loss = 0.04956501\n",
      "Iteration 76, loss = 0.05006171\n",
      "Iteration 77, loss = 0.04901457\n",
      "Iteration 78, loss = 0.04854583\n",
      "Iteration 79, loss = 0.04815933\n",
      "Iteration 80, loss = 0.04776070\n",
      "Iteration 81, loss = 0.04739365\n",
      "Iteration 82, loss = 0.04711523\n",
      "Iteration 83, loss = 0.04683052\n",
      "Iteration 84, loss = 0.04659552\n",
      "Iteration 85, loss = 0.04634257\n",
      "Iteration 86, loss = 0.04608018\n",
      "Iteration 87, loss = 0.04582782\n",
      "Iteration 88, loss = 0.04561799\n",
      "Iteration 89, loss = 0.04536565\n",
      "Iteration 90, loss = 0.04515929\n",
      "Iteration 91, loss = 0.04492928\n",
      "Iteration 92, loss = 0.04470141\n",
      "Iteration 93, loss = 0.04451697\n",
      "Iteration 94, loss = 0.04429288\n",
      "Iteration 95, loss = 0.04407257\n",
      "Iteration 96, loss = 0.04389412\n",
      "Iteration 97, loss = 0.04366526\n",
      "Iteration 98, loss = 0.04362234\n",
      "Iteration 99, loss = 0.04303311\n",
      "Iteration 100, loss = 0.04270717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import time\n",
    "score_matrice_train=np.zeros((10,5))\n",
    "score_matrice_test=np.zeros((10,5))\n",
    "timefit_matrice=np.zeros((10,5))\n",
    "time_matrice=np.zeros((10,5))\n",
    "compte_columns=0\n",
    "for hiden_layer in range(12,25,3):\n",
    "    for compteur_ligne in range(10):\n",
    "        model = MLPClassifier(hidden_layer_sizes=hiden_layer, validation_fraction=0.2,activation='tanh',solver='sgd', batch_size=1, alpha=0, learning_rate='adaptive', verbose=0,max_iter=100)\n",
    "        #j'entraine mon model et calcul du temps\n",
    "        start_time=time.time()\n",
    "        model.fit(X_train,y_train)\n",
    "        timefit=time.time() - start_time\n",
    "        timefit_matrice[compteur_ligne][compte_columns]=timefit\n",
    "        #time du predict\n",
    "        start_time2=time.time()\n",
    "        model.predict(X_test)\n",
    "        timepredict=time.time() - start_time2\n",
    "        time_matrice[compteur_ligne][compte_columns]=timepredict\n",
    "        #je recupere les resultats pour le train\n",
    "        score_matrice_train[compteur_ligne][compte_columns]=model.score(X_train,y_train)\n",
    "        #je récupere les résultat pour la base de test\n",
    "        score_matrice_test[compteur_ligne][compte_columns]=model.score(X_test,y_test)\n",
    "\n",
    "    \n",
    "    #je passe a la colonne suivante    \n",
    "    compte_columns=compte_columns+1\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarde des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Sauvegarde des tableau de numpy en df pour eviter de les refaires a chauqe fois. random state = 42 on aura toujours les même resultats\\ndf_score_matrice_train_partCV= pd.DataFrame(data=score_matrice_train,columns=[\\'C=12\\',\\'C=15\\',\\'C=18\\',\\'C=21\\',\\'C=24\\'])\\ndf_score_matrice_test_partCV = pd.DataFrame(data=score_matrice_test,columns=[\\'C=12\\',\\'C=15\\',\\'C=18\\',\\'C=21\\',\\'C=24\\'])\\ndf_timefit_matrice_partCV = pd.DataFrame(data=timefit_matrice,columns=[\\'C=12\\',\\'C=15\\',\\'C=18\\',\\'C=21\\',\\'C=24\\'])\\ndf_time_matrice_partCV = pd.DataFrame(data=time_matrice,columns=[\\'C=12\\',\\'C=15\\',\\'C=18\\',\\'C=21\\',\\'C=24\\'])\\n#Export en PDF pour save en local et eviter le recalcul\\ndf_score_matrice_train_partCV.to_excel(\"df_score_matrice_train_partCV.xlsx\") \\ndf_score_matrice_test_partCV.to_excel(\"df_score_matrice_test_partCV.xlsx\") \\ndf_timefit_matrice_partCV.to_excel(\"df_timefit_matrice_partCV.xlsx\") \\ndf_time_matrice_partCV.to_excel(\"df_time_matrice_partCV.xlsx\") \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Sauvegarde des tableau de numpy en df pour eviter de les refaires a chauqe fois. random state = 42 on aura toujours les même resultats\n",
    "df_score_matrice_train_partCV= pd.DataFrame(data=score_matrice_train,columns=['C=12','C=15','C=18','C=21','C=24'])\n",
    "df_score_matrice_test_partCV = pd.DataFrame(data=score_matrice_test,columns=['C=12','C=15','C=18','C=21','C=24'])\n",
    "df_timefit_matrice_partCV = pd.DataFrame(data=timefit_matrice,columns=['C=12','C=15','C=18','C=21','C=24'])\n",
    "df_time_matrice_partCV = pd.DataFrame(data=time_matrice,columns=['C=12','C=15','C=18','C=21','C=24'])\n",
    "#Export en PDF pour save en local et eviter le recalcul\n",
    "df_score_matrice_train_partCV.to_excel(\"df_score_matrice_train_partCV.xlsx\") \n",
    "df_score_matrice_test_partCV.to_excel(\"df_score_matrice_test_partCV.xlsx\") \n",
    "df_timefit_matrice_partCV.to_excel(\"df_timefit_matrice_partCV.xlsx\") \n",
    "df_time_matrice_partCV.to_excel(\"df_time_matrice_partCV.xlsx\") \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_matrice_train=[]\n",
    "score_matrice_test=[]\n",
    "timefit_matrice=[]\n",
    "time_matrice=[]\n",
    "#recuperation des données\n",
    "score_matrice_train=pd.read_excel('df_score_matrice_train_partCV.xlsx', index_col=0)  \n",
    "score_matrice_test=pd.read_excel('df_score_matrice_test_partCV.xlsx', index_col=0)  \n",
    "timefit_matrice=pd.read_excel('df_timefit_matrice_partCV.xlsx', index_col=0) \n",
    "time_matrice=pd.read_excel('df_time_matrice_partCV.xlsx', index_col=0) \n",
    "#transformation en np\n",
    "score_matrice_train=np.array(score_matrice_train)\n",
    "score_matrice_test=np.array(score_matrice_test)\n",
    "timefit_matrice=np.array(timefit_matrice)\n",
    "time_matrice=np.array(time_matrice)\n",
    "#calcule de la moyenne du score pour chaque colonne\n",
    "accurancy_train=[]\n",
    "accurancy_test=[]\n",
    "accurancyfit_time=[]\n",
    "accurancy_time=[]\n",
    "for i in range(5):\n",
    "    accurancy_train.append(score_matrice_train[:,i].mean())\n",
    "    accurancy_test.append(score_matrice_test[:,i].mean())\n",
    "    accurancyfit_time.append(timefit_matrice[:,i].mean())\n",
    "    accurancy_time.append(time_matrice[:,i].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa55d88c940>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=[\"C=12\",\"C=15\",\"C=18\",\"C=21\",\"C=24\"]\n",
    "scoretrain = plt.plot(x,accurancy_train, color = 'blue')\n",
    "scoretest = plt.plot(x,accurancy_test, color = 'green')\n",
    "\n",
    "plt.legend()\n",
    "plt.legend(scoretest, ['scoretest'])\n",
    "\n",
    "ax2 = plt.gca().twinx()\n",
    "time = ax2.plot(x,accurancyfit_time, color = 'red')\n",
    "\n",
    "plt.legend(time, ['time']) \n",
    "#reste a fix la legend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal C=18\n",
    "minimise le rapport temps score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#Il faut relancer le prog avec le C optimal nous avons ici choisi 18\n",
    "start_time = time.time()\n",
    "model1= MLPClassifier(hidden_layer_sizes=18, activation='tanh',solver='sgd', batch_size=1, alpha=0, learning_rate='adaptive', verbose=0,max_iter=400)\n",
    "model1.fit(X_train,y_train)\n",
    "parameter= model1.get_params(deep=True) #return un dictionaire\n",
    "timeCoptimal=time.time() - start_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le score en train est de : 0.9817024661893397\n",
      "le score en test est de : 0.9537037037037037\n"
     ]
    }
   ],
   "source": [
    "print(f\"le score en train est de : {model1.score(X_train,y_train)}\")\n",
    "print(f\"le score en test est de : {model1.score(X_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test=model1.predict(X_test)\n",
    "y_pred_train=model1.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[53,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0, 48,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0, 45,  2,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 51,  0,  1,  0,  0,  2,  0],\n",
       "       [ 0,  0,  0,  0, 59,  0,  1,  0,  0,  0],\n",
       "       [ 0,  3,  0,  0,  0, 60,  1,  0,  1,  1],\n",
       "       [ 1,  0,  0,  0,  0,  0, 52,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 54,  0,  1],\n",
       "       [ 0,  1,  0,  0,  0,  0,  0,  1, 41,  0],\n",
       "       [ 0,  0,  1,  1,  0,  1,  0,  0,  4, 52]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test,y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 5, 0, 0, 3, 2, 0, 2, 6, 5]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compteur=0\n",
    "erreurarraytrain=[]\n",
    "for i in range(10):\n",
    "    erreur=0\n",
    "    for compteur in range(10):\n",
    "        if (compteur != i):\n",
    "            erreur=erreur + metrics.confusion_matrix(y_train,y_pred_train)[compteur][i]\n",
    "            compteur=compteur+1\n",
    "        else: \n",
    "            compteur=compteur+1\n",
    "    erreurarraytrain.append(erreur)\n",
    "    compteur=0\n",
    "\n",
    "erreurarraytrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette matrice de confusions nous pouvons remarquer que le plus grand nombre d'erreur se trouve pour les valeur 1 et 8 de plus nous pouvons observer que le 1 est régulierement confondu avec un 5(3fois)et le 8 avec un neuf (4 fois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[125,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0, 129,   0,   0,   2,   0,   0,   0,   0,   1],\n",
       "       [  0,   0, 128,   0,   0,   0,   0,   0,   2,   0],\n",
       "       [  0,   0,   0, 125,   0,   0,   0,   0,   3,   1],\n",
       "       [  0,   0,   0,   0, 121,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0, 114,   0,   0,   0,   2],\n",
       "       [  0,   0,   0,   0,   0,   0, 128,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   1,   0,   0, 123,   0,   0],\n",
       "       [  0,   5,   0,   0,   0,   0,   0,   1, 124,   1],\n",
       "       [  0,   0,   0,   0,   0,   2,   0,   1,   1, 117]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_train,y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "compteur=0\n",
    "erreurarraytest=[]\n",
    "for i in range(10):\n",
    "    erreur=0\n",
    "    for compteur in range(10):\n",
    "        if (compteur != i):\n",
    "            erreur=erreur + metrics.confusion_matrix(y_test,y_pred_test)[compteur][i]\n",
    "            compteur=compteur+1\n",
    "        else: \n",
    "            compteur=compteur+1\n",
    "    erreurarraytest.append(erreur)\n",
    "    compteur=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 3, 3, 0, 2, 2, 1, 7, 2]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "erreurarraytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette matrice de confusions nous pouvons remarquer que le plus grand nombre d'erreur se trouve pour les valeur 1 et 8 tjrs de plus nous pouvons observer que le 1 est régulierement confondu avec un 9(5fois)et le 8 avec un trois (3 fois)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recuperer parameter as un dictionaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open('parameter.json') as parameter:\n",
    "    parameterdl = json.load(parameter)\n",
    "#charge le model avec les poids du dictionaire\n",
    "clf8 = MLPClassifier(hidden_layer_sizes=hiden_layer, activation=’tanh’, solver=’sgd’, batch_size=1, alpha=0, learning_rate=’adaptive’,max_iter=100, validation _fraction=0.2,verbose=1)\n",
    "clf8.set_params(parameterdl)\n",
    "clf9=clf8.fit(X_train,y_train)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre K va varier de 0 a 1797/10 soit Kmax=180 apres on sera en surentrainement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import time\n",
    "score_matriceKNN_train=np.zeros((10,181))\n",
    "score_matriceKNN_test=np.zeros((10,181))\n",
    "timeKNNfit_matrice=np.zeros((10,181))\n",
    "timeKNN_matrice=np.zeros((10,181))\n",
    "compte_columns=0\n",
    "for k in range(1,181):\n",
    "    for i in range(10):\n",
    "        model=KNeighborsClassifier(n_neighbors=k, algorithm='brute')\n",
    "        #calcul du temps\n",
    "        start_time=time.time()\n",
    "        model.fit(X_train,y_train)\n",
    "        timefit=time.time() - start_time\n",
    "        \n",
    "        start_time2=time.time()\n",
    "        model.predict(X_test)\n",
    "        timepredict=time.time() - start_time2\n",
    "        \n",
    "        score_matriceKNN_train[i][compte_columns]=model.score(X_train,y_train)\n",
    "        score_matriceKNN_test[i][compte_columns]=model.score(X_test,y_test)\n",
    "        timeKNNfit_matrice[i][k]=timefit\n",
    "        timeKNN_matrice[i][k]=timepredict\n",
    "\n",
    "    #je passe a la colonne suivante    \n",
    "    compte_columns=compte_columns+1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Sauvegarde des tableau de numpy en df pour eviter de les refaires a chauqe fois. random state = 42 on aura toujours les même resultats\n",
    "df_score_matrice_train_partKNN= pd.DataFrame(data=score_matriceKNN_train)\n",
    "df_score_matrice_test_partKNN = pd.DataFrame(data=score_matriceKNN_test)\n",
    "df_timefit_matrice_partKNN = pd.DataFrame(data=timeKNNfit_matrice)\n",
    "df_time_matrice_partKNN = pd.DataFrame(data=timeKNN_matrice)\n",
    "#Export en PDF pour save en local et eviter le recalcul\n",
    "df_score_matrice_train_partKNN.to_excel(\"df_score_matrice_train_partKNN.xlsx\") \n",
    "df_score_matrice_test_partKNN.to_excel(\"df_score_matrice_test_partKNN.xlsx\") \n",
    "df_timefit_matrice_partKNN.to_excel(\"df_timefit_matrice_partKNN.xlsx\") \n",
    "df_time_matrice_partKNN.to_excel(\"df_time_matrice_partKNN.xlsx\") \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_matrice_train=[]\n",
    "score_matrice_test=[]\n",
    "time_matrice=[]\n",
    "timefit_matrice=[]\n",
    "#recuperation des données\n",
    "score_matrice_train=pd.read_excel('df_score_matrice_train_partKNN.xlsx', index_col=0)  \n",
    "score_matrice_test=pd.read_excel('df_score_matrice_test_partKNN.xlsx', index_col=0)  \n",
    "time_matrice=pd.read_excel('df_time_matrice_partKNN.xlsx', index_col=0) \n",
    "timefit_matrice=pd.read_excel('df_timefit_matrice_partKNN.xlsx', index_col=0) \n",
    "#transformation en np\n",
    "\n",
    "score_matrice_train=np.array(score_matrice_train)\n",
    "score_matrice_test=np.array(score_matrice_test)\n",
    "time_matrice=np.array(time_matrice)\n",
    "timefit_matrice=np.array(timefit_matrice)\n",
    "#calcule de la moyenne du score pour chaque colonne\n",
    "accurancy_train=[]\n",
    "accurancy_test=[]\n",
    "accurancy_time=[]\n",
    "accurancyfit_time=[]\n",
    "for i in range(180):\n",
    "    accurancy_train.append(score_matrice_train[:,i].mean())\n",
    "    accurancy_test.append(score_matrice_test[:,i].mean())\n",
    "    accurancy_time.append(time_matrice[:,i].mean())\n",
    "    accurancyfit_time.append(timefit_matrice[:,i].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb4c686eeb0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=[]\n",
    "for i in range(180):\n",
    "    x.append(i)\n",
    "    \n",
    "    \n",
    "scoretrain = plt.plot(x,accurancy_train, color = 'blue')\n",
    "scoretest = plt.plot(x,accurancy_test, color = 'green')\n",
    "\n",
    "plt.legend()\n",
    "plt.legend(scoretest, ['scoretest'])\n",
    "\n",
    "ax2 = plt.gca().twinx()\n",
    "time = ax2.plot(x,accurancy_time, color = 'red')\n",
    "\n",
    "plt.legend(time, ['time']) \n",
    "#reste a fix la legend\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre K optimal se situe au environ  de 15 en effet plus le K augmente plus le temps de calcul augmente et la precissions baisse en test et en train. Notre model a trop de voisin et donc de distance ou il se trompe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rejet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On se propose d’améliorer les performances précédentes en autorisant le rejet dans l’étape de décision. On étudiera successivement :\n",
    "- le rejet de distance : argmax_reject_threshold\n",
    "- le rejet d’ambigüité : argmax_top2_reject_threshold\n",
    "On utilisera la fonction predict_proba pour obtenir les probabilités a posteriori des classes. Faire varier le seuil (threshold) de 0 à 1 par pas de 10-2. Pour chaque valeur, calculer :\n",
    "- le taux de rejet (#exemple rejetés/#exemples total)\n",
    "- le taux de reconnaissance (#exemple bien classés / #exemples classés)\n",
    "Tracer dans les deux cas la courbe (taux de reconnaissance en fonction du taux de rejet). Choisir la méthode la plus efficace et le seuil associé (meilleur rapport #exemples bien classés / #exemples rejetés)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edouardnadaud/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#Il faut relancer le prog avec le C optimal\n",
    "start_time = time.time()\n",
    "model1= MLPClassifier(hidden_layer_sizes=18, activation='tanh',solver='sgd', batch_size=1, alpha=0, learning_rate='adaptive', verbose=0,max_iter=400)\n",
    "model1.fit(X_train,y_train)\n",
    "parameter= model1.get_params(deep=True) #return un dictionaire\n",
    "timeCoptimal=time.time() - start_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax_reject_threshold(y, threshold):\n",
    "    y_argmax = np.argmax(y, axis=1)\n",
    "    y_masked = np.ma.array(y_argmax, mask=(np.amax(y, axis=1) < threshold))\n",
    "\n",
    "    return y_masked.filled(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model1.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-323e0d070e81>:22: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  tauxdereconnaisance.append(exemplebienclasse/exempleclasse)\n"
     ]
    }
   ],
   "source": [
    "tauxderejet=[]\n",
    "tauxdereconnaisance=[]\n",
    "for i in np.arange(0.001,1.001,0.001):\n",
    "    tauxderejet.append((sum(argmax_reject_threshold(y_pred, i)==-1)/X_test.shape[0]))\n",
    "    #matrice de confusion\n",
    "    y_pred_test=argmax_reject_threshold(y_pred, i)\n",
    "    \n",
    "    y_pred_test[0]=-1\n",
    "    matricedeconfusion=metrics.confusion_matrix(y_test,y_pred_test)\n",
    "    #exemple bien classé\n",
    "    exemplebienclasse=0\n",
    "    #exempleclasse\n",
    "    exempleclasse=0\n",
    "    \n",
    "    for i in range(1,11):\n",
    "        for compteur in range (1,11):\n",
    "            if (i==compteur):\n",
    "                exemplebienclasse=exemplebienclasse+matricedeconfusion[i][compteur]\n",
    "            else:\n",
    "                exempleclasse=exempleclasse+matricedeconfusion[i][compteur]\n",
    "    exempleclasse=exempleclasse+exemplebienclasse\n",
    "    tauxdereconnaisance.append(exemplebienclasse/exempleclasse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "x=np.zeros((1000,1))\n",
    "compteur=0\n",
    "for i in np.arange(0.001,1.001,0.001):\n",
    "    x[compteur]=i\n",
    "    compteur=compteur+1\n",
    "    \n",
    "plt.plot(x,tauxderejet)\n",
    "plt.plot(x,tauxdereconnaisance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus le taux de rejet est important plus le taux de reconnaisance augmente néanmoins cela n'est pas immense mais oeut permettre d'ameliorer le modele. EN effet l'avantage d'un reseau de neuronne MLP a lieu lors de data set immense ( pour notre dataset un KNN serai plus efficace) mais si ce model devait rentrer en prod alors il serait bien de maximiser le rejet ( creer une data base de donner rejeter (donc bcp plus petite )) que l'on recalssifierai grace a un KNN cette association augmenterais significativement le nombre de donnée bien calssé en minimisant le temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax_top2_reject_threshold(y, threshold):\n",
    "    y_argmax = np.argmax(y, axis=1)\n",
    "    y_top2 = np.sort(y, axis=1)[:,-1:-3:-1]\n",
    "    y_masked = np.ma.array(y_argmax, mask=((y_top2[:,0] - y_top2[:,1]) < threshold))\n",
    "\n",
    "    return y_masked.filled(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1=model1.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-000dc8c768a2>:22: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  tauxdereconnaisance.append(exemplebienclasse/exempleclasse)\n"
     ]
    }
   ],
   "source": [
    "tauxderejet=[]\n",
    "tauxdereconnaisance=[]\n",
    "for i in np.arange(0.001,1.001,0.001):\n",
    "    tauxderejet.append((sum(argmax_top2_reject_threshold(y_pred1, i)==-1)/X_test.shape[0]))\n",
    "    #matrice de confusion\n",
    "    y_pred_test=argmax_top2_reject_threshold(y_pred1, i)\n",
    "    #je modifie une valeur pour que toutes les matrices de confusion inclue -1\n",
    "    y_pred_test[0]=-1\n",
    "    matricedeconfusion=metrics.confusion_matrix(y_test,y_pred_test)\n",
    "    #exemple bien classé\n",
    "    exemplebienclasse=0\n",
    "    #exempleclasse\n",
    "    exempleclasse=0\n",
    "    \n",
    "    for i in range(1,11):\n",
    "        for compteur in range (1,11):\n",
    "            if (i==compteur):\n",
    "                exemplebienclasse=exemplebienclasse+matricedeconfusion[i][compteur]\n",
    "            else:\n",
    "                exempleclasse=exempleclasse+matricedeconfusion[i][compteur]\n",
    "    exempleclasse=exempleclasse+exemplebienclasse\n",
    "    tauxdereconnaisance.append(exemplebienclasse/exempleclasse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "x=np.zeros((1000,1))\n",
    "compteur=0\n",
    "for i in np.arange(0.001,1.001,0.001):\n",
    "    x[compteur]=i\n",
    "    compteur=compteur+1\n",
    "    \n",
    "plt.plot(x,tauxderejet)\n",
    "plt.plot(x,tauxdereconnaisance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same resultat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cascade de classifieurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliser l'algorithme des k-ppv pour classer les exemples rejetés. Calculer le taux d'erreur global de la cascade de classifieurs constituée de deux étages. Donner les matrices de confusion du 1er étage et de la cascade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0, batch_size=1, hidden_layer_sizes=18,\n",
       "              learning_rate='adaptive', max_iter=400, solver='sgd', verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on execute notre reseau de neuronne:\n",
    "model5= MLPClassifier(hidden_layer_sizes=18, activation='tanh',solver='sgd', batch_size=1, alpha=0, learning_rate='adaptive', verbose=0,max_iter=400)\n",
    "model5.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 18)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.coefs_=np.array(model5.coefs_)\n",
    "model5.coefs_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='brute', n_neighbors=15)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#entrainement du KNN\n",
    "modelKNN=KNeighborsClassifier(n_neighbors=15, algorithm='brute')\n",
    "modelKNN.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#realisation des prediction uniquement grace au model MLP entrainé\n",
    "y_pred_test=model5.predict(X_test)\n",
    "#on save la matrice de confusion\n",
    "matriceconfusionpremieretage=metrics.confusion_matrix(y_test,y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[51,  0,  0,  0,  1,  0,  1,  0,  0,  0],\n",
       "       [ 0, 46,  4,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  2, 41,  3,  0,  0,  0,  0,  1,  0],\n",
       "       [ 0,  0,  4, 48,  0,  1,  0,  0,  1,  0],\n",
       "       [ 0,  1,  0,  0, 58,  0,  1,  0,  0,  0],\n",
       "       [ 0,  0,  0,  3,  1, 61,  0,  0,  1,  0],\n",
       "       [ 1,  1,  0,  0,  0,  0, 51,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 54,  0,  1],\n",
       "       [ 0,  2,  0,  1,  0,  1,  0,  0, 39,  0],\n",
       "       [ 0,  1,  0,  1,  0,  1,  0,  1,  3, 52]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriceconfusionpremieretage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nous avons 39 donnée mal classé \n"
     ]
    }
   ],
   "source": [
    "compteur=0\n",
    "erreurarraytrain=[]\n",
    "for i in range(10):\n",
    "    erreur=0\n",
    "    for compteur in range(10):\n",
    "        if (compteur != i):\n",
    "            erreur=erreur + matriceconfusionpremieretage[compteur][i]\n",
    "            compteur=compteur+1\n",
    "        else: \n",
    "            compteur=compteur+1\n",
    "    erreurarraytrain.append(erreur)\n",
    "    compteur=0\n",
    "\n",
    "print(f\"nous avons {sum(erreurarraytrain)} donnée mal classé \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette matrice de confusions ne prends pas le rejet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rejet\n",
    "y_pred_test1=model5.predict_proba(X_test)\n",
    "rejet=argmax_reject_threshold(y_pred_test1,0.9)\n",
    "matriceconfusionrejet=metrics.confusion_matrix(y_test,rejet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nous avons 46 exemples rejeté\n"
     ]
    }
   ],
   "source": [
    "print(f\"nous avons {sum(rejet==-1)} exemples rejeté\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 2, 50,  0,  0,  0,  1,  0,  0,  0,  0,  0],\n",
       "       [18,  0, 30,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 4,  0,  0, 40,  2,  0,  0,  0,  0,  1,  0],\n",
       "       [ 1,  0,  0,  4, 47,  0,  1,  0,  0,  1,  0],\n",
       "       [ 2,  0,  1,  0,  0, 56,  0,  1,  0,  0,  0],\n",
       "       [ 3,  0,  0,  0,  0,  1, 61,  0,  0,  1,  0],\n",
       "       [ 3,  1,  0,  0,  0,  0,  0, 49,  0,  0,  0],\n",
       "       [ 3,  0,  0,  0,  0,  0,  0,  0, 51,  0,  1],\n",
       "       [ 5,  0,  0,  0,  0,  0,  1,  0,  0, 37,  0],\n",
       "       [ 5,  0,  1,  0,  0,  0,  0,  0,  0,  2, 51]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriceconfusionrejet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nous pouvons voir que que :\n",
    "2-->0\n",
    "18-->1\n",
    "4-->2\n",
    "1-->3\n",
    "2-->4\n",
    "3-->5\n",
    "3-->6\n",
    "3-->7\n",
    "5-->8\n",
    "5-->9\n",
    "la probabilité n'a pas été suffisante pour décider de leur calssification. kNN ayant un meilleure taux de reconaissance mais un temps plus long ( si nouss avions un dataset de centaine de millier de donnée) nous ne l'utilisons que en deuxieme recours pour décider sur 46 données ( 8.5% du dataset (le threshold mis etant tres haut pour minimiser l'erreur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexrejete=(np.where(rejet==-1))    \n",
    "#ON FAIT UN PREDICT AVEC NOTRE KNN ET NOTRE KOPI=20    \n",
    "predictKNN=modelKNN.predict(X_test[indexrejete])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on remplace les predict du knn dans le predict avec rejet pour faire la derniere matrice de confusion\n",
    "for compteur in range(len(indexrejete[0])):\n",
    "    rejet[indexrejete[0][compteur]]=predictKNN[compteur]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nous avons 0 exemples rejeté\n"
     ]
    }
   ],
   "source": [
    "rejetinitial=sum(rejet==-1)\n",
    "print(f\"nous avons {sum(rejet==-1)} exemples rejeté\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela est normal nous les avons tous reclassé grace au knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrice de confusion final\n",
    "matriceconfusionfinal=metrics.confusion_matrix(y_test,rejet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[52,  0,  0,  0,  1,  0,  0,  0,  0,  0],\n",
       "       [ 0, 48,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0, 44,  2,  0,  0,  0,  0,  1,  0],\n",
       "       [ 0,  0,  4, 48,  0,  1,  0,  0,  1,  0],\n",
       "       [ 0,  1,  0,  0, 58,  0,  1,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  1, 64,  0,  0,  1,  0],\n",
       "       [ 1,  0,  0,  0,  0,  0, 52,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 54,  0,  1],\n",
       "       [ 0,  1,  0,  0,  0,  1,  0,  0, 41,  0],\n",
       "       [ 0,  1,  0,  0,  0,  1,  0,  2,  2, 53]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriceconfusionfinal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nous avons 26 donnée mal classé \n",
      "notre taux d'erreur est de : 0.04814814814814815\n",
      "notre taux d'erreur est passé de :8.518518518518519 a : 0.04814814814814815\n"
     ]
    }
   ],
   "source": [
    "compteur=0\n",
    "erreurarraytrain=[]\n",
    "for i in range(10):\n",
    "    erreur=0\n",
    "    for compteur in range(10):\n",
    "        if (compteur != i):\n",
    "            erreur=erreur + matriceconfusionfinal[compteur][i]\n",
    "            compteur=compteur+1\n",
    "        else: \n",
    "            compteur=compteur+1\n",
    "    erreurarraytrain.append(erreur)\n",
    "    compteur=0\n",
    "erreur=sum(erreurarraytrain)/100\n",
    "print(f\"nous avons {sum(erreurarraytrain)} donnée mal classé \")\n",
    "print(f\"notre taux d'erreur est de : {erreur*100/540}\")\n",
    "print(f\"notre taux d'erreur est passé de :{46*100/540} a : {erreur*100/540}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
